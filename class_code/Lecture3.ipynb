{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture3",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN36uCJD3zka",
        "colab_type": "code",
        "outputId": "462944b6-a0df-4f47-a99a-2e20e89e444c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 1.14.0\n",
            "Eager execution: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk4GGh2C3Idm",
        "colab_type": "code",
        "outputId": "fda545a8-d62d-461d-8508-ca52c2e93459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
        "\n",
        "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
        "                                           origin=train_dataset_url)\n",
        "\n",
        "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local copy of the dataset file: /root/.keras/datasets/iris_training.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W-BEgzB3YZ3",
        "colab_type": "code",
        "outputId": "1d142725-f89f-4fb7-92a9-bf21c5626ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!head -n5 {train_dataset_fp}\n",
        "!wc -l {train_dataset_fp}"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120,4,setosa,versicolor,virginica\n",
            "6.4,2.8,5.6,2.2,2\n",
            "5.0,2.3,3.3,1.0,1\n",
            "4.9,2.5,4.5,1.7,2\n",
            "4.9,3.1,1.5,0.1,0\n",
            "121 /root/.keras/datasets/iris_training.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwbg1hdr3dBv",
        "colab_type": "code",
        "outputId": "5452f3f9-7e93-4442-da02-927e5fbc1271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# column order in CSV file\n",
        "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
        "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n",
        "\n",
        "feature_names = column_names[:-1]\n",
        "label_name = column_names[-1]\n",
        "\n",
        "print(\"Features: {}\".format(feature_names))\n",
        "print(\"Label: {}\".format(label_name))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
            "Label: species\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQhxvfD43g4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset = tf.contrib.data.make_csv_dataset(\n",
        "    train_dataset_fp,\n",
        "    batch_size,\n",
        "    column_names=column_names,\n",
        "    label_name=label_name,\n",
        "    num_epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_oKg1Sg3oZf",
        "colab_type": "code",
        "outputId": "3316aa7e-abe2-4e6e-93e9-a20c8e48f815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "features, labels = next(iter(train_dataset))\n",
        "\n",
        "features"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('sepal_length',\n",
              "              <tf.Tensor: id=28192, shape=(32,), dtype=float32, numpy=\n",
              "              array([5. , 4.9, 4.7, 5.1, 6.5, 6.7, 6.4, 6.9, 6.9, 5.2, 6.4, 4.7, 4.8,\n",
              "                     6.7, 5.8, 7.7, 5.7, 6.8, 5.8, 6. , 5.2, 4.8, 6.1, 5.7, 5. , 6. ,\n",
              "                     4.4, 6.8, 5.4, 5. , 6.6, 5.4], dtype=float32)>),\n",
              "             ('sepal_width',\n",
              "              <tf.Tensor: id=28193, shape=(32,), dtype=float32, numpy=\n",
              "              array([3.2, 3.1, 3.2, 3.8, 3.2, 3. , 3.2, 3.1, 3.1, 3.4, 3.2, 3.2, 3. ,\n",
              "                     3.3, 2.7, 3.8, 2.8, 3.2, 2.7, 3. , 2.7, 3.1, 2.8, 3.8, 3.4, 2.2,\n",
              "                     2.9, 2.8, 3.4, 3. , 2.9, 3.9], dtype=float32)>),\n",
              "             ('petal_length',\n",
              "              <tf.Tensor: id=28190, shape=(32,), dtype=float32, numpy=\n",
              "              array([1.2, 1.5, 1.6, 1.5, 5.1, 5.2, 4.5, 4.9, 5.1, 1.4, 5.3, 1.3, 1.4,\n",
              "                     5.7, 5.1, 6.7, 4.5, 5.9, 5.1, 4.8, 3.9, 1.6, 4. , 1.7, 1.6, 5. ,\n",
              "                     1.4, 4.8, 1.5, 1.6, 4.6, 1.3], dtype=float32)>),\n",
              "             ('petal_width',\n",
              "              <tf.Tensor: id=28191, shape=(32,), dtype=float32, numpy=\n",
              "              array([0.2, 0.1, 0.2, 0.3, 2. , 2.3, 1.5, 1.5, 2.3, 0.2, 2.3, 0.2, 0.3,\n",
              "                     2.1, 1.9, 2.2, 1.3, 2.3, 1.9, 1.8, 1.4, 0.2, 1.3, 0.3, 0.4, 1.5,\n",
              "                     0.2, 1.4, 0.4, 0.2, 1.3, 0.4], dtype=float32)>)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5iwM5v33vPu",
        "colab_type": "code",
        "outputId": "980d9478-40b6-4fcd-ea89-e9d213d401f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.scatter(features['petal_length'].numpy(),\n",
        "            features['sepal_length'].numpy(),\n",
        "            c=labels.numpy(),\n",
        "            cmap='viridis')\n",
        "\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Sepal length\")\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPt6LnyM30hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pack_features_vector(features, labels):\n",
        "  \"\"\"Pack the features into a single array.\"\"\"\n",
        "  features = tf.stack(list(features.values()), axis=1)\n",
        "  return features, labels\n",
        "train_dataset = train_dataset.map(pack_features_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEv3We2V33qQ",
        "colab_type": "code",
        "outputId": "3fd960cd-abb6-4c26-8f67-cce30caca202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "features, labels = next(iter(train_dataset))\n",
        "\n",
        "print(features[:5])\n",
        "print(tf.one_hot(labels[:5], 3, dtype='float32'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[5.  3.2 1.2 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [6.5 3.2 5.1 2. ]], shape=(5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]], shape=(5, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1uz-84u45zH",
        "colab_type": "code",
        "outputId": "f7c21945-469c-4ec8-c132-03701de3706a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "class Model(object):\n",
        "  def __init__(self, num_hidden=3, num_output=3, num_inputs=4):\n",
        "    self.b1 = tf.Variable(tf.random.normal(\n",
        "                  (1, num_hidden),\n",
        "                  mean=0.0,\n",
        "                  stddev=1.0,\n",
        "                  dtype=tf.dtypes.float32,\n",
        "                  seed=None,\n",
        "                  name='b1'))\n",
        "    self.W1 = tf.Variable(tf.random.normal(\n",
        "                  (num_inputs, num_hidden),\n",
        "                   mean=0.0,\n",
        "                  stddev=1.0,\n",
        "                  dtype=tf.dtypes.float32,\n",
        "                  seed=None,\n",
        "                  name='W1'))\n",
        "    self.b2 = tf.Variable(tf.random.normal(\n",
        "                  (1, num_output),\n",
        "                  mean=0.0,\n",
        "                  stddev=1.0,\n",
        "                  dtype=tf.dtypes.float32,\n",
        "                  seed=None,\n",
        "                  name='b2'))\n",
        "    self.W2 = tf.Variable(tf.random.normal(\n",
        "                  (num_hidden, num_output),\n",
        "                   mean=0.0,\n",
        "                  stddev=1.0,\n",
        "                  dtype=tf.dtypes.float32,\n",
        "                  seed=None,\n",
        "                  name='W2'))\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    a1 = tf.sigmoid(\n",
        "          tf.linalg.matmul(x, self.W1) + self.b1\n",
        "          )\n",
        "    a2 = tf.nn.softmax(\n",
        "          tf.linalg.matmul(a1, self.W2) + self.b2\n",
        "          )\n",
        "    return a2\n",
        "\n",
        "model = Model()\n",
        "model(features[:5])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=28368, shape=(5, 3), dtype=float32, numpy=\n",
              "array([[0.13765976, 0.05198822, 0.810352  ],\n",
              "       [0.13766102, 0.05198886, 0.8103501 ],\n",
              "       [0.13726467, 0.05176727, 0.81096804],\n",
              "       [0.13638625, 0.0512777 , 0.8123361 ],\n",
              "       [0.1354088 , 0.05073471, 0.81385654]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42XbcwCo8FAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(y_hat, y):\n",
        "  return tf.reduce_mean(tf.square(y_hat - y))\n",
        "\n",
        "def train_step(model, inputs, outputs, learning_rate):\n",
        "  with tf.GradientTape() as t:\n",
        "    current_loss = loss(model(inputs), outputs)\n",
        "    dW1, dW2, db1, db2 = t.gradient(current_loss, [model.W1, model.W2, model.b1, model.b2])\n",
        "  \n",
        "  model.W1.assign_sub(learning_rate * dW1)\n",
        "  model.W2.assign_sub(learning_rate * dW2)\n",
        "  model.b1.assign_sub(learning_rate * db1)\n",
        "  model.b2.assign_sub(learning_rate * db2)\n",
        "\n",
        "train_step(model, features, tf.one_hot(labels, 3, dtype='float32'), 0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi-kJavm88cE",
        "colab_type": "code",
        "outputId": "d5f0355c-8544-497c-f68a-623833b69c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model()\n",
        "learning_rate = 0.1\n",
        "\n",
        "for epoch in range(100):\n",
        "  for inputs, outputs in iter(train_dataset):\n",
        "    outputs = tf.one_hot(outputs, 3, dtype='float32')\n",
        "    train_step(model, inputs, outputs, learning_rate)\n",
        "    \n",
        "  current_loss = tf.reduce_mean([loss(model(x), tf.one_hot(y, 3, dtype='float32')) for x, y in iter(train_dataset)])\n",
        "  current_loss = loss(model(inputs), outputs).numpy()\n",
        "  \n",
        "  print(f'The loss value for epoch {epoch} is {current_loss:0.2f}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The loss value for epoch 0 is 0.25\n",
            "The loss value for epoch 1 is 0.25\n",
            "The loss value for epoch 2 is 0.25\n",
            "The loss value for epoch 3 is 0.25\n",
            "The loss value for epoch 4 is 0.24\n",
            "The loss value for epoch 5 is 0.24\n",
            "The loss value for epoch 6 is 0.24\n",
            "The loss value for epoch 7 is 0.24\n",
            "The loss value for epoch 8 is 0.24\n",
            "The loss value for epoch 9 is 0.24\n",
            "The loss value for epoch 10 is 0.24\n",
            "The loss value for epoch 11 is 0.24\n",
            "The loss value for epoch 12 is 0.24\n",
            "The loss value for epoch 13 is 0.23\n",
            "The loss value for epoch 14 is 0.23\n",
            "The loss value for epoch 15 is 0.23\n",
            "The loss value for epoch 16 is 0.23\n",
            "The loss value for epoch 17 is 0.23\n",
            "The loss value for epoch 18 is 0.23\n",
            "The loss value for epoch 19 is 0.23\n",
            "The loss value for epoch 20 is 0.23\n",
            "The loss value for epoch 21 is 0.23\n",
            "The loss value for epoch 22 is 0.23\n",
            "The loss value for epoch 23 is 0.23\n",
            "The loss value for epoch 24 is 0.22\n",
            "The loss value for epoch 25 is 0.22\n",
            "The loss value for epoch 26 is 0.22\n",
            "The loss value for epoch 27 is 0.22\n",
            "The loss value for epoch 28 is 0.22\n",
            "The loss value for epoch 29 is 0.22\n",
            "The loss value for epoch 30 is 0.22\n",
            "The loss value for epoch 31 is 0.22\n",
            "The loss value for epoch 32 is 0.22\n",
            "The loss value for epoch 33 is 0.22\n",
            "The loss value for epoch 34 is 0.22\n",
            "The loss value for epoch 35 is 0.22\n",
            "The loss value for epoch 36 is 0.21\n",
            "The loss value for epoch 37 is 0.21\n",
            "The loss value for epoch 38 is 0.21\n",
            "The loss value for epoch 39 is 0.21\n",
            "The loss value for epoch 40 is 0.21\n",
            "The loss value for epoch 41 is 0.21\n",
            "The loss value for epoch 42 is 0.21\n",
            "The loss value for epoch 43 is 0.21\n",
            "The loss value for epoch 44 is 0.21\n",
            "The loss value for epoch 45 is 0.21\n",
            "The loss value for epoch 46 is 0.21\n",
            "The loss value for epoch 47 is 0.21\n",
            "The loss value for epoch 48 is 0.20\n",
            "The loss value for epoch 49 is 0.20\n",
            "The loss value for epoch 50 is 0.20\n",
            "The loss value for epoch 51 is 0.20\n",
            "The loss value for epoch 52 is 0.20\n",
            "The loss value for epoch 53 is 0.20\n",
            "The loss value for epoch 54 is 0.20\n",
            "The loss value for epoch 55 is 0.20\n",
            "The loss value for epoch 56 is 0.20\n",
            "The loss value for epoch 57 is 0.20\n",
            "The loss value for epoch 58 is 0.20\n",
            "The loss value for epoch 59 is 0.20\n",
            "The loss value for epoch 60 is 0.20\n",
            "The loss value for epoch 61 is 0.19\n",
            "The loss value for epoch 62 is 0.19\n",
            "The loss value for epoch 63 is 0.19\n",
            "The loss value for epoch 64 is 0.19\n",
            "The loss value for epoch 65 is 0.19\n",
            "The loss value for epoch 66 is 0.19\n",
            "The loss value for epoch 67 is 0.19\n",
            "The loss value for epoch 68 is 0.19\n",
            "The loss value for epoch 69 is 0.19\n",
            "The loss value for epoch 70 is 0.19\n",
            "The loss value for epoch 71 is 0.19\n",
            "The loss value for epoch 72 is 0.19\n",
            "The loss value for epoch 73 is 0.19\n",
            "The loss value for epoch 74 is 0.19\n",
            "The loss value for epoch 75 is 0.19\n",
            "The loss value for epoch 76 is 0.19\n",
            "The loss value for epoch 77 is 0.18\n",
            "The loss value for epoch 78 is 0.18\n",
            "The loss value for epoch 79 is 0.18\n",
            "The loss value for epoch 80 is 0.18\n",
            "The loss value for epoch 81 is 0.18\n",
            "The loss value for epoch 82 is 0.18\n",
            "The loss value for epoch 83 is 0.18\n",
            "The loss value for epoch 84 is 0.18\n",
            "The loss value for epoch 85 is 0.18\n",
            "The loss value for epoch 86 is 0.18\n",
            "The loss value for epoch 87 is 0.18\n",
            "The loss value for epoch 88 is 0.18\n",
            "The loss value for epoch 89 is 0.18\n",
            "The loss value for epoch 90 is 0.18\n",
            "The loss value for epoch 91 is 0.18\n",
            "The loss value for epoch 92 is 0.18\n",
            "The loss value for epoch 93 is 0.18\n",
            "The loss value for epoch 94 is 0.18\n",
            "The loss value for epoch 95 is 0.18\n",
            "The loss value for epoch 96 is 0.17\n",
            "The loss value for epoch 97 is 0.17\n",
            "The loss value for epoch 98 is 0.17\n",
            "The loss value for epoch 99 is 0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2_pp61ZKqKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "f57fcd22-8e06-4510-e3ed-92659cac7cb4"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-214974d0e0c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'DatasetV1Adapter' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAzwqk9oKsD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}