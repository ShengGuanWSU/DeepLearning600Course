{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of EECS600_HW3_Sheng.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnFbvwkcNoqq",
        "colab_type": "text"
      },
      "source": [
        "# Homework 3 - EECS 600 Deep Learning\n",
        "\n",
        "Name: \n",
        "\n",
        "Instructions: All assignments are to be completed individually. Please complete each question as best you can. Once you have completed all of the problems, reset your runtime or kernel and run the notebook in order. Download the .ipynb file and submit it via Canvas.\n",
        "\n",
        "Library usage: Several of the algorithms implemented in this, and other assignments, have implementations available in tensorflow. For this assignment, you may use any of the functionality within tensorflow.\n",
        "\n",
        "All code presented in class is free to use in your assignments.\n",
        "\n",
        "Your grade is based on the correctness of your implementation, not the quality of your code but you are encouraged to include comments in your code to help the graders understand your decisions.\n",
        "\n",
        "For clarity, the final operation you should take is restarting the runtime and running all.\n",
        "\n",
        "Due Date: 12/6/19 - midnight EST\n",
        "\n",
        "100 Points Total"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AOhHqMZ_wN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For this question, the reference is given in the class:\n",
        "#https://www.tensorflow.org/tutorials/generative/dcgan. \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGx8t61C_-2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To generate GIFs\n",
        "!pip install -q imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElBYya4BNs_v",
        "colab_type": "code",
        "outputId": "410b0a0d-1745-4977-fefb-2397e04fc88b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras import layers\n",
        "tf.enable_eager_execution()\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "from IPython import display\n",
        "#Grant the access and mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "TensorFlow version: 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy2kpBq8NxFT",
        "colab_type": "code",
        "outputId": "c8890964-4709-4338-f8cf-377e851a6e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "print(train_images.shape)\n",
        "print(train_labels[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EqfoQQmE-m9",
        "colab_type": "text"
      },
      "source": [
        "Loading the dataset returns four NumPy arrays:\n",
        "\n",
        "The train_images and train_labels arrays are the training set—the data the model uses to learn.\n",
        "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The labels are an array of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n",
        "\n",
        "Label |\tClass\n",
        "------|------\n",
        "0 |\tT-shirt/top\n",
        "1 |\tTrouser\n",
        "2 |\tPullover\n",
        "3 |\tDress\n",
        "4 |\tCoat\n",
        "5 |\tSandal\n",
        "6 |\tShirt\n",
        "7 |\tSneaker\n",
        "8 |\tBag\n",
        "9 |\tAnkle boot\n",
        "\n",
        "Each image is mapped to a single label. Since the class names are not included with the dataset.\n",
        "\n",
        "More information and a working example using this dataset here: https://www.tensorflow.org/tutorials/keras/classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adcmfAIFT7o3",
        "colab_type": "text"
      },
      "source": [
        "## Recommendation\n",
        "When building and testing your code, work on a smaller subset of the data. [This example](https://stackoverflow.com/questions/14262654/numpy-get-random-set-of-rows-from-2d-array) may help in that.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_udcy0TEQ_pK",
        "colab_type": "text"
      },
      "source": [
        "# Question 1\n",
        "Implement a Deep Convolutional Generative Adversarial Network for generating fasion MNIST photos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-AJ_KpMO9r5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocess the dataset\n",
        "#The images are black and white, must add an additional channel dimension to transform them to be three\n",
        "#dimensional\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg3UvGQ1_weG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNA6ZUMk_1rW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b_JHMxGBcGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The generator part\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    #Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument\n",
        "    #Here no activation is applied\n",
        "    #output arrays of shape (None,12544) None is the batch size\n",
        "    model.add(layers.BatchNormalization())\n",
        "    #batch normalization layer\n",
        "    #normalize the activations of the previous layer at each batch,\n",
        "    #(None,12544)\n",
        "    #i.e. applies a transformation that maintains the mean activation close to 0 and the activation \n",
        "    #standard deviation close to 1\n",
        "    model.add(layers.LeakyReLU())\n",
        "    #It allows a small gradient when the unit is not active: f(x) = alpha * x for x < 0, f(x) = x for x >= 0\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    #same results in padding the inpyt such that output has the same length as the original input\n",
        "\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    #It is like a layer that combines the UpSampling2D and Conv2D layers into one layer\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ews1A0iDB-io",
        "colab_type": "code",
        "outputId": "7bc336d5-87b3-4ce7-8d90-8969cb495fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        }
      },
      "source": [
        "generator = make_generator_model()\n",
        "#generator.summary()\n",
        "\n",
        "noise = tf.random.normal([1, 100])\n",
        "print(noise)\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-1.2511084  -0.2907497  -0.6354276  -2.1702087  -0.7893085  -0.11765523\n",
            "  -0.9017894  -0.46143922  0.04251894  0.19624196 -0.6717362  -0.08144298\n",
            "  -0.05721562 -0.9626788  -0.26699966 -0.47461146  1.2070478   0.44105676\n",
            "  -2.3801048  -0.21702248 -0.50575525  1.0984397  -0.5459532   0.25978976\n",
            "  -0.6144541   1.0201269  -0.11944397 -1.885078    0.32252854 -0.39888474\n",
            "   0.25540295  0.57348543 -0.43570548  1.2531585  -0.01545737 -2.453495\n",
            "   0.72890806  0.42527384 -2.2018628  -0.79977304  0.811498   -0.00964605\n",
            "   1.0146042  -1.1954978  -0.92771626  0.7624016   0.51208264  0.8230632\n",
            "  -0.48415768  0.39286917  0.8228257   0.47349477  2.8845007   0.03006286\n",
            "   1.6901126  -2.1074185   0.51547307 -0.53840786 -0.6890511  -0.33234468\n",
            "  -0.2720782  -0.29968274  0.45504406 -1.2086502  -1.1793591   0.05499498\n",
            "   0.4257317   0.32532158 -0.45615613  0.1713939   0.89324033  0.26142284\n",
            "   0.1485169   0.71476585 -2.2286315   0.09506568 -0.99376184 -0.9363976\n",
            "  -0.35704255  0.73980135 -0.00507716  0.82152134 -0.573013    0.01331436\n",
            "   0.1712386  -1.3579148   0.9014753   0.19606061 -0.32632887 -0.8720089\n",
            "   0.88450325  0.07245128  0.8438267   1.4877458   0.85215586 -1.8525828\n",
            "   1.5823867  -0.6350431  -0.25359562 -0.34035227]], shape=(1, 100), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f77b1f73470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIvw/1R2DXwHfEuwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxLhhtyABBUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[28, 28, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.gen_loss_result=[]\n",
        "    model.disc_loss_result=[]\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR3_so9vBXXa",
        "colab_type": "code",
        "outputId": "03ed5346-3f20-4feb-c962-1910ae987a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "discriminator = make_discriminator_model()\n",
        "#discriminator.summary()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[-0.00092956]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmiDQndCBxdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpr60hM4B2Jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro7c-xmwCLoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qE9honQCOJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CJxzwqhExwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save checkpoints, save and restore models in case training task is interrupted\n",
        "checkpoint_dir = F'/content/gdrive/My Drive/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBbZEcvZDXju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlQeD5sPDv0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "#@tf.function\n",
        "def train_step(images,current_step):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      #print(real_output.numpy())\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "      #print(fake_output.numpy())\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      print(f'current step is {current_step}')\n",
        "      print(f'gen_loss is {gen_loss.numpy():0.4f}')\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "      print(f'disc_loss is {disc_loss.numpy():0.4f}')\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    step_gen_loss = gen_loss.numpy()\n",
        "    step_disc_loss = disc_loss.numpy()\n",
        "    return step_gen_loss, step_disc_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW4L-8hMBLgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotLoss(gen_loss_result,disc_loss_result):\n",
        "  epoch_count = range(1, len(gen_loss_result) + 1)\n",
        "  plt.plot(epoch_count, gen_loss_result, 'r-')\n",
        "  plt.plot(epoch_count, disc_loss_result, 'b--')\n",
        "  plt.legend(['Generator Loss', 'Discriminator Loss'])\n",
        "  plt.title('Two losses in GANs')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP7-ZqRoEn-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs):\n",
        "\n",
        "  gen_loss_result =[]\n",
        "  disc_loss_result = []\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    current_step =0\n",
        "    batch_step_gen_loss =[]\n",
        "    batch_step_disc_loss = []\n",
        "    for image_batch in dataset:\n",
        "      #print(\"image_batch shape is\")\n",
        "      #print(image_batch.shape)\n",
        "      step_gen_loss, step_disc_loss = train_step(image_batch,current_step)\n",
        "      current_step+=1\n",
        "      batch_step_gen_loss.append(step_gen_loss)\n",
        "      batch_step_disc_loss.append(step_disc_loss)\n",
        "      #print(f'batch_step_gen_loss length is {len(batch_step_gen_loss)}')\n",
        "      #print(f'batch_step_disc_loss length is {len(batch_step_disc_loss)}')\n",
        "    epoch_mean_gen_loss = np.mean(batch_step_gen_loss)\n",
        "    epoch_mean_disc_loss = np.mean(batch_step_disc_loss)\n",
        "    print('Current epoch {} is'.format(epoch+1))\n",
        "\n",
        "    gen_loss_result.append(epoch_mean_gen_loss)\n",
        "    disc_loss_result.append(epoch_mean_disc_loss)\n",
        "\n",
        "    '''\n",
        "    # Produce images for the GIF as we go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "    '''\n",
        "    discriminator.gen_loss_result = gen_loss_result\n",
        "    discriminator.disc_loss_result = disc_loss_result\n",
        "    #print('gen_loss_result length is {}'.format(len(discriminator.gen_loss_result)))\n",
        "    #print('disc_loss_result length is {}'.format(len(discriminator.disc_loss_result)))\n",
        "    print(f'The gen_loss value for epoch {epoch+1} is {epoch_mean_gen_loss:0.4f}')\n",
        "    print(f'The disc_loss value for epoch {epoch+1} is {epoch_mean_disc_loss:0.4f}')\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    # Here we set epcochs as 15 since we have a pre-trained model that has 15 epochs for this assignment\n",
        "    # Just in case we have a backup pre-trained model, but cannot evaluate the loss info\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "  #plotLoss(gen_loss_result,disc_loss_result)\n",
        "  '''\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)\n",
        "  '''\n",
        "  return gen_loss_result, disc_loss_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnkcwCAHFrRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DRzJRcZGV8x",
        "colab_type": "code",
        "outputId": "7d97993a-a2bc-4bdf-c02e-b81ffa642d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gen_loss_result, disc_loss_result=train(train_dataset, EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "current step is 0\n",
            "gen_loss is 0.6952\n",
            "disc_loss is 1.3527\n",
            "current step is 1\n",
            "gen_loss is 0.6762\n",
            "disc_loss is 1.3387\n",
            "current step is 2\n",
            "gen_loss is 0.6573\n",
            "disc_loss is 1.3302\n",
            "current step is 3\n",
            "gen_loss is 0.6400\n",
            "disc_loss is 1.3111\n",
            "current step is 4\n",
            "gen_loss is 0.6246\n",
            "disc_loss is 1.3060\n",
            "current step is 5\n",
            "gen_loss is 0.6119\n",
            "disc_loss is 1.2917\n",
            "current step is 6\n",
            "gen_loss is 0.5979\n",
            "disc_loss is 1.2809\n",
            "current step is 7\n",
            "gen_loss is 0.5923\n",
            "disc_loss is 1.2631\n",
            "current step is 8\n",
            "gen_loss is 0.5898\n",
            "disc_loss is 1.2443\n",
            "current step is 9\n",
            "gen_loss is 0.5828\n",
            "disc_loss is 1.2255\n",
            "current step is 10\n",
            "gen_loss is 0.5813\n",
            "disc_loss is 1.2009\n",
            "current step is 11\n",
            "gen_loss is 0.5817\n",
            "disc_loss is 1.1904\n",
            "current step is 12\n",
            "gen_loss is 0.5761\n",
            "disc_loss is 1.1773\n",
            "current step is 13\n",
            "gen_loss is 0.5749\n",
            "disc_loss is 1.1590\n",
            "current step is 14\n",
            "gen_loss is 0.5710\n",
            "disc_loss is 1.1484\n",
            "current step is 15\n",
            "gen_loss is 0.5644\n",
            "disc_loss is 1.1465\n",
            "current step is 16\n",
            "gen_loss is 0.5590\n",
            "disc_loss is 1.1428\n",
            "current step is 17\n",
            "gen_loss is 0.5574\n",
            "disc_loss is 1.1285\n",
            "current step is 18\n",
            "gen_loss is 0.5521\n",
            "disc_loss is 1.1264\n",
            "current step is 19\n",
            "gen_loss is 0.5461\n",
            "disc_loss is 1.1323\n",
            "current step is 20\n",
            "gen_loss is 0.5420\n",
            "disc_loss is 1.1274\n",
            "current step is 21\n",
            "gen_loss is 0.5360\n",
            "disc_loss is 1.1181\n",
            "current step is 22\n",
            "gen_loss is 0.5384\n",
            "disc_loss is 1.1149\n",
            "current step is 23\n",
            "gen_loss is 0.5341\n",
            "disc_loss is 1.1135\n",
            "current step is 24\n",
            "gen_loss is 0.5351\n",
            "disc_loss is 1.1024\n",
            "current step is 25\n",
            "gen_loss is 0.5392\n",
            "disc_loss is 1.0923\n",
            "current step is 26\n",
            "gen_loss is 0.5437\n",
            "disc_loss is 1.0862\n",
            "current step is 27\n",
            "gen_loss is 0.5457\n",
            "disc_loss is 1.0943\n",
            "current step is 28\n",
            "gen_loss is 0.5592\n",
            "disc_loss is 1.0644\n",
            "current step is 29\n",
            "gen_loss is 0.5611\n",
            "disc_loss is 1.0641\n",
            "current step is 30\n",
            "gen_loss is 0.5661\n",
            "disc_loss is 1.0497\n",
            "current step is 31\n",
            "gen_loss is 0.5703\n",
            "disc_loss is 1.0463\n",
            "current step is 32\n",
            "gen_loss is 0.5786\n",
            "disc_loss is 1.0383\n",
            "current step is 33\n",
            "gen_loss is 0.5741\n",
            "disc_loss is 1.0414\n",
            "current step is 34\n",
            "gen_loss is 0.5790\n",
            "disc_loss is 1.0405\n",
            "current step is 35\n",
            "gen_loss is 0.5778\n",
            "disc_loss is 1.0530\n",
            "current step is 36\n",
            "gen_loss is 0.5736\n",
            "disc_loss is 1.0494\n",
            "current step is 37\n",
            "gen_loss is 0.5710\n",
            "disc_loss is 1.0632\n",
            "current step is 38\n",
            "gen_loss is 0.5654\n",
            "disc_loss is 1.0697\n",
            "current step is 39\n",
            "gen_loss is 0.5657\n",
            "disc_loss is 1.0663\n",
            "current step is 40\n",
            "gen_loss is 0.5630\n",
            "disc_loss is 1.0861\n",
            "current step is 41\n",
            "gen_loss is 0.5628\n",
            "disc_loss is 1.0912\n",
            "current step is 42\n",
            "gen_loss is 0.5701\n",
            "disc_loss is 1.0859\n",
            "current step is 43\n",
            "gen_loss is 0.5704\n",
            "disc_loss is 1.1011\n",
            "current step is 44\n",
            "gen_loss is 0.5734\n",
            "disc_loss is 1.1102\n",
            "current step is 45\n",
            "gen_loss is 0.5719\n",
            "disc_loss is 1.1032\n",
            "current step is 46\n",
            "gen_loss is 0.5680\n",
            "disc_loss is 1.1567\n",
            "current step is 47\n",
            "gen_loss is 0.5654\n",
            "disc_loss is 1.1251\n",
            "current step is 48\n",
            "gen_loss is 0.5476\n",
            "disc_loss is 1.1714\n",
            "current step is 49\n",
            "gen_loss is 0.5356\n",
            "disc_loss is 1.1866\n",
            "current step is 50\n",
            "gen_loss is 0.5292\n",
            "disc_loss is 1.1961\n",
            "current step is 51\n",
            "gen_loss is 0.5300\n",
            "disc_loss is 1.2082\n",
            "current step is 52\n",
            "gen_loss is 0.5406\n",
            "disc_loss is 1.2170\n",
            "current step is 53\n",
            "gen_loss is 0.5509\n",
            "disc_loss is 1.2097\n",
            "current step is 54\n",
            "gen_loss is 0.5653\n",
            "disc_loss is 1.2058\n",
            "current step is 55\n",
            "gen_loss is 0.5800\n",
            "disc_loss is 1.1927\n",
            "current step is 56\n",
            "gen_loss is 0.5910\n",
            "disc_loss is 1.1929\n",
            "current step is 57\n",
            "gen_loss is 0.6010\n",
            "disc_loss is 1.1889\n",
            "current step is 58\n",
            "gen_loss is 0.6142\n",
            "disc_loss is 1.2006\n",
            "current step is 59\n",
            "gen_loss is 0.6343\n",
            "disc_loss is 1.1913\n",
            "current step is 60\n",
            "gen_loss is 0.6654\n",
            "disc_loss is 1.1563\n",
            "current step is 61\n",
            "gen_loss is 0.6929\n",
            "disc_loss is 1.1321\n",
            "current step is 62\n",
            "gen_loss is 0.7244\n",
            "disc_loss is 1.1331\n",
            "current step is 63\n",
            "gen_loss is 0.7511\n",
            "disc_loss is 1.0880\n",
            "current step is 64\n",
            "gen_loss is 0.7797\n",
            "disc_loss is 1.0923\n",
            "current step is 65\n",
            "gen_loss is 0.8060\n",
            "disc_loss is 1.0608\n",
            "current step is 66\n",
            "gen_loss is 0.8300\n",
            "disc_loss is 1.0520\n",
            "current step is 67\n",
            "gen_loss is 0.8474\n",
            "disc_loss is 1.0284\n",
            "current step is 68\n",
            "gen_loss is 0.8606\n",
            "disc_loss is 1.0096\n",
            "current step is 69\n",
            "gen_loss is 0.8708\n",
            "disc_loss is 1.0091\n",
            "current step is 70\n",
            "gen_loss is 0.8746\n",
            "disc_loss is 0.9864\n",
            "current step is 71\n",
            "gen_loss is 0.8780\n",
            "disc_loss is 0.9765\n",
            "current step is 72\n",
            "gen_loss is 0.8780\n",
            "disc_loss is 0.9393\n",
            "current step is 73\n",
            "gen_loss is 0.8849\n",
            "disc_loss is 0.9524\n",
            "current step is 74\n",
            "gen_loss is 0.8739\n",
            "disc_loss is 0.9301\n",
            "current step is 75\n",
            "gen_loss is 0.8727\n",
            "disc_loss is 0.9223\n",
            "current step is 76\n",
            "gen_loss is 0.8778\n",
            "disc_loss is 0.9245\n",
            "current step is 77\n",
            "gen_loss is 0.8697\n",
            "disc_loss is 0.9214\n",
            "current step is 78\n",
            "gen_loss is 0.8581\n",
            "disc_loss is 0.9172\n",
            "current step is 79\n",
            "gen_loss is 0.8548\n",
            "disc_loss is 0.8941\n",
            "current step is 80\n",
            "gen_loss is 0.8382\n",
            "disc_loss is 0.8831\n",
            "current step is 81\n",
            "gen_loss is 0.8024\n",
            "disc_loss is 0.9300\n",
            "current step is 82\n",
            "gen_loss is 0.7455\n",
            "disc_loss is 0.9688\n",
            "current step is 83\n",
            "gen_loss is 0.7063\n",
            "disc_loss is 0.9976\n",
            "current step is 84\n",
            "gen_loss is 0.6531\n",
            "disc_loss is 1.0510\n",
            "current step is 85\n",
            "gen_loss is 0.6007\n",
            "disc_loss is 1.1091\n",
            "current step is 86\n",
            "gen_loss is 0.5606\n",
            "disc_loss is 1.1662\n",
            "current step is 87\n",
            "gen_loss is 0.5123\n",
            "disc_loss is 1.2329\n",
            "current step is 88\n",
            "gen_loss is 0.4876\n",
            "disc_loss is 1.2499\n",
            "current step is 89\n",
            "gen_loss is 0.4581\n",
            "disc_loss is 1.3194\n",
            "current step is 90\n",
            "gen_loss is 0.4310\n",
            "disc_loss is 1.3611\n",
            "current step is 91\n",
            "gen_loss is 0.4053\n",
            "disc_loss is 1.4378\n",
            "current step is 92\n",
            "gen_loss is 0.3864\n",
            "disc_loss is 1.4483\n",
            "current step is 93\n",
            "gen_loss is 0.3702\n",
            "disc_loss is 1.4985\n",
            "current step is 94\n",
            "gen_loss is 0.3612\n",
            "disc_loss is 1.5204\n",
            "current step is 95\n",
            "gen_loss is 0.3642\n",
            "disc_loss is 1.5097\n",
            "current step is 96\n",
            "gen_loss is 0.3861\n",
            "disc_loss is 1.4845\n",
            "current step is 97\n",
            "gen_loss is 0.4069\n",
            "disc_loss is 1.4676\n",
            "current step is 98\n",
            "gen_loss is 0.4507\n",
            "disc_loss is 1.4050\n",
            "current step is 99\n",
            "gen_loss is 0.4944\n",
            "disc_loss is 1.3419\n",
            "current step is 100\n",
            "gen_loss is 0.5456\n",
            "disc_loss is 1.2749\n",
            "current step is 101\n",
            "gen_loss is 0.6059\n",
            "disc_loss is 1.2007\n",
            "current step is 102\n",
            "gen_loss is 0.6598\n",
            "disc_loss is 1.1740\n",
            "current step is 103\n",
            "gen_loss is 0.7053\n",
            "disc_loss is 1.1118\n",
            "current step is 104\n",
            "gen_loss is 0.7520\n",
            "disc_loss is 1.0879\n",
            "current step is 105\n",
            "gen_loss is 0.7746\n",
            "disc_loss is 1.0582\n",
            "current step is 106\n",
            "gen_loss is 0.8004\n",
            "disc_loss is 1.0360\n",
            "current step is 107\n",
            "gen_loss is 0.8088\n",
            "disc_loss is 1.0413\n",
            "current step is 108\n",
            "gen_loss is 0.8012\n",
            "disc_loss is 1.0322\n",
            "current step is 109\n",
            "gen_loss is 0.7889\n",
            "disc_loss is 1.0602\n",
            "current step is 110\n",
            "gen_loss is 0.7560\n",
            "disc_loss is 1.0821\n",
            "current step is 111\n",
            "gen_loss is 0.7359\n",
            "disc_loss is 1.0796\n",
            "current step is 112\n",
            "gen_loss is 0.7105\n",
            "disc_loss is 1.0928\n",
            "current step is 113\n",
            "gen_loss is 0.6793\n",
            "disc_loss is 1.1309\n",
            "current step is 114\n",
            "gen_loss is 0.6792\n",
            "disc_loss is 1.1194\n",
            "current step is 115\n",
            "gen_loss is 0.6893\n",
            "disc_loss is 1.1153\n",
            "current step is 116\n",
            "gen_loss is 0.7131\n",
            "disc_loss is 1.1058\n",
            "current step is 117\n",
            "gen_loss is 0.7530\n",
            "disc_loss is 1.0502\n",
            "current step is 118\n",
            "gen_loss is 0.7972\n",
            "disc_loss is 1.0067\n",
            "current step is 119\n",
            "gen_loss is 0.8336\n",
            "disc_loss is 0.9906\n",
            "current step is 120\n",
            "gen_loss is 0.8879\n",
            "disc_loss is 0.9543\n",
            "current step is 121\n",
            "gen_loss is 0.9476\n",
            "disc_loss is 0.9178\n",
            "current step is 122\n",
            "gen_loss is 0.9957\n",
            "disc_loss is 0.8713\n",
            "current step is 123\n",
            "gen_loss is 1.0448\n",
            "disc_loss is 0.8004\n",
            "current step is 124\n",
            "gen_loss is 1.0874\n",
            "disc_loss is 0.8273\n",
            "current step is 125\n",
            "gen_loss is 1.0934\n",
            "disc_loss is 0.7749\n",
            "current step is 126\n",
            "gen_loss is 1.1012\n",
            "disc_loss is 0.7805\n",
            "current step is 127\n",
            "gen_loss is 1.0834\n",
            "disc_loss is 0.7924\n",
            "current step is 128\n",
            "gen_loss is 1.0595\n",
            "disc_loss is 0.8215\n",
            "current step is 129\n",
            "gen_loss is 1.0350\n",
            "disc_loss is 0.8465\n",
            "current step is 130\n",
            "gen_loss is 1.0268\n",
            "disc_loss is 0.7958\n",
            "current step is 131\n",
            "gen_loss is 1.0420\n",
            "disc_loss is 0.8256\n",
            "current step is 132\n",
            "gen_loss is 1.0544\n",
            "disc_loss is 0.7701\n",
            "current step is 133\n",
            "gen_loss is 1.0497\n",
            "disc_loss is 0.7870\n",
            "current step is 134\n",
            "gen_loss is 1.0481\n",
            "disc_loss is 0.8244\n",
            "current step is 135\n",
            "gen_loss is 1.0357\n",
            "disc_loss is 0.7988\n",
            "current step is 136\n",
            "gen_loss is 1.0161\n",
            "disc_loss is 0.8107\n",
            "current step is 137\n",
            "gen_loss is 1.0103\n",
            "disc_loss is 0.8050\n",
            "current step is 138\n",
            "gen_loss is 0.9817\n",
            "disc_loss is 0.8838\n",
            "current step is 139\n",
            "gen_loss is 0.9491\n",
            "disc_loss is 0.8547\n",
            "current step is 140\n",
            "gen_loss is 0.9518\n",
            "disc_loss is 0.8574\n",
            "current step is 141\n",
            "gen_loss is 0.9279\n",
            "disc_loss is 0.8528\n",
            "current step is 142\n",
            "gen_loss is 0.9575\n",
            "disc_loss is 0.8493\n",
            "current step is 143\n",
            "gen_loss is 0.9484\n",
            "disc_loss is 0.8552\n",
            "current step is 144\n",
            "gen_loss is 0.9863\n",
            "disc_loss is 0.8474\n",
            "current step is 145\n",
            "gen_loss is 1.0021\n",
            "disc_loss is 0.8503\n",
            "current step is 146\n",
            "gen_loss is 1.0367\n",
            "disc_loss is 0.8497\n",
            "current step is 147\n",
            "gen_loss is 1.0430\n",
            "disc_loss is 0.8170\n",
            "current step is 148\n",
            "gen_loss is 1.0514\n",
            "disc_loss is 0.8793\n",
            "current step is 149\n",
            "gen_loss is 1.0483\n",
            "disc_loss is 0.8949\n",
            "current step is 150\n",
            "gen_loss is 1.0209\n",
            "disc_loss is 0.8866\n",
            "current step is 151\n",
            "gen_loss is 0.9725\n",
            "disc_loss is 0.9289\n",
            "current step is 152\n",
            "gen_loss is 0.9313\n",
            "disc_loss is 0.9611\n",
            "current step is 153\n",
            "gen_loss is 0.8782\n",
            "disc_loss is 0.9948\n",
            "current step is 154\n",
            "gen_loss is 0.8694\n",
            "disc_loss is 1.0248\n",
            "current step is 155\n",
            "gen_loss is 0.8210\n",
            "disc_loss is 1.0179\n",
            "current step is 156\n",
            "gen_loss is 0.8080\n",
            "disc_loss is 1.0907\n",
            "current step is 157\n",
            "gen_loss is 0.7983\n",
            "disc_loss is 1.0943\n",
            "current step is 158\n",
            "gen_loss is 0.7910\n",
            "disc_loss is 1.1073\n",
            "current step is 159\n",
            "gen_loss is 0.7849\n",
            "disc_loss is 1.1117\n",
            "current step is 160\n",
            "gen_loss is 0.7739\n",
            "disc_loss is 1.1377\n",
            "current step is 161\n",
            "gen_loss is 0.7775\n",
            "disc_loss is 1.1411\n",
            "current step is 162\n",
            "gen_loss is 0.7873\n",
            "disc_loss is 1.1416\n",
            "current step is 163\n",
            "gen_loss is 0.7839\n",
            "disc_loss is 1.1290\n",
            "current step is 164\n",
            "gen_loss is 0.8038\n",
            "disc_loss is 1.1392\n",
            "current step is 165\n",
            "gen_loss is 0.7868\n",
            "disc_loss is 1.1448\n",
            "current step is 166\n",
            "gen_loss is 0.7994\n",
            "disc_loss is 1.1525\n",
            "current step is 167\n",
            "gen_loss is 0.8040\n",
            "disc_loss is 1.1635\n",
            "current step is 168\n",
            "gen_loss is 0.8252\n",
            "disc_loss is 1.1135\n",
            "current step is 169\n",
            "gen_loss is 0.8307\n",
            "disc_loss is 1.1039\n",
            "current step is 170\n",
            "gen_loss is 0.8266\n",
            "disc_loss is 1.0785\n",
            "current step is 171\n",
            "gen_loss is 0.8585\n",
            "disc_loss is 1.0320\n",
            "current step is 172\n",
            "gen_loss is 0.8577\n",
            "disc_loss is 1.0727\n",
            "current step is 173\n",
            "gen_loss is 0.8891\n",
            "disc_loss is 0.9860\n",
            "current step is 174\n",
            "gen_loss is 0.9044\n",
            "disc_loss is 0.9822\n",
            "current step is 175\n",
            "gen_loss is 0.9458\n",
            "disc_loss is 0.9501\n",
            "current step is 176\n",
            "gen_loss is 0.9472\n",
            "disc_loss is 0.9229\n",
            "current step is 177\n",
            "gen_loss is 0.9994\n",
            "disc_loss is 0.9077\n",
            "current step is 178\n",
            "gen_loss is 1.0382\n",
            "disc_loss is 0.8500\n",
            "current step is 179\n",
            "gen_loss is 1.0758\n",
            "disc_loss is 0.8815\n",
            "current step is 180\n",
            "gen_loss is 1.1079\n",
            "disc_loss is 0.8170\n",
            "current step is 181\n",
            "gen_loss is 1.1536\n",
            "disc_loss is 0.7904\n",
            "current step is 182\n",
            "gen_loss is 1.1918\n",
            "disc_loss is 0.8116\n",
            "current step is 183\n",
            "gen_loss is 1.2206\n",
            "disc_loss is 0.7947\n",
            "current step is 184\n",
            "gen_loss is 1.2601\n",
            "disc_loss is 0.7604\n",
            "current step is 185\n",
            "gen_loss is 1.2526\n",
            "disc_loss is 0.7638\n",
            "current step is 186\n",
            "gen_loss is 1.2658\n",
            "disc_loss is 0.7295\n",
            "current step is 187\n",
            "gen_loss is 1.2782\n",
            "disc_loss is 0.7544\n",
            "current step is 188\n",
            "gen_loss is 1.2860\n",
            "disc_loss is 0.6838\n",
            "current step is 189\n",
            "gen_loss is 1.2503\n",
            "disc_loss is 0.7524\n",
            "current step is 190\n",
            "gen_loss is 1.2443\n",
            "disc_loss is 0.7364\n",
            "current step is 191\n",
            "gen_loss is 1.2067\n",
            "disc_loss is 0.7829\n",
            "current step is 192\n",
            "gen_loss is 1.1565\n",
            "disc_loss is 0.8194\n",
            "current step is 193\n",
            "gen_loss is 1.1182\n",
            "disc_loss is 0.8798\n",
            "current step is 194\n",
            "gen_loss is 1.0585\n",
            "disc_loss is 0.8550\n",
            "current step is 195\n",
            "gen_loss is 1.0112\n",
            "disc_loss is 0.9219\n",
            "current step is 196\n",
            "gen_loss is 0.9565\n",
            "disc_loss is 0.9964\n",
            "current step is 197\n",
            "gen_loss is 0.8879\n",
            "disc_loss is 0.9869\n",
            "current step is 198\n",
            "gen_loss is 0.8735\n",
            "disc_loss is 1.0476\n",
            "current step is 199\n",
            "gen_loss is 0.8331\n",
            "disc_loss is 1.1561\n",
            "current step is 200\n",
            "gen_loss is 0.7889\n",
            "disc_loss is 1.1372\n",
            "current step is 201\n",
            "gen_loss is 0.7429\n",
            "disc_loss is 1.2770\n",
            "current step is 202\n",
            "gen_loss is 0.7547\n",
            "disc_loss is 1.4484\n",
            "current step is 203\n",
            "gen_loss is 0.7225\n",
            "disc_loss is 1.4164\n",
            "current step is 204\n",
            "gen_loss is 0.6650\n",
            "disc_loss is 1.5518\n",
            "current step is 205\n",
            "gen_loss is 0.6626\n",
            "disc_loss is 1.7175\n",
            "current step is 206\n",
            "gen_loss is 0.6502\n",
            "disc_loss is 1.7804\n",
            "current step is 207\n",
            "gen_loss is 0.6232\n",
            "disc_loss is 1.7649\n",
            "current step is 208\n",
            "gen_loss is 0.6029\n",
            "disc_loss is 1.8257\n",
            "current step is 209\n",
            "gen_loss is 0.6005\n",
            "disc_loss is 1.8196\n",
            "current step is 210\n",
            "gen_loss is 0.5630\n",
            "disc_loss is 1.8933\n",
            "current step is 211\n",
            "gen_loss is 0.5805\n",
            "disc_loss is 1.8863\n",
            "current step is 212\n",
            "gen_loss is 0.5630\n",
            "disc_loss is 1.9670\n",
            "current step is 213\n",
            "gen_loss is 0.5689\n",
            "disc_loss is 2.0073\n",
            "current step is 214\n",
            "gen_loss is 0.5606\n",
            "disc_loss is 1.9720\n",
            "current step is 215\n",
            "gen_loss is 0.5739\n",
            "disc_loss is 1.8644\n",
            "current step is 216\n",
            "gen_loss is 0.5797\n",
            "disc_loss is 2.0243\n",
            "current step is 217\n",
            "gen_loss is 0.5878\n",
            "disc_loss is 1.9909\n",
            "current step is 218\n",
            "gen_loss is 0.6092\n",
            "disc_loss is 1.9101\n",
            "current step is 219\n",
            "gen_loss is 0.6219\n",
            "disc_loss is 1.9086\n",
            "current step is 220\n",
            "gen_loss is 0.6437\n",
            "disc_loss is 1.9060\n",
            "current step is 221\n",
            "gen_loss is 0.6483\n",
            "disc_loss is 1.9133\n",
            "current step is 222\n",
            "gen_loss is 0.6854\n",
            "disc_loss is 1.7918\n",
            "current step is 223\n",
            "gen_loss is 0.6864\n",
            "disc_loss is 1.8402\n",
            "current step is 224\n",
            "gen_loss is 0.7233\n",
            "disc_loss is 1.7530\n",
            "current step is 225\n",
            "gen_loss is 0.7255\n",
            "disc_loss is 1.7306\n",
            "current step is 226\n",
            "gen_loss is 0.7551\n",
            "disc_loss is 1.7116\n",
            "current step is 227\n",
            "gen_loss is 0.7556\n",
            "disc_loss is 1.6127\n",
            "current step is 228\n",
            "gen_loss is 0.7444\n",
            "disc_loss is 1.6637\n",
            "current step is 229\n",
            "gen_loss is 0.7390\n",
            "disc_loss is 1.6296\n",
            "current step is 230\n",
            "gen_loss is 0.7343\n",
            "disc_loss is 1.5945\n",
            "current step is 231\n",
            "gen_loss is 0.7115\n",
            "disc_loss is 1.5401\n",
            "current step is 232\n",
            "gen_loss is 0.7298\n",
            "disc_loss is 1.4695\n",
            "current step is 233\n",
            "gen_loss is 0.7366\n",
            "disc_loss is 1.4567\n",
            "current step is 234\n",
            "gen_loss is 0.7353\n",
            "disc_loss is 1.4605\n",
            "Current epoch 1 is\n",
            "The train loss value for epoch 1 is 0.7596\n",
            "The test loss value for epoch 1 is 1.1489\n",
            "Time for epoch 1 is 1094.3601834774017 sec\n",
            "current step is 0\n",
            "gen_loss is 0.7680\n",
            "disc_loss is 1.4066\n",
            "current step is 1\n",
            "gen_loss is 0.7989\n",
            "disc_loss is 1.3777\n",
            "current step is 2\n",
            "gen_loss is 0.7997\n",
            "disc_loss is 1.3628\n",
            "current step is 3\n",
            "gen_loss is 0.8218\n",
            "disc_loss is 1.3393\n",
            "current step is 4\n",
            "gen_loss is 0.8229\n",
            "disc_loss is 1.3266\n",
            "current step is 5\n",
            "gen_loss is 0.8485\n",
            "disc_loss is 1.2678\n",
            "current step is 6\n",
            "gen_loss is 0.8530\n",
            "disc_loss is 1.2342\n",
            "current step is 7\n",
            "gen_loss is 0.8598\n",
            "disc_loss is 1.2046\n",
            "current step is 8\n",
            "gen_loss is 0.8590\n",
            "disc_loss is 1.1994\n",
            "current step is 9\n",
            "gen_loss is 0.8464\n",
            "disc_loss is 1.1772\n",
            "current step is 10\n",
            "gen_loss is 0.8496\n",
            "disc_loss is 1.1599\n",
            "current step is 11\n",
            "gen_loss is 0.8365\n",
            "disc_loss is 1.1644\n",
            "current step is 12\n",
            "gen_loss is 0.8394\n",
            "disc_loss is 1.1271\n",
            "current step is 13\n",
            "gen_loss is 0.8548\n",
            "disc_loss is 1.0991\n",
            "current step is 14\n",
            "gen_loss is 0.8351\n",
            "disc_loss is 1.0821\n",
            "current step is 15\n",
            "gen_loss is 0.8409\n",
            "disc_loss is 1.0748\n",
            "current step is 16\n",
            "gen_loss is 0.8650\n",
            "disc_loss is 1.0610\n",
            "current step is 17\n",
            "gen_loss is 0.8790\n",
            "disc_loss is 1.0427\n",
            "current step is 18\n",
            "gen_loss is 0.9008\n",
            "disc_loss is 1.0268\n",
            "current step is 19\n",
            "gen_loss is 0.9014\n",
            "disc_loss is 1.0336\n",
            "current step is 20\n",
            "gen_loss is 0.8961\n",
            "disc_loss is 1.0690\n",
            "current step is 21\n",
            "gen_loss is 0.9170\n",
            "disc_loss is 1.0059\n",
            "current step is 22\n",
            "gen_loss is 0.8985\n",
            "disc_loss is 1.0357\n",
            "current step is 23\n",
            "gen_loss is 0.9402\n",
            "disc_loss is 0.9938\n",
            "current step is 24\n",
            "gen_loss is 0.9060\n",
            "disc_loss is 1.0155\n",
            "current step is 25\n",
            "gen_loss is 0.9029\n",
            "disc_loss is 1.0088\n",
            "current step is 26\n",
            "gen_loss is 0.9068\n",
            "disc_loss is 1.0043\n",
            "current step is 27\n",
            "gen_loss is 0.9080\n",
            "disc_loss is 1.0251\n",
            "current step is 28\n",
            "gen_loss is 0.9077\n",
            "disc_loss is 1.0477\n",
            "current step is 29\n",
            "gen_loss is 0.8972\n",
            "disc_loss is 1.0700\n",
            "current step is 30\n",
            "gen_loss is 0.8801\n",
            "disc_loss is 1.0472\n",
            "current step is 31\n",
            "gen_loss is 0.8776\n",
            "disc_loss is 1.0417\n",
            "current step is 32\n",
            "gen_loss is 0.8635\n",
            "disc_loss is 1.0664\n",
            "current step is 33\n",
            "gen_loss is 0.8789\n",
            "disc_loss is 1.0800\n",
            "current step is 34\n",
            "gen_loss is 0.8477\n",
            "disc_loss is 1.1026\n",
            "current step is 35\n",
            "gen_loss is 0.8189\n",
            "disc_loss is 1.1567\n",
            "current step is 36\n",
            "gen_loss is 0.8037\n",
            "disc_loss is 1.1577\n",
            "current step is 37\n",
            "gen_loss is 0.7981\n",
            "disc_loss is 1.1648\n",
            "current step is 38\n",
            "gen_loss is 0.7665\n",
            "disc_loss is 1.2132\n",
            "current step is 39\n",
            "gen_loss is 0.7718\n",
            "disc_loss is 1.2372\n",
            "current step is 40\n",
            "gen_loss is 0.7663\n",
            "disc_loss is 1.2845\n",
            "current step is 41\n",
            "gen_loss is 0.7524\n",
            "disc_loss is 1.2793\n",
            "current step is 42\n",
            "gen_loss is 0.7086\n",
            "disc_loss is 1.3230\n",
            "current step is 43\n",
            "gen_loss is 0.6798\n",
            "disc_loss is 1.3947\n",
            "current step is 44\n",
            "gen_loss is 0.6580\n",
            "disc_loss is 1.4392\n",
            "current step is 45\n",
            "gen_loss is 0.6467\n",
            "disc_loss is 1.4824\n",
            "current step is 46\n",
            "gen_loss is 0.6185\n",
            "disc_loss is 1.6242\n",
            "current step is 47\n",
            "gen_loss is 0.6114\n",
            "disc_loss is 1.6460\n",
            "current step is 48\n",
            "gen_loss is 0.5822\n",
            "disc_loss is 1.6945\n",
            "current step is 49\n",
            "gen_loss is 0.5850\n",
            "disc_loss is 1.7475\n",
            "current step is 50\n",
            "gen_loss is 0.5835\n",
            "disc_loss is 1.7431\n",
            "current step is 51\n",
            "gen_loss is 0.5900\n",
            "disc_loss is 1.7404\n",
            "current step is 52\n",
            "gen_loss is 0.5747\n",
            "disc_loss is 1.7587\n",
            "current step is 53\n",
            "gen_loss is 0.5726\n",
            "disc_loss is 1.9063\n",
            "current step is 54\n",
            "gen_loss is 0.5733\n",
            "disc_loss is 1.9328\n",
            "current step is 55\n",
            "gen_loss is 0.5792\n",
            "disc_loss is 1.9246\n",
            "current step is 56\n",
            "gen_loss is 0.5780\n",
            "disc_loss is 1.9638\n",
            "current step is 57\n",
            "gen_loss is 0.5838\n",
            "disc_loss is 1.8665\n",
            "current step is 58\n",
            "gen_loss is 0.5893\n",
            "disc_loss is 1.9666\n",
            "current step is 59\n",
            "gen_loss is 0.5972\n",
            "disc_loss is 1.9474\n",
            "current step is 60\n",
            "gen_loss is 0.6171\n",
            "disc_loss is 1.8724\n",
            "current step is 61\n",
            "gen_loss is 0.6281\n",
            "disc_loss is 1.9320\n",
            "current step is 62\n",
            "gen_loss is 0.6301\n",
            "disc_loss is 1.8745\n",
            "current step is 63\n",
            "gen_loss is 0.6467\n",
            "disc_loss is 1.8913\n",
            "current step is 64\n",
            "gen_loss is 0.6593\n",
            "disc_loss is 1.7900\n",
            "current step is 65\n",
            "gen_loss is 0.6856\n",
            "disc_loss is 1.8068\n",
            "current step is 66\n",
            "gen_loss is 0.7128\n",
            "disc_loss is 1.6574\n",
            "current step is 67\n",
            "gen_loss is 0.7193\n",
            "disc_loss is 1.7140\n",
            "current step is 68\n",
            "gen_loss is 0.7311\n",
            "disc_loss is 1.7082\n",
            "current step is 69\n",
            "gen_loss is 0.7560\n",
            "disc_loss is 1.6528\n",
            "current step is 70\n",
            "gen_loss is 0.7751\n",
            "disc_loss is 1.5886\n",
            "current step is 71\n",
            "gen_loss is 0.7838\n",
            "disc_loss is 1.5205\n",
            "current step is 72\n",
            "gen_loss is 0.8079\n",
            "disc_loss is 1.4430\n",
            "current step is 73\n",
            "gen_loss is 0.8258\n",
            "disc_loss is 1.4318\n",
            "current step is 74\n",
            "gen_loss is 0.8225\n",
            "disc_loss is 1.4061\n",
            "current step is 75\n",
            "gen_loss is 0.8415\n",
            "disc_loss is 1.4124\n",
            "current step is 76\n",
            "gen_loss is 0.8608\n",
            "disc_loss is 1.3565\n",
            "current step is 77\n",
            "gen_loss is 0.8527\n",
            "disc_loss is 1.3173\n",
            "current step is 78\n",
            "gen_loss is 0.8754\n",
            "disc_loss is 1.3399\n",
            "current step is 79\n",
            "gen_loss is 0.8807\n",
            "disc_loss is 1.2617\n",
            "current step is 80\n",
            "gen_loss is 0.8768\n",
            "disc_loss is 1.2493\n",
            "current step is 81\n",
            "gen_loss is 0.8945\n",
            "disc_loss is 1.2441\n",
            "current step is 82\n",
            "gen_loss is 0.9111\n",
            "disc_loss is 1.2177\n",
            "current step is 83\n",
            "gen_loss is 0.9027\n",
            "disc_loss is 1.2095\n",
            "current step is 84\n",
            "gen_loss is 0.8961\n",
            "disc_loss is 1.2146\n",
            "current step is 85\n",
            "gen_loss is 0.9046\n",
            "disc_loss is 1.1810\n",
            "current step is 86\n",
            "gen_loss is 0.8932\n",
            "disc_loss is 1.2159\n",
            "current step is 87\n",
            "gen_loss is 0.8939\n",
            "disc_loss is 1.1401\n",
            "current step is 88\n",
            "gen_loss is 0.8874\n",
            "disc_loss is 1.1492\n",
            "current step is 89\n",
            "gen_loss is 0.8900\n",
            "disc_loss is 1.1557\n",
            "current step is 90\n",
            "gen_loss is 0.8744\n",
            "disc_loss is 1.1478\n",
            "current step is 91\n",
            "gen_loss is 0.8689\n",
            "disc_loss is 1.1473\n",
            "current step is 92\n",
            "gen_loss is 0.8561\n",
            "disc_loss is 1.1367\n",
            "current step is 93\n",
            "gen_loss is 0.8425\n",
            "disc_loss is 1.1459\n",
            "current step is 94\n",
            "gen_loss is 0.8312\n",
            "disc_loss is 1.1420\n",
            "current step is 95\n",
            "gen_loss is 0.8042\n",
            "disc_loss is 1.1596\n",
            "current step is 96\n",
            "gen_loss is 0.7796\n",
            "disc_loss is 1.1721\n",
            "current step is 97\n",
            "gen_loss is 0.7693\n",
            "disc_loss is 1.2075\n",
            "current step is 98\n",
            "gen_loss is 0.7426\n",
            "disc_loss is 1.2463\n",
            "current step is 99\n",
            "gen_loss is 0.7273\n",
            "disc_loss is 1.2416\n",
            "current step is 100\n",
            "gen_loss is 0.7086\n",
            "disc_loss is 1.2516\n",
            "current step is 101\n",
            "gen_loss is 0.6977\n",
            "disc_loss is 1.2893\n",
            "current step is 102\n",
            "gen_loss is 0.6844\n",
            "disc_loss is 1.3330\n",
            "current step is 103\n",
            "gen_loss is 0.6771\n",
            "disc_loss is 1.3171\n",
            "current step is 104\n",
            "gen_loss is 0.6649\n",
            "disc_loss is 1.4025\n",
            "current step is 105\n",
            "gen_loss is 0.6529\n",
            "disc_loss is 1.3946\n",
            "current step is 106\n",
            "gen_loss is 0.6612\n",
            "disc_loss is 1.3820\n",
            "current step is 107\n",
            "gen_loss is 0.6630\n",
            "disc_loss is 1.4776\n",
            "current step is 108\n",
            "gen_loss is 0.6543\n",
            "disc_loss is 1.4720\n",
            "current step is 109\n",
            "gen_loss is 0.6787\n",
            "disc_loss is 1.4939\n",
            "current step is 110\n",
            "gen_loss is 0.6872\n",
            "disc_loss is 1.5013\n",
            "current step is 111\n",
            "gen_loss is 0.6909\n",
            "disc_loss is 1.5150\n",
            "current step is 112\n",
            "gen_loss is 0.6927\n",
            "disc_loss is 1.4934\n",
            "current step is 113\n",
            "gen_loss is 0.7039\n",
            "disc_loss is 1.5030\n",
            "current step is 114\n",
            "gen_loss is 0.7017\n",
            "disc_loss is 1.5017\n",
            "current step is 115\n",
            "gen_loss is 0.7100\n",
            "disc_loss is 1.4632\n",
            "current step is 116\n",
            "gen_loss is 0.7126\n",
            "disc_loss is 1.5199\n",
            "current step is 117\n",
            "gen_loss is 0.7190\n",
            "disc_loss is 1.4977\n",
            "current step is 118\n",
            "gen_loss is 0.7186\n",
            "disc_loss is 1.5187\n",
            "current step is 119\n",
            "gen_loss is 0.7113\n",
            "disc_loss is 1.4693\n",
            "current step is 120\n",
            "gen_loss is 0.7086\n",
            "disc_loss is 1.5053\n",
            "current step is 121\n",
            "gen_loss is 0.6994\n",
            "disc_loss is 1.5022\n",
            "current step is 122\n",
            "gen_loss is 0.7059\n",
            "disc_loss is 1.5033\n",
            "current step is 123\n",
            "gen_loss is 0.6970\n",
            "disc_loss is 1.5350\n",
            "current step is 124\n",
            "gen_loss is 0.6948\n",
            "disc_loss is 1.4805\n",
            "current step is 125\n",
            "gen_loss is 0.7003\n",
            "disc_loss is 1.4670\n",
            "current step is 126\n",
            "gen_loss is 0.6955\n",
            "disc_loss is 1.4849\n",
            "current step is 127\n",
            "gen_loss is 0.6834\n",
            "disc_loss is 1.4615\n",
            "current step is 128\n",
            "gen_loss is 0.6803\n",
            "disc_loss is 1.4609\n",
            "current step is 129\n",
            "gen_loss is 0.6974\n",
            "disc_loss is 1.4704\n",
            "current step is 130\n",
            "gen_loss is 0.6941\n",
            "disc_loss is 1.4203\n",
            "current step is 131\n",
            "gen_loss is 0.7071\n",
            "disc_loss is 1.3932\n",
            "current step is 132\n",
            "gen_loss is 0.7266\n",
            "disc_loss is 1.4154\n",
            "current step is 133\n",
            "gen_loss is 0.7302\n",
            "disc_loss is 1.3827\n",
            "current step is 134\n",
            "gen_loss is 0.7413\n",
            "disc_loss is 1.3626\n",
            "current step is 135\n",
            "gen_loss is 0.7518\n",
            "disc_loss is 1.3426\n",
            "current step is 136\n",
            "gen_loss is 0.7843\n",
            "disc_loss is 1.3452\n",
            "current step is 137\n",
            "gen_loss is 0.7956\n",
            "disc_loss is 1.2970\n",
            "current step is 138\n",
            "gen_loss is 0.8199\n",
            "disc_loss is 1.2721\n",
            "current step is 139\n",
            "gen_loss is 0.8489\n",
            "disc_loss is 1.2474\n",
            "current step is 140\n",
            "gen_loss is 0.8555\n",
            "disc_loss is 1.2564\n",
            "current step is 141\n",
            "gen_loss is 0.8610\n",
            "disc_loss is 1.2790\n",
            "current step is 142\n",
            "gen_loss is 0.8497\n",
            "disc_loss is 1.2385\n",
            "current step is 143\n",
            "gen_loss is 0.8812\n",
            "disc_loss is 1.2074\n",
            "current step is 144\n",
            "gen_loss is 0.8583\n",
            "disc_loss is 1.2047\n",
            "current step is 145\n",
            "gen_loss is 0.8849\n",
            "disc_loss is 1.1848\n",
            "current step is 146\n",
            "gen_loss is 0.8771\n",
            "disc_loss is 1.1986\n",
            "current step is 147\n",
            "gen_loss is 0.8488\n",
            "disc_loss is 1.1951\n",
            "current step is 148\n",
            "gen_loss is 0.8432\n",
            "disc_loss is 1.1978\n",
            "current step is 149\n",
            "gen_loss is 0.8238\n",
            "disc_loss is 1.2123\n",
            "current step is 150\n",
            "gen_loss is 0.8198\n",
            "disc_loss is 1.1863\n",
            "current step is 151\n",
            "gen_loss is 0.8137\n",
            "disc_loss is 1.2001\n",
            "current step is 152\n",
            "gen_loss is 0.7952\n",
            "disc_loss is 1.2292\n",
            "current step is 153\n",
            "gen_loss is 0.7846\n",
            "disc_loss is 1.2215\n",
            "current step is 154\n",
            "gen_loss is 0.7926\n",
            "disc_loss is 1.2493\n",
            "current step is 155\n",
            "gen_loss is 0.8053\n",
            "disc_loss is 1.2294\n",
            "current step is 156\n",
            "gen_loss is 0.7817\n",
            "disc_loss is 1.2871\n",
            "current step is 157\n",
            "gen_loss is 0.8007\n",
            "disc_loss is 1.2602\n",
            "current step is 158\n",
            "gen_loss is 0.8114\n",
            "disc_loss is 1.2371\n",
            "current step is 159\n",
            "gen_loss is 0.8054\n",
            "disc_loss is 1.2883\n",
            "current step is 160\n",
            "gen_loss is 0.8250\n",
            "disc_loss is 1.2384\n",
            "current step is 161\n",
            "gen_loss is 0.8490\n",
            "disc_loss is 1.2666\n",
            "current step is 162\n",
            "gen_loss is 0.8632\n",
            "disc_loss is 1.2330\n",
            "current step is 163\n",
            "gen_loss is 0.8710\n",
            "disc_loss is 1.2342\n",
            "current step is 164\n",
            "gen_loss is 0.8916\n",
            "disc_loss is 1.2323\n",
            "current step is 165\n",
            "gen_loss is 0.9056\n",
            "disc_loss is 1.2128\n",
            "current step is 166\n",
            "gen_loss is 0.9156\n",
            "disc_loss is 1.2127\n",
            "current step is 167\n",
            "gen_loss is 0.9364\n",
            "disc_loss is 1.2178\n",
            "current step is 168\n",
            "gen_loss is 0.9302\n",
            "disc_loss is 1.2420\n",
            "current step is 169\n",
            "gen_loss is 0.9314\n",
            "disc_loss is 1.1700\n",
            "current step is 170\n",
            "gen_loss is 0.9305\n",
            "disc_loss is 1.2057\n",
            "current step is 171\n",
            "gen_loss is 0.9375\n",
            "disc_loss is 1.1356\n",
            "current step is 172\n",
            "gen_loss is 0.9470\n",
            "disc_loss is 1.1681\n",
            "current step is 173\n",
            "gen_loss is 0.9526\n",
            "disc_loss is 1.0916\n",
            "current step is 174\n",
            "gen_loss is 0.9620\n",
            "disc_loss is 1.0894\n",
            "current step is 175\n",
            "gen_loss is 0.9583\n",
            "disc_loss is 1.0916\n",
            "current step is 176\n",
            "gen_loss is 0.9664\n",
            "disc_loss is 1.0852\n",
            "current step is 177\n",
            "gen_loss is 0.9758\n",
            "disc_loss is 1.0960\n",
            "current step is 178\n",
            "gen_loss is 0.9848\n",
            "disc_loss is 1.1158\n",
            "current step is 179\n",
            "gen_loss is 0.9893\n",
            "disc_loss is 1.0928\n",
            "current step is 180\n",
            "gen_loss is 0.9964\n",
            "disc_loss is 1.1010\n",
            "current step is 181\n",
            "gen_loss is 1.0227\n",
            "disc_loss is 1.0308\n",
            "current step is 182\n",
            "gen_loss is 1.0344\n",
            "disc_loss is 1.0917\n",
            "current step is 183\n",
            "gen_loss is 1.0345\n",
            "disc_loss is 1.0983\n",
            "current step is 184\n",
            "gen_loss is 1.0514\n",
            "disc_loss is 1.0390\n",
            "current step is 185\n",
            "gen_loss is 1.0684\n",
            "disc_loss is 0.9970\n",
            "current step is 186\n",
            "gen_loss is 1.0708\n",
            "disc_loss is 1.0459\n",
            "current step is 187\n",
            "gen_loss is 1.0960\n",
            "disc_loss is 1.0230\n",
            "current step is 188\n",
            "gen_loss is 1.1251\n",
            "disc_loss is 1.0238\n",
            "current step is 189\n",
            "gen_loss is 1.1322\n",
            "disc_loss is 0.9704\n",
            "current step is 190\n",
            "gen_loss is 1.1542\n",
            "disc_loss is 0.9542\n",
            "current step is 191\n",
            "gen_loss is 1.1322\n",
            "disc_loss is 0.9826\n",
            "current step is 192\n",
            "gen_loss is 1.1516\n",
            "disc_loss is 0.9796\n",
            "current step is 193\n",
            "gen_loss is 1.1577\n",
            "disc_loss is 0.9178\n",
            "current step is 194\n",
            "gen_loss is 1.1293\n",
            "disc_loss is 0.9326\n",
            "current step is 195\n",
            "gen_loss is 1.1152\n",
            "disc_loss is 0.9363\n",
            "current step is 196\n",
            "gen_loss is 1.1016\n",
            "disc_loss is 0.9256\n",
            "current step is 197\n",
            "gen_loss is 1.0929\n",
            "disc_loss is 0.9551\n",
            "current step is 198\n",
            "gen_loss is 1.0754\n",
            "disc_loss is 0.9609\n",
            "current step is 199\n",
            "gen_loss is 1.0386\n",
            "disc_loss is 0.9800\n",
            "current step is 200\n",
            "gen_loss is 1.0028\n",
            "disc_loss is 1.0267\n",
            "current step is 201\n",
            "gen_loss is 0.9422\n",
            "disc_loss is 1.0355\n",
            "current step is 202\n",
            "gen_loss is 0.9305\n",
            "disc_loss is 0.9535\n",
            "current step is 203\n",
            "gen_loss is 0.9128\n",
            "disc_loss is 1.0442\n",
            "current step is 204\n",
            "gen_loss is 0.8683\n",
            "disc_loss is 1.0672\n",
            "current step is 205\n",
            "gen_loss is 0.8701\n",
            "disc_loss is 1.0845\n",
            "current step is 206\n",
            "gen_loss is 0.8374\n",
            "disc_loss is 1.0951\n",
            "current step is 207\n",
            "gen_loss is 0.8384\n",
            "disc_loss is 1.1487\n",
            "current step is 208\n",
            "gen_loss is 0.8296\n",
            "disc_loss is 1.1917\n",
            "current step is 209\n",
            "gen_loss is 0.8262\n",
            "disc_loss is 1.2377\n",
            "current step is 210\n",
            "gen_loss is 0.8176\n",
            "disc_loss is 1.2698\n",
            "current step is 211\n",
            "gen_loss is 0.8138\n",
            "disc_loss is 1.2220\n",
            "current step is 212\n",
            "gen_loss is 0.7777\n",
            "disc_loss is 1.2816\n",
            "current step is 213\n",
            "gen_loss is 0.7779\n",
            "disc_loss is 1.2974\n",
            "current step is 214\n",
            "gen_loss is 0.7709\n",
            "disc_loss is 1.3261\n",
            "current step is 215\n",
            "gen_loss is 0.7607\n",
            "disc_loss is 1.3849\n",
            "current step is 216\n",
            "gen_loss is 0.7858\n",
            "disc_loss is 1.2581\n",
            "current step is 217\n",
            "gen_loss is 0.7567\n",
            "disc_loss is 1.3375\n",
            "current step is 218\n",
            "gen_loss is 0.7479\n",
            "disc_loss is 1.3235\n",
            "current step is 219\n",
            "gen_loss is 0.7676\n",
            "disc_loss is 1.3233\n",
            "current step is 220\n",
            "gen_loss is 0.7444\n",
            "disc_loss is 1.3182\n",
            "current step is 221\n",
            "gen_loss is 0.7598\n",
            "disc_loss is 1.2524\n",
            "current step is 222\n",
            "gen_loss is 0.7887\n",
            "disc_loss is 1.2514\n",
            "current step is 223\n",
            "gen_loss is 0.8088\n",
            "disc_loss is 1.2177\n",
            "current step is 224\n",
            "gen_loss is 0.8428\n",
            "disc_loss is 1.1915\n",
            "current step is 225\n",
            "gen_loss is 0.8312\n",
            "disc_loss is 1.2081\n",
            "current step is 226\n",
            "gen_loss is 0.8521\n",
            "disc_loss is 1.1563\n",
            "current step is 227\n",
            "gen_loss is 0.8931\n",
            "disc_loss is 1.1788\n",
            "current step is 228\n",
            "gen_loss is 0.9109\n",
            "disc_loss is 1.1028\n",
            "current step is 229\n",
            "gen_loss is 0.9256\n",
            "disc_loss is 1.1032\n",
            "current step is 230\n",
            "gen_loss is 0.9523\n",
            "disc_loss is 1.0671\n",
            "current step is 231\n",
            "gen_loss is 0.9800\n",
            "disc_loss is 1.0214\n",
            "current step is 232\n",
            "gen_loss is 0.9952\n",
            "disc_loss is 1.0147\n",
            "current step is 233\n",
            "gen_loss is 1.0159\n",
            "disc_loss is 1.0427\n",
            "current step is 234\n",
            "gen_loss is 1.0412\n",
            "disc_loss is 0.9818\n",
            "Current epoch 2 is\n",
            "The train loss value for epoch 2 is 0.8259\n",
            "The test loss value for epoch 2 is 1.2799\n",
            "Time for epoch 2 is 1111.1607999801636 sec\n",
            "current step is 0\n",
            "gen_loss is 1.0797\n",
            "disc_loss is 0.9909\n",
            "current step is 1\n",
            "gen_loss is 1.0970\n",
            "disc_loss is 0.9610\n",
            "current step is 2\n",
            "gen_loss is 1.1029\n",
            "disc_loss is 0.9642\n",
            "current step is 3\n",
            "gen_loss is 1.1502\n",
            "disc_loss is 0.9382\n",
            "current step is 4\n",
            "gen_loss is 1.1574\n",
            "disc_loss is 0.9506\n",
            "current step is 5\n",
            "gen_loss is 1.1731\n",
            "disc_loss is 0.9431\n",
            "current step is 6\n",
            "gen_loss is 1.1917\n",
            "disc_loss is 0.9283\n",
            "current step is 7\n",
            "gen_loss is 1.2009\n",
            "disc_loss is 0.9592\n",
            "current step is 8\n",
            "gen_loss is 1.1959\n",
            "disc_loss is 0.9643\n",
            "current step is 9\n",
            "gen_loss is 1.1988\n",
            "disc_loss is 0.8882\n",
            "current step is 10\n",
            "gen_loss is 1.1853\n",
            "disc_loss is 0.8677\n",
            "current step is 11\n",
            "gen_loss is 1.1626\n",
            "disc_loss is 0.9969\n",
            "current step is 12\n",
            "gen_loss is 1.1472\n",
            "disc_loss is 0.8961\n",
            "current step is 13\n",
            "gen_loss is 1.1172\n",
            "disc_loss is 0.9512\n",
            "current step is 14\n",
            "gen_loss is 1.0625\n",
            "disc_loss is 0.9569\n",
            "current step is 15\n",
            "gen_loss is 1.0897\n",
            "disc_loss is 1.0031\n",
            "current step is 16\n",
            "gen_loss is 1.0204\n",
            "disc_loss is 1.1077\n",
            "current step is 17\n",
            "gen_loss is 0.9892\n",
            "disc_loss is 1.0958\n",
            "current step is 18\n",
            "gen_loss is 0.9548\n",
            "disc_loss is 1.1021\n",
            "current step is 19\n",
            "gen_loss is 0.9065\n",
            "disc_loss is 1.2074\n",
            "current step is 20\n",
            "gen_loss is 0.8827\n",
            "disc_loss is 1.2253\n",
            "current step is 21\n",
            "gen_loss is 0.8614\n",
            "disc_loss is 1.2094\n",
            "current step is 22\n",
            "gen_loss is 0.8591\n",
            "disc_loss is 1.2650\n",
            "current step is 23\n",
            "gen_loss is 0.8432\n",
            "disc_loss is 1.2594\n",
            "current step is 24\n",
            "gen_loss is 0.8262\n",
            "disc_loss is 1.3025\n",
            "current step is 25\n",
            "gen_loss is 0.8338\n",
            "disc_loss is 1.3768\n",
            "current step is 26\n",
            "gen_loss is 0.8211\n",
            "disc_loss is 1.4104\n",
            "current step is 27\n",
            "gen_loss is 0.8597\n",
            "disc_loss is 1.4905\n",
            "current step is 28\n",
            "gen_loss is 0.8529\n",
            "disc_loss is 1.4902\n",
            "current step is 29\n",
            "gen_loss is 0.8576\n",
            "disc_loss is 1.5557\n",
            "current step is 30\n",
            "gen_loss is 0.8668\n",
            "disc_loss is 1.4414\n",
            "current step is 31\n",
            "gen_loss is 0.8861\n",
            "disc_loss is 1.5442\n",
            "current step is 32\n",
            "gen_loss is 0.8822\n",
            "disc_loss is 1.5411\n",
            "current step is 33\n",
            "gen_loss is 0.8949\n",
            "disc_loss is 1.5363\n",
            "current step is 34\n",
            "gen_loss is 0.8914\n",
            "disc_loss is 1.5748\n",
            "current step is 35\n",
            "gen_loss is 0.8867\n",
            "disc_loss is 1.5216\n",
            "current step is 36\n",
            "gen_loss is 0.8757\n",
            "disc_loss is 1.5082\n",
            "current step is 37\n",
            "gen_loss is 0.8526\n",
            "disc_loss is 1.5796\n",
            "current step is 38\n",
            "gen_loss is 0.8333\n",
            "disc_loss is 1.5091\n",
            "current step is 39\n",
            "gen_loss is 0.8262\n",
            "disc_loss is 1.5860\n",
            "current step is 40\n",
            "gen_loss is 0.8153\n",
            "disc_loss is 1.5190\n",
            "current step is 41\n",
            "gen_loss is 0.8206\n",
            "disc_loss is 1.5196\n",
            "current step is 42\n",
            "gen_loss is 0.8106\n",
            "disc_loss is 1.5073\n",
            "current step is 43\n",
            "gen_loss is 0.7953\n",
            "disc_loss is 1.4695\n",
            "current step is 44\n",
            "gen_loss is 0.7919\n",
            "disc_loss is 1.5316\n",
            "current step is 45\n",
            "gen_loss is 0.8029\n",
            "disc_loss is 1.4684\n",
            "current step is 46\n",
            "gen_loss is 0.8118\n",
            "disc_loss is 1.5040\n",
            "current step is 47\n",
            "gen_loss is 0.8226\n",
            "disc_loss is 1.3932\n",
            "current step is 48\n",
            "gen_loss is 0.8349\n",
            "disc_loss is 1.4178\n",
            "current step is 49\n",
            "gen_loss is 0.8430\n",
            "disc_loss is 1.3736\n",
            "current step is 50\n",
            "gen_loss is 0.8438\n",
            "disc_loss is 1.3664\n",
            "current step is 51\n",
            "gen_loss is 0.8573\n",
            "disc_loss is 1.3335\n",
            "current step is 52\n",
            "gen_loss is 0.8691\n",
            "disc_loss is 1.3875\n",
            "current step is 53\n",
            "gen_loss is 0.8881\n",
            "disc_loss is 1.3011\n",
            "current step is 54\n",
            "gen_loss is 0.8772\n",
            "disc_loss is 1.2619\n",
            "current step is 55\n",
            "gen_loss is 0.8820\n",
            "disc_loss is 1.2743\n",
            "current step is 56\n",
            "gen_loss is 0.9009\n",
            "disc_loss is 1.2270\n",
            "current step is 57\n",
            "gen_loss is 0.8826\n",
            "disc_loss is 1.2619\n",
            "current step is 58\n",
            "gen_loss is 0.8776\n",
            "disc_loss is 1.2730\n",
            "current step is 59\n",
            "gen_loss is 0.8739\n",
            "disc_loss is 1.2743\n",
            "current step is 60\n",
            "gen_loss is 0.8986\n",
            "disc_loss is 1.2422\n",
            "current step is 61\n",
            "gen_loss is 0.8801\n",
            "disc_loss is 1.1912\n",
            "current step is 62\n",
            "gen_loss is 0.8754\n",
            "disc_loss is 1.2090\n",
            "current step is 63\n",
            "gen_loss is 0.8704\n",
            "disc_loss is 1.1411\n",
            "current step is 64\n",
            "gen_loss is 0.8621\n",
            "disc_loss is 1.1895\n",
            "current step is 65\n",
            "gen_loss is 0.8716\n",
            "disc_loss is 1.1744\n",
            "current step is 66\n",
            "gen_loss is 0.8849\n",
            "disc_loss is 1.1766\n",
            "current step is 67\n",
            "gen_loss is 0.8927\n",
            "disc_loss is 1.1404\n",
            "current step is 68\n",
            "gen_loss is 0.8826\n",
            "disc_loss is 1.1399\n",
            "current step is 69\n",
            "gen_loss is 0.8949\n",
            "disc_loss is 1.1347\n",
            "current step is 70\n",
            "gen_loss is 0.8979\n",
            "disc_loss is 1.1532\n",
            "current step is 71\n",
            "gen_loss is 0.9142\n",
            "disc_loss is 1.1483\n",
            "current step is 72\n",
            "gen_loss is 0.8953\n",
            "disc_loss is 1.1695\n",
            "current step is 73\n",
            "gen_loss is 0.8965\n",
            "disc_loss is 1.1621\n",
            "current step is 74\n",
            "gen_loss is 0.8914\n",
            "disc_loss is 1.1249\n",
            "current step is 75\n",
            "gen_loss is 0.8948\n",
            "disc_loss is 1.1281\n",
            "current step is 76\n",
            "gen_loss is 0.8931\n",
            "disc_loss is 1.1280\n",
            "current step is 77\n",
            "gen_loss is 0.8925\n",
            "disc_loss is 1.0892\n",
            "current step is 78\n",
            "gen_loss is 0.9058\n",
            "disc_loss is 1.0999\n",
            "current step is 79\n",
            "gen_loss is 0.9119\n",
            "disc_loss is 1.1131\n",
            "current step is 80\n",
            "gen_loss is 0.9133\n",
            "disc_loss is 1.0835\n",
            "current step is 81\n",
            "gen_loss is 0.9052\n",
            "disc_loss is 1.1050\n",
            "current step is 82\n",
            "gen_loss is 0.9270\n",
            "disc_loss is 1.0558\n",
            "current step is 83\n",
            "gen_loss is 0.9425\n",
            "disc_loss is 1.0679\n",
            "current step is 84\n",
            "gen_loss is 0.9530\n",
            "disc_loss is 1.0362\n",
            "current step is 85\n",
            "gen_loss is 0.9786\n",
            "disc_loss is 1.0380\n",
            "current step is 86\n",
            "gen_loss is 0.9841\n",
            "disc_loss is 1.0240\n",
            "current step is 87\n",
            "gen_loss is 0.9945\n",
            "disc_loss is 1.0287\n",
            "current step is 88\n",
            "gen_loss is 1.0241\n",
            "disc_loss is 1.0257\n",
            "current step is 89\n",
            "gen_loss is 1.0171\n",
            "disc_loss is 0.9991\n",
            "current step is 90\n",
            "gen_loss is 1.0314\n",
            "disc_loss is 0.9889\n",
            "current step is 91\n",
            "gen_loss is 1.0318\n",
            "disc_loss is 1.0052\n",
            "current step is 92\n",
            "gen_loss is 1.0439\n",
            "disc_loss is 1.0008\n",
            "current step is 93\n",
            "gen_loss is 1.0587\n",
            "disc_loss is 0.9595\n",
            "current step is 94\n",
            "gen_loss is 1.0360\n",
            "disc_loss is 0.9757\n",
            "current step is 95\n",
            "gen_loss is 1.0331\n",
            "disc_loss is 0.9641\n",
            "current step is 96\n",
            "gen_loss is 1.0325\n",
            "disc_loss is 0.9490\n",
            "current step is 97\n",
            "gen_loss is 1.0311\n",
            "disc_loss is 0.9312\n",
            "current step is 98\n",
            "gen_loss is 1.0401\n",
            "disc_loss is 0.9309\n",
            "current step is 99\n",
            "gen_loss is 1.0443\n",
            "disc_loss is 0.9021\n",
            "current step is 100\n",
            "gen_loss is 1.0333\n",
            "disc_loss is 0.9400\n",
            "current step is 101\n",
            "gen_loss is 1.0323\n",
            "disc_loss is 0.9225\n",
            "current step is 102\n",
            "gen_loss is 1.0723\n",
            "disc_loss is 0.9141\n",
            "current step is 103\n",
            "gen_loss is 1.0811\n",
            "disc_loss is 0.8949\n",
            "current step is 104\n",
            "gen_loss is 1.0835\n",
            "disc_loss is 0.9169\n",
            "current step is 105\n",
            "gen_loss is 1.1110\n",
            "disc_loss is 0.9180\n",
            "current step is 106\n",
            "gen_loss is 1.1174\n",
            "disc_loss is 0.9161\n",
            "current step is 107\n",
            "gen_loss is 1.0924\n",
            "disc_loss is 0.9273\n",
            "current step is 108\n",
            "gen_loss is 1.1060\n",
            "disc_loss is 0.9103\n",
            "current step is 109\n",
            "gen_loss is 1.0694\n",
            "disc_loss is 0.9006\n",
            "current step is 110\n",
            "gen_loss is 1.0953\n",
            "disc_loss is 0.9066\n",
            "current step is 111\n",
            "gen_loss is 1.0725\n",
            "disc_loss is 0.9645\n",
            "current step is 112\n",
            "gen_loss is 1.0341\n",
            "disc_loss is 0.9444\n",
            "current step is 113\n",
            "gen_loss is 1.0171\n",
            "disc_loss is 0.9475\n",
            "current step is 114\n",
            "gen_loss is 1.0249\n",
            "disc_loss is 0.9282\n",
            "current step is 115\n",
            "gen_loss is 1.0055\n",
            "disc_loss is 0.9872\n",
            "current step is 116\n",
            "gen_loss is 0.9881\n",
            "disc_loss is 0.9723\n",
            "current step is 117\n",
            "gen_loss is 0.9873\n",
            "disc_loss is 1.0076\n",
            "current step is 118\n",
            "gen_loss is 0.9557\n",
            "disc_loss is 1.0033\n",
            "current step is 119\n",
            "gen_loss is 0.9453\n",
            "disc_loss is 1.0821\n",
            "current step is 120\n",
            "gen_loss is 0.9460\n",
            "disc_loss is 1.0574\n",
            "current step is 121\n",
            "gen_loss is 0.9352\n",
            "disc_loss is 1.0902\n",
            "current step is 122\n",
            "gen_loss is 0.9376\n",
            "disc_loss is 1.1095\n",
            "current step is 123\n",
            "gen_loss is 0.8814\n",
            "disc_loss is 1.1506\n",
            "current step is 124\n",
            "gen_loss is 0.8617\n",
            "disc_loss is 1.1919\n",
            "current step is 125\n",
            "gen_loss is 0.8187\n",
            "disc_loss is 1.2722\n",
            "current step is 126\n",
            "gen_loss is 0.7842\n",
            "disc_loss is 1.2852\n",
            "current step is 127\n",
            "gen_loss is 0.7354\n",
            "disc_loss is 1.3493\n",
            "current step is 128\n",
            "gen_loss is 0.7036\n",
            "disc_loss is 1.3120\n",
            "current step is 129\n",
            "gen_loss is 0.6807\n",
            "disc_loss is 1.3802\n",
            "current step is 130\n",
            "gen_loss is 0.6641\n",
            "disc_loss is 1.4517\n",
            "current step is 131\n",
            "gen_loss is 0.6440\n",
            "disc_loss is 1.4530\n",
            "current step is 132\n",
            "gen_loss is 0.6401\n",
            "disc_loss is 1.4880\n",
            "current step is 133\n",
            "gen_loss is 0.6341\n",
            "disc_loss is 1.5350\n",
            "current step is 134\n",
            "gen_loss is 0.6386\n",
            "disc_loss is 1.5607\n",
            "current step is 135\n",
            "gen_loss is 0.6512\n",
            "disc_loss is 1.5733\n",
            "current step is 136\n",
            "gen_loss is 0.6507\n",
            "disc_loss is 1.5841\n",
            "current step is 137\n",
            "gen_loss is 0.6544\n",
            "disc_loss is 1.5789\n",
            "current step is 138\n",
            "gen_loss is 0.6468\n",
            "disc_loss is 1.5863\n",
            "current step is 139\n",
            "gen_loss is 0.6583\n",
            "disc_loss is 1.6171\n",
            "current step is 140\n",
            "gen_loss is 0.6295\n",
            "disc_loss is 1.6265\n",
            "current step is 141\n",
            "gen_loss is 0.6323\n",
            "disc_loss is 1.6495\n",
            "current step is 142\n",
            "gen_loss is 0.6352\n",
            "disc_loss is 1.5798\n",
            "current step is 143\n",
            "gen_loss is 0.6186\n",
            "disc_loss is 1.5917\n",
            "current step is 144\n",
            "gen_loss is 0.6351\n",
            "disc_loss is 1.5856\n",
            "current step is 145\n",
            "gen_loss is 0.6369\n",
            "disc_loss is 1.5524\n",
            "current step is 146\n",
            "gen_loss is 0.6552\n",
            "disc_loss is 1.5453\n",
            "current step is 147\n",
            "gen_loss is 0.6472\n",
            "disc_loss is 1.5449\n",
            "current step is 148\n",
            "gen_loss is 0.6649\n",
            "disc_loss is 1.5435\n",
            "current step is 149\n",
            "gen_loss is 0.6673\n",
            "disc_loss is 1.6213\n",
            "current step is 150\n",
            "gen_loss is 0.6749\n",
            "disc_loss is 1.5305\n",
            "current step is 151\n",
            "gen_loss is 0.6987\n",
            "disc_loss is 1.5327\n",
            "current step is 152\n",
            "gen_loss is 0.7048\n",
            "disc_loss is 1.5006\n",
            "current step is 153\n",
            "gen_loss is 0.7129\n",
            "disc_loss is 1.4840\n",
            "current step is 154\n",
            "gen_loss is 0.7151\n",
            "disc_loss is 1.5337\n",
            "current step is 155\n",
            "gen_loss is 0.7139\n",
            "disc_loss is 1.4130\n",
            "current step is 156\n",
            "gen_loss is 0.7253\n",
            "disc_loss is 1.4904\n",
            "current step is 157\n",
            "gen_loss is 0.7352\n",
            "disc_loss is 1.4129\n",
            "current step is 158\n",
            "gen_loss is 0.7678\n",
            "disc_loss is 1.3846\n",
            "current step is 159\n",
            "gen_loss is 0.7825\n",
            "disc_loss is 1.3611\n",
            "current step is 160\n",
            "gen_loss is 0.7885\n",
            "disc_loss is 1.3466\n",
            "current step is 161\n",
            "gen_loss is 0.8002\n",
            "disc_loss is 1.3622\n",
            "current step is 162\n",
            "gen_loss is 0.8019\n",
            "disc_loss is 1.3367\n",
            "current step is 163\n",
            "gen_loss is 0.8275\n",
            "disc_loss is 1.3121\n",
            "current step is 164\n",
            "gen_loss is 0.8495\n",
            "disc_loss is 1.3394\n",
            "current step is 165\n",
            "gen_loss is 0.8544\n",
            "disc_loss is 1.2700\n",
            "current step is 166\n",
            "gen_loss is 0.8300\n",
            "disc_loss is 1.3153\n",
            "current step is 167\n",
            "gen_loss is 0.8413\n",
            "disc_loss is 1.3157\n",
            "current step is 168\n",
            "gen_loss is 0.8743\n",
            "disc_loss is 1.2495\n",
            "current step is 169\n",
            "gen_loss is 0.8805\n",
            "disc_loss is 1.1669\n",
            "current step is 170\n",
            "gen_loss is 0.8733\n",
            "disc_loss is 1.2382\n",
            "current step is 171\n",
            "gen_loss is 0.8857\n",
            "disc_loss is 1.2133\n",
            "current step is 172\n",
            "gen_loss is 0.8790\n",
            "disc_loss is 1.2205\n",
            "current step is 173\n",
            "gen_loss is 0.9155\n",
            "disc_loss is 1.1545\n",
            "current step is 174\n",
            "gen_loss is 0.9231\n",
            "disc_loss is 1.1172\n",
            "current step is 175\n",
            "gen_loss is 0.9321\n",
            "disc_loss is 1.1188\n",
            "current step is 176\n",
            "gen_loss is 0.9405\n",
            "disc_loss is 1.1085\n",
            "current step is 177\n",
            "gen_loss is 0.9612\n",
            "disc_loss is 1.1287\n",
            "current step is 178\n",
            "gen_loss is 0.9807\n",
            "disc_loss is 1.1189\n",
            "current step is 179\n",
            "gen_loss is 0.9673\n",
            "disc_loss is 1.1255\n",
            "current step is 180\n",
            "gen_loss is 0.9936\n",
            "disc_loss is 1.0690\n",
            "current step is 181\n",
            "gen_loss is 0.9976\n",
            "disc_loss is 1.0711\n",
            "current step is 182\n",
            "gen_loss is 1.0323\n",
            "disc_loss is 1.0683\n",
            "current step is 183\n",
            "gen_loss is 1.0551\n",
            "disc_loss is 1.0459\n",
            "current step is 184\n",
            "gen_loss is 1.0493\n",
            "disc_loss is 1.0229\n",
            "current step is 185\n",
            "gen_loss is 1.0495\n",
            "disc_loss is 0.9695\n",
            "current step is 186\n",
            "gen_loss is 1.0573\n",
            "disc_loss is 0.9900\n",
            "current step is 187\n",
            "gen_loss is 1.0812\n",
            "disc_loss is 0.9928\n",
            "current step is 188\n",
            "gen_loss is 1.0950\n",
            "disc_loss is 0.9513\n",
            "current step is 189\n",
            "gen_loss is 1.0701\n",
            "disc_loss is 0.9595\n",
            "current step is 190\n",
            "gen_loss is 1.0747\n",
            "disc_loss is 0.9651\n",
            "current step is 191\n",
            "gen_loss is 1.0959\n",
            "disc_loss is 0.9425\n",
            "current step is 192\n",
            "gen_loss is 1.0888\n",
            "disc_loss is 0.9238\n",
            "current step is 193\n",
            "gen_loss is 1.0573\n",
            "disc_loss is 0.9615\n",
            "current step is 194\n",
            "gen_loss is 1.1228\n",
            "disc_loss is 0.9158\n",
            "current step is 195\n",
            "gen_loss is 1.1521\n",
            "disc_loss is 0.9216\n",
            "current step is 196\n",
            "gen_loss is 1.1190\n",
            "disc_loss is 0.9388\n",
            "current step is 197\n",
            "gen_loss is 1.1634\n",
            "disc_loss is 0.8973\n",
            "current step is 198\n",
            "gen_loss is 1.1741\n",
            "disc_loss is 0.9093\n",
            "current step is 199\n",
            "gen_loss is 1.1704\n",
            "disc_loss is 0.8611\n",
            "current step is 200\n",
            "gen_loss is 1.1955\n",
            "disc_loss is 0.8673\n",
            "current step is 201\n",
            "gen_loss is 1.1506\n",
            "disc_loss is 0.8774\n",
            "current step is 202\n",
            "gen_loss is 1.1505\n",
            "disc_loss is 0.8605\n",
            "current step is 203\n",
            "gen_loss is 1.1730\n",
            "disc_loss is 0.8528\n",
            "current step is 204\n",
            "gen_loss is 1.1574\n",
            "disc_loss is 0.8747\n",
            "current step is 205\n",
            "gen_loss is 1.1636\n",
            "disc_loss is 0.8793\n",
            "current step is 206\n",
            "gen_loss is 1.1999\n",
            "disc_loss is 0.8443\n",
            "current step is 207\n",
            "gen_loss is 1.2174\n",
            "disc_loss is 0.8239\n",
            "current step is 208\n",
            "gen_loss is 1.1865\n",
            "disc_loss is 0.8401\n",
            "current step is 209\n",
            "gen_loss is 1.2246\n",
            "disc_loss is 0.8463\n",
            "current step is 210\n",
            "gen_loss is 1.2159\n",
            "disc_loss is 0.8471\n",
            "current step is 211\n",
            "gen_loss is 1.2320\n",
            "disc_loss is 0.8346\n",
            "current step is 212\n",
            "gen_loss is 1.2631\n",
            "disc_loss is 0.8632\n",
            "current step is 213\n",
            "gen_loss is 1.2259\n",
            "disc_loss is 0.8656\n",
            "current step is 214\n",
            "gen_loss is 1.2104\n",
            "disc_loss is 0.8410\n",
            "current step is 215\n",
            "gen_loss is 1.1831\n",
            "disc_loss is 0.8903\n",
            "current step is 216\n",
            "gen_loss is 1.1678\n",
            "disc_loss is 0.8651\n",
            "current step is 217\n",
            "gen_loss is 1.1545\n",
            "disc_loss is 0.8571\n",
            "current step is 218\n",
            "gen_loss is 1.1257\n",
            "disc_loss is 0.8928\n",
            "current step is 219\n",
            "gen_loss is 1.0733\n",
            "disc_loss is 0.9450\n",
            "current step is 220\n",
            "gen_loss is 1.0843\n",
            "disc_loss is 0.9374\n",
            "current step is 221\n",
            "gen_loss is 1.0661\n",
            "disc_loss is 0.9604\n",
            "current step is 222\n",
            "gen_loss is 1.0547\n",
            "disc_loss is 0.9213\n",
            "current step is 223\n",
            "gen_loss is 1.0352\n",
            "disc_loss is 0.9901\n",
            "current step is 224\n",
            "gen_loss is 1.0410\n",
            "disc_loss is 0.9556\n",
            "current step is 225\n",
            "gen_loss is 1.0528\n",
            "disc_loss is 1.0210\n",
            "current step is 226\n",
            "gen_loss is 1.0447\n",
            "disc_loss is 1.0506\n",
            "current step is 227\n",
            "gen_loss is 1.0264\n",
            "disc_loss is 1.0667\n",
            "current step is 228\n",
            "gen_loss is 1.0189\n",
            "disc_loss is 1.1134\n",
            "current step is 229\n",
            "gen_loss is 0.9648\n",
            "disc_loss is 1.1031\n",
            "current step is 230\n",
            "gen_loss is 0.9484\n",
            "disc_loss is 1.1175\n",
            "current step is 231\n",
            "gen_loss is 0.8616\n",
            "disc_loss is 1.2348\n",
            "current step is 232\n",
            "gen_loss is 0.8518\n",
            "disc_loss is 1.1772\n",
            "current step is 233\n",
            "gen_loss is 0.8248\n",
            "disc_loss is 1.2186\n",
            "current step is 234\n",
            "gen_loss is 0.8076\n",
            "disc_loss is 1.2240\n",
            "Current epoch 3 is\n",
            "The train loss value for epoch 3 is 0.9409\n",
            "The test loss value for epoch 3 is 1.1647\n",
            "Time for epoch 3 is 1111.135221004486 sec\n",
            "current step is 0\n",
            "gen_loss is 0.8399\n",
            "disc_loss is 1.2140\n",
            "current step is 1\n",
            "gen_loss is 0.8204\n",
            "disc_loss is 1.3133\n",
            "current step is 2\n",
            "gen_loss is 0.7957\n",
            "disc_loss is 1.3660\n",
            "current step is 3\n",
            "gen_loss is 0.7821\n",
            "disc_loss is 1.4088\n",
            "current step is 4\n",
            "gen_loss is 0.8209\n",
            "disc_loss is 1.3510\n",
            "current step is 5\n",
            "gen_loss is 0.7509\n",
            "disc_loss is 1.4754\n",
            "current step is 6\n",
            "gen_loss is 0.7576\n",
            "disc_loss is 1.3665\n",
            "current step is 7\n",
            "gen_loss is 0.7253\n",
            "disc_loss is 1.4266\n",
            "current step is 8\n",
            "gen_loss is 0.7279\n",
            "disc_loss is 1.4766\n",
            "current step is 9\n",
            "gen_loss is 0.7418\n",
            "disc_loss is 1.4610\n",
            "current step is 10\n",
            "gen_loss is 0.7188\n",
            "disc_loss is 1.5720\n",
            "current step is 11\n",
            "gen_loss is 0.6759\n",
            "disc_loss is 1.5783\n",
            "current step is 12\n",
            "gen_loss is 0.6804\n",
            "disc_loss is 1.5424\n",
            "current step is 13\n",
            "gen_loss is 0.6564\n",
            "disc_loss is 1.5793\n",
            "current step is 14\n",
            "gen_loss is 0.6663\n",
            "disc_loss is 1.4784\n",
            "current step is 15\n",
            "gen_loss is 0.6419\n",
            "disc_loss is 1.4375\n",
            "current step is 16\n",
            "gen_loss is 0.6655\n",
            "disc_loss is 1.4901\n",
            "current step is 17\n",
            "gen_loss is 0.7188\n",
            "disc_loss is 1.4622\n",
            "current step is 18\n",
            "gen_loss is 0.7123\n",
            "disc_loss is 1.3846\n",
            "current step is 19\n",
            "gen_loss is 0.7316\n",
            "disc_loss is 1.4416\n",
            "current step is 20\n",
            "gen_loss is 0.7507\n",
            "disc_loss is 1.3989\n",
            "current step is 21\n",
            "gen_loss is 0.7742\n",
            "disc_loss is 1.3540\n",
            "current step is 22\n",
            "gen_loss is 0.7742\n",
            "disc_loss is 1.3542\n",
            "current step is 23\n",
            "gen_loss is 0.7987\n",
            "disc_loss is 1.3473\n",
            "current step is 24\n",
            "gen_loss is 0.7961\n",
            "disc_loss is 1.3102\n",
            "current step is 25\n",
            "gen_loss is 0.8110\n",
            "disc_loss is 1.2924\n",
            "current step is 26\n",
            "gen_loss is 0.7770\n",
            "disc_loss is 1.2502\n",
            "current step is 27\n",
            "gen_loss is 0.7772\n",
            "disc_loss is 1.2614\n",
            "current step is 28\n",
            "gen_loss is 0.7773\n",
            "disc_loss is 1.2958\n",
            "current step is 29\n",
            "gen_loss is 0.7856\n",
            "disc_loss is 1.2792\n",
            "current step is 30\n",
            "gen_loss is 0.7785\n",
            "disc_loss is 1.2431\n",
            "current step is 31\n",
            "gen_loss is 0.7742\n",
            "disc_loss is 1.2557\n",
            "current step is 32\n",
            "gen_loss is 0.7845\n",
            "disc_loss is 1.2270\n",
            "current step is 33\n",
            "gen_loss is 0.7987\n",
            "disc_loss is 1.2235\n",
            "current step is 34\n",
            "gen_loss is 0.8019\n",
            "disc_loss is 1.2279\n",
            "current step is 35\n",
            "gen_loss is 0.7776\n",
            "disc_loss is 1.2161\n",
            "current step is 36\n",
            "gen_loss is 0.8051\n",
            "disc_loss is 1.2157\n",
            "current step is 37\n",
            "gen_loss is 0.8253\n",
            "disc_loss is 1.2190\n",
            "current step is 38\n",
            "gen_loss is 0.8234\n",
            "disc_loss is 1.2447\n",
            "current step is 39\n",
            "gen_loss is 0.8326\n",
            "disc_loss is 1.2543\n",
            "current step is 40\n",
            "gen_loss is 0.8433\n",
            "disc_loss is 1.2230\n",
            "current step is 41\n",
            "gen_loss is 0.8637\n",
            "disc_loss is 1.2128\n",
            "current step is 42\n",
            "gen_loss is 0.8519\n",
            "disc_loss is 1.2552\n",
            "current step is 43\n",
            "gen_loss is 0.8563\n",
            "disc_loss is 1.2148\n",
            "current step is 44\n",
            "gen_loss is 0.8695\n",
            "disc_loss is 1.2191\n",
            "current step is 45\n",
            "gen_loss is 0.8622\n",
            "disc_loss is 1.2159\n",
            "current step is 46\n",
            "gen_loss is 0.8697\n",
            "disc_loss is 1.2506\n",
            "current step is 47\n",
            "gen_loss is 0.8679\n",
            "disc_loss is 1.1998\n",
            "current step is 48\n",
            "gen_loss is 0.8605\n",
            "disc_loss is 1.2542\n",
            "current step is 49\n",
            "gen_loss is 0.8859\n",
            "disc_loss is 1.2168\n",
            "current step is 50\n",
            "gen_loss is 0.8899\n",
            "disc_loss is 1.2071\n",
            "current step is 51\n",
            "gen_loss is 0.8831\n",
            "disc_loss is 1.2230\n",
            "current step is 52\n",
            "gen_loss is 0.8611\n",
            "disc_loss is 1.2339\n",
            "current step is 53\n",
            "gen_loss is 0.8797\n",
            "disc_loss is 1.2188\n",
            "current step is 54\n",
            "gen_loss is 0.8996\n",
            "disc_loss is 1.1771\n",
            "current step is 55\n",
            "gen_loss is 0.8858\n",
            "disc_loss is 1.1672\n",
            "current step is 56\n",
            "gen_loss is 0.8797\n",
            "disc_loss is 1.1995\n",
            "current step is 57\n",
            "gen_loss is 0.8893\n",
            "disc_loss is 1.1797\n",
            "current step is 58\n",
            "gen_loss is 0.9025\n",
            "disc_loss is 1.2349\n",
            "current step is 59\n",
            "gen_loss is 0.8904\n",
            "disc_loss is 1.2179\n",
            "current step is 60\n",
            "gen_loss is 0.9005\n",
            "disc_loss is 1.2073\n",
            "current step is 61\n",
            "gen_loss is 0.8667\n",
            "disc_loss is 1.2289\n",
            "current step is 62\n",
            "gen_loss is 0.8884\n",
            "disc_loss is 1.1832\n",
            "current step is 63\n",
            "gen_loss is 0.8884\n",
            "disc_loss is 1.1959\n",
            "current step is 64\n",
            "gen_loss is 0.8964\n",
            "disc_loss is 1.2067\n",
            "current step is 65\n",
            "gen_loss is 0.9221\n",
            "disc_loss is 1.2304\n",
            "current step is 66\n",
            "gen_loss is 0.9243\n",
            "disc_loss is 1.2010\n",
            "current step is 67\n",
            "gen_loss is 0.8995\n",
            "disc_loss is 1.2249\n",
            "current step is 68\n",
            "gen_loss is 0.8940\n",
            "disc_loss is 1.2353\n",
            "current step is 69\n",
            "gen_loss is 0.8986\n",
            "disc_loss is 1.1844\n",
            "current step is 70\n",
            "gen_loss is 0.9132\n",
            "disc_loss is 1.2223\n",
            "current step is 71\n",
            "gen_loss is 0.9250\n",
            "disc_loss is 1.2188\n",
            "current step is 72\n",
            "gen_loss is 0.9164\n",
            "disc_loss is 1.1520\n",
            "current step is 73\n",
            "gen_loss is 0.9151\n",
            "disc_loss is 1.1934\n",
            "current step is 74\n",
            "gen_loss is 0.9214\n",
            "disc_loss is 1.1599\n",
            "current step is 75\n",
            "gen_loss is 0.9033\n",
            "disc_loss is 1.1540\n",
            "current step is 76\n",
            "gen_loss is 0.9264\n",
            "disc_loss is 1.1229\n",
            "current step is 77\n",
            "gen_loss is 0.9212\n",
            "disc_loss is 1.0998\n",
            "current step is 78\n",
            "gen_loss is 0.9441\n",
            "disc_loss is 1.1139\n",
            "current step is 79\n",
            "gen_loss is 0.9447\n",
            "disc_loss is 1.0882\n",
            "current step is 80\n",
            "gen_loss is 0.9559\n",
            "disc_loss is 1.0886\n",
            "current step is 81\n",
            "gen_loss is 1.0118\n",
            "disc_loss is 1.0785\n",
            "current step is 82\n",
            "gen_loss is 0.9975\n",
            "disc_loss is 1.0701\n",
            "current step is 83\n",
            "gen_loss is 1.0171\n",
            "disc_loss is 1.0704\n",
            "current step is 84\n",
            "gen_loss is 1.0301\n",
            "disc_loss is 1.0337\n",
            "current step is 85\n",
            "gen_loss is 1.0272\n",
            "disc_loss is 1.0737\n",
            "current step is 86\n",
            "gen_loss is 1.0310\n",
            "disc_loss is 1.0490\n",
            "current step is 87\n",
            "gen_loss is 1.0145\n",
            "disc_loss is 1.0377\n",
            "current step is 88\n",
            "gen_loss is 1.0386\n",
            "disc_loss is 1.0590\n",
            "current step is 89\n",
            "gen_loss is 1.0064\n",
            "disc_loss is 1.0481\n",
            "current step is 90\n",
            "gen_loss is 1.0119\n",
            "disc_loss is 1.0613\n",
            "current step is 91\n",
            "gen_loss is 1.0213\n",
            "disc_loss is 1.0874\n",
            "current step is 92\n",
            "gen_loss is 0.9779\n",
            "disc_loss is 1.1067\n",
            "current step is 93\n",
            "gen_loss is 0.9612\n",
            "disc_loss is 1.1148\n",
            "current step is 94\n",
            "gen_loss is 0.9649\n",
            "disc_loss is 1.1012\n",
            "current step is 95\n",
            "gen_loss is 0.9414\n",
            "disc_loss is 1.1209\n",
            "current step is 96\n",
            "gen_loss is 0.9175\n",
            "disc_loss is 1.1274\n",
            "current step is 97\n",
            "gen_loss is 0.9097\n",
            "disc_loss is 1.1421\n",
            "current step is 98\n",
            "gen_loss is 0.9034\n",
            "disc_loss is 1.1808\n",
            "current step is 99\n",
            "gen_loss is 0.8785\n",
            "disc_loss is 1.2070\n",
            "current step is 100\n",
            "gen_loss is 0.8991\n",
            "disc_loss is 1.2744\n",
            "current step is 101\n",
            "gen_loss is 0.8684\n",
            "disc_loss is 1.2218\n",
            "current step is 102\n",
            "gen_loss is 0.9010\n",
            "disc_loss is 1.2670\n",
            "current step is 103\n",
            "gen_loss is 0.8449\n",
            "disc_loss is 1.3115\n",
            "current step is 104\n",
            "gen_loss is 0.8511\n",
            "disc_loss is 1.3012\n",
            "current step is 105\n",
            "gen_loss is 0.8292\n",
            "disc_loss is 1.3827\n",
            "current step is 106\n",
            "gen_loss is 0.8358\n",
            "disc_loss is 1.3473\n",
            "current step is 107\n",
            "gen_loss is 0.8060\n",
            "disc_loss is 1.3856\n",
            "current step is 108\n",
            "gen_loss is 0.8152\n",
            "disc_loss is 1.3765\n",
            "current step is 109\n",
            "gen_loss is 0.8052\n",
            "disc_loss is 1.3525\n",
            "current step is 110\n",
            "gen_loss is 0.8020\n",
            "disc_loss is 1.3860\n",
            "current step is 111\n",
            "gen_loss is 0.8013\n",
            "disc_loss is 1.4779\n",
            "current step is 112\n",
            "gen_loss is 0.7809\n",
            "disc_loss is 1.4424\n",
            "current step is 113\n",
            "gen_loss is 0.7703\n",
            "disc_loss is 1.4526\n",
            "current step is 114\n",
            "gen_loss is 0.7694\n",
            "disc_loss is 1.4222\n",
            "current step is 115\n",
            "gen_loss is 0.7729\n",
            "disc_loss is 1.4727\n",
            "current step is 116\n",
            "gen_loss is 0.7851\n",
            "disc_loss is 1.4596\n",
            "current step is 117\n",
            "gen_loss is 0.7950\n",
            "disc_loss is 1.4421\n",
            "current step is 118\n",
            "gen_loss is 0.7876\n",
            "disc_loss is 1.4699\n",
            "current step is 119\n",
            "gen_loss is 0.7679\n",
            "disc_loss is 1.5762\n",
            "current step is 120\n",
            "gen_loss is 0.7905\n",
            "disc_loss is 1.5145\n",
            "current step is 121\n",
            "gen_loss is 0.8136\n",
            "disc_loss is 1.4829\n",
            "current step is 122\n",
            "gen_loss is 0.8284\n",
            "disc_loss is 1.4339\n",
            "current step is 123\n",
            "gen_loss is 0.7802\n",
            "disc_loss is 1.4899\n",
            "current step is 124\n",
            "gen_loss is 0.7858\n",
            "disc_loss is 1.5240\n",
            "current step is 125\n",
            "gen_loss is 0.7821\n",
            "disc_loss is 1.5057\n",
            "current step is 126\n",
            "gen_loss is 0.7774\n",
            "disc_loss is 1.5274\n",
            "current step is 127\n",
            "gen_loss is 0.7600\n",
            "disc_loss is 1.5492\n",
            "current step is 128\n",
            "gen_loss is 0.7442\n",
            "disc_loss is 1.4880\n",
            "current step is 129\n",
            "gen_loss is 0.7813\n",
            "disc_loss is 1.4770\n",
            "current step is 130\n",
            "gen_loss is 0.7469\n",
            "disc_loss is 1.4969\n",
            "current step is 131\n",
            "gen_loss is 0.7465\n",
            "disc_loss is 1.5083\n",
            "current step is 132\n",
            "gen_loss is 0.7691\n",
            "disc_loss is 1.4852\n",
            "current step is 133\n",
            "gen_loss is 0.7762\n",
            "disc_loss is 1.4881\n",
            "current step is 134\n",
            "gen_loss is 0.7558\n",
            "disc_loss is 1.4669\n",
            "current step is 135\n",
            "gen_loss is 0.7693\n",
            "disc_loss is 1.4673\n",
            "current step is 136\n",
            "gen_loss is 0.7959\n",
            "disc_loss is 1.5058\n",
            "current step is 137\n",
            "gen_loss is 0.7651\n",
            "disc_loss is 1.4708\n",
            "current step is 138\n",
            "gen_loss is 0.7568\n",
            "disc_loss is 1.5015\n",
            "current step is 139\n",
            "gen_loss is 0.7803\n",
            "disc_loss is 1.4424\n",
            "current step is 140\n",
            "gen_loss is 0.7684\n",
            "disc_loss is 1.4662\n",
            "current step is 141\n",
            "gen_loss is 0.7349\n",
            "disc_loss is 1.4579\n",
            "current step is 142\n",
            "gen_loss is 0.7650\n",
            "disc_loss is 1.4160\n",
            "current step is 143\n",
            "gen_loss is 0.7533\n",
            "disc_loss is 1.4594\n",
            "current step is 144\n",
            "gen_loss is 0.7616\n",
            "disc_loss is 1.4042\n",
            "current step is 145\n",
            "gen_loss is 0.7658\n",
            "disc_loss is 1.3779\n",
            "current step is 146\n",
            "gen_loss is 0.7429\n",
            "disc_loss is 1.4671\n",
            "current step is 147\n",
            "gen_loss is 0.7663\n",
            "disc_loss is 1.3891\n",
            "current step is 148\n",
            "gen_loss is 0.7740\n",
            "disc_loss is 1.4050\n",
            "current step is 149\n",
            "gen_loss is 0.7688\n",
            "disc_loss is 1.4022\n",
            "current step is 150\n",
            "gen_loss is 0.7790\n",
            "disc_loss is 1.3664\n",
            "current step is 151\n",
            "gen_loss is 0.7765\n",
            "disc_loss is 1.3486\n",
            "current step is 152\n",
            "gen_loss is 0.7872\n",
            "disc_loss is 1.3145\n",
            "current step is 153\n",
            "gen_loss is 0.7827\n",
            "disc_loss is 1.3508\n",
            "current step is 154\n",
            "gen_loss is 0.7959\n",
            "disc_loss is 1.3918\n",
            "current step is 155\n",
            "gen_loss is 0.7988\n",
            "disc_loss is 1.2912\n",
            "current step is 156\n",
            "gen_loss is 0.7845\n",
            "disc_loss is 1.3297\n",
            "current step is 157\n",
            "gen_loss is 0.8014\n",
            "disc_loss is 1.3360\n",
            "current step is 158\n",
            "gen_loss is 0.7826\n",
            "disc_loss is 1.2731\n",
            "current step is 159\n",
            "gen_loss is 0.8084\n",
            "disc_loss is 1.2961\n",
            "current step is 160\n",
            "gen_loss is 0.8341\n",
            "disc_loss is 1.2928\n",
            "current step is 161\n",
            "gen_loss is 0.8399\n",
            "disc_loss is 1.2294\n",
            "current step is 162\n",
            "gen_loss is 0.8425\n",
            "disc_loss is 1.2611\n",
            "current step is 163\n",
            "gen_loss is 0.8466\n",
            "disc_loss is 1.2283\n",
            "current step is 164\n",
            "gen_loss is 0.8915\n",
            "disc_loss is 1.2315\n",
            "current step is 165\n",
            "gen_loss is 0.8779\n",
            "disc_loss is 1.1944\n",
            "current step is 166\n",
            "gen_loss is 0.8708\n",
            "disc_loss is 1.2124\n",
            "current step is 167\n",
            "gen_loss is 0.8614\n",
            "disc_loss is 1.2245\n",
            "current step is 168\n",
            "gen_loss is 0.8693\n",
            "disc_loss is 1.1805\n",
            "current step is 169\n",
            "gen_loss is 0.8762\n",
            "disc_loss is 1.1594\n",
            "current step is 170\n",
            "gen_loss is 0.8715\n",
            "disc_loss is 1.2353\n",
            "current step is 171\n",
            "gen_loss is 0.8863\n",
            "disc_loss is 1.1494\n",
            "current step is 172\n",
            "gen_loss is 0.8908\n",
            "disc_loss is 1.1729\n",
            "current step is 173\n",
            "gen_loss is 0.8763\n",
            "disc_loss is 1.1864\n",
            "current step is 174\n",
            "gen_loss is 0.8854\n",
            "disc_loss is 1.1659\n",
            "current step is 175\n",
            "gen_loss is 0.9031\n",
            "disc_loss is 1.1309\n",
            "current step is 176\n",
            "gen_loss is 0.8865\n",
            "disc_loss is 1.1246\n",
            "current step is 177\n",
            "gen_loss is 0.9098\n",
            "disc_loss is 1.1334\n",
            "current step is 178\n",
            "gen_loss is 0.9291\n",
            "disc_loss is 1.0859\n",
            "current step is 179\n",
            "gen_loss is 0.9330\n",
            "disc_loss is 1.1480\n",
            "current step is 180\n",
            "gen_loss is 0.9087\n",
            "disc_loss is 1.1553\n",
            "current step is 181\n",
            "gen_loss is 0.9396\n",
            "disc_loss is 1.1438\n",
            "current step is 182\n",
            "gen_loss is 0.9094\n",
            "disc_loss is 1.2163\n",
            "current step is 183\n",
            "gen_loss is 0.9181\n",
            "disc_loss is 1.1360\n",
            "current step is 184\n",
            "gen_loss is 0.9046\n",
            "disc_loss is 1.1689\n",
            "current step is 185\n",
            "gen_loss is 0.9221\n",
            "disc_loss is 1.1427\n",
            "current step is 186\n",
            "gen_loss is 0.9402\n",
            "disc_loss is 1.1047\n",
            "current step is 187\n",
            "gen_loss is 0.9333\n",
            "disc_loss is 1.1470\n",
            "current step is 188\n",
            "gen_loss is 0.9320\n",
            "disc_loss is 1.1233\n",
            "current step is 189\n",
            "gen_loss is 0.8985\n",
            "disc_loss is 1.1350\n",
            "current step is 190\n",
            "gen_loss is 0.9193\n",
            "disc_loss is 1.1300\n",
            "current step is 191\n",
            "gen_loss is 0.9125\n",
            "disc_loss is 1.1510\n",
            "current step is 192\n",
            "gen_loss is 0.9135\n",
            "disc_loss is 1.1616\n",
            "current step is 193\n",
            "gen_loss is 0.9279\n",
            "disc_loss is 1.1720\n",
            "current step is 194\n",
            "gen_loss is 0.9184\n",
            "disc_loss is 1.1401\n",
            "current step is 195\n",
            "gen_loss is 0.8984\n",
            "disc_loss is 1.1366\n",
            "current step is 196\n",
            "gen_loss is 0.9024\n",
            "disc_loss is 1.1826\n",
            "current step is 197\n",
            "gen_loss is 0.9061\n",
            "disc_loss is 1.1495\n",
            "current step is 198\n",
            "gen_loss is 0.8985\n",
            "disc_loss is 1.1627\n",
            "current step is 199\n",
            "gen_loss is 0.9094\n",
            "disc_loss is 1.1036\n",
            "current step is 200\n",
            "gen_loss is 0.9178\n",
            "disc_loss is 1.1541\n",
            "current step is 201\n",
            "gen_loss is 0.9142\n",
            "disc_loss is 1.1389\n",
            "current step is 202\n",
            "gen_loss is 0.9123\n",
            "disc_loss is 1.1713\n",
            "current step is 203\n",
            "gen_loss is 0.9053\n",
            "disc_loss is 1.1145\n",
            "current step is 204\n",
            "gen_loss is 0.8841\n",
            "disc_loss is 1.1722\n",
            "current step is 205\n",
            "gen_loss is 0.8839\n",
            "disc_loss is 1.1717\n",
            "current step is 206\n",
            "gen_loss is 0.8873\n",
            "disc_loss is 1.1499\n",
            "current step is 207\n",
            "gen_loss is 0.8815\n",
            "disc_loss is 1.1607\n",
            "current step is 208\n",
            "gen_loss is 0.8842\n",
            "disc_loss is 1.1800\n",
            "current step is 209\n",
            "gen_loss is 0.8990\n",
            "disc_loss is 1.1610\n",
            "current step is 210\n",
            "gen_loss is 0.8755\n",
            "disc_loss is 1.1712\n",
            "current step is 211\n",
            "gen_loss is 0.8650\n",
            "disc_loss is 1.2423\n",
            "current step is 212\n",
            "gen_loss is 0.8760\n",
            "disc_loss is 1.2223\n",
            "current step is 213\n",
            "gen_loss is 0.8368\n",
            "disc_loss is 1.2268\n",
            "current step is 214\n",
            "gen_loss is 0.8406\n",
            "disc_loss is 1.1981\n",
            "current step is 215\n",
            "gen_loss is 0.8440\n",
            "disc_loss is 1.2586\n",
            "current step is 216\n",
            "gen_loss is 0.8054\n",
            "disc_loss is 1.2668\n",
            "current step is 217\n",
            "gen_loss is 0.8393\n",
            "disc_loss is 1.1939\n",
            "current step is 218\n",
            "gen_loss is 0.8059\n",
            "disc_loss is 1.3212\n",
            "current step is 219\n",
            "gen_loss is 0.8013\n",
            "disc_loss is 1.2573\n",
            "current step is 220\n",
            "gen_loss is 0.7785\n",
            "disc_loss is 1.3136\n",
            "current step is 221\n",
            "gen_loss is 0.7628\n",
            "disc_loss is 1.2850\n",
            "current step is 222\n",
            "gen_loss is 0.7593\n",
            "disc_loss is 1.2848\n",
            "current step is 223\n",
            "gen_loss is 0.7568\n",
            "disc_loss is 1.3102\n",
            "current step is 224\n",
            "gen_loss is 0.7646\n",
            "disc_loss is 1.3250\n",
            "current step is 225\n",
            "gen_loss is 0.7656\n",
            "disc_loss is 1.3395\n",
            "current step is 226\n",
            "gen_loss is 0.7875\n",
            "disc_loss is 1.3648\n",
            "current step is 227\n",
            "gen_loss is 0.7679\n",
            "disc_loss is 1.3463\n",
            "current step is 228\n",
            "gen_loss is 0.7532\n",
            "disc_loss is 1.3902\n",
            "current step is 229\n",
            "gen_loss is 0.7518\n",
            "disc_loss is 1.3613\n",
            "current step is 230\n",
            "gen_loss is 0.7243\n",
            "disc_loss is 1.3815\n",
            "current step is 231\n",
            "gen_loss is 0.7025\n",
            "disc_loss is 1.4525\n",
            "current step is 232\n",
            "gen_loss is 0.6951\n",
            "disc_loss is 1.3988\n",
            "current step is 233\n",
            "gen_loss is 0.6817\n",
            "disc_loss is 1.4229\n",
            "current step is 234\n",
            "gen_loss is 0.6850\n",
            "disc_loss is 1.4229\n",
            "Current epoch 4 is\n",
            "The train loss value for epoch 4 is 0.8402\n",
            "The test loss value for epoch 4 is 1.2780\n",
            "Time for epoch 4 is 1085.0655055046082 sec\n",
            "current step is 0\n",
            "gen_loss is 0.6889\n",
            "disc_loss is 1.3873\n",
            "current step is 1\n",
            "gen_loss is 0.6979\n",
            "disc_loss is 1.4717\n",
            "current step is 2\n",
            "gen_loss is 0.6977\n",
            "disc_loss is 1.4635\n",
            "current step is 3\n",
            "gen_loss is 0.7098\n",
            "disc_loss is 1.5177\n",
            "current step is 4\n",
            "gen_loss is 0.7017\n",
            "disc_loss is 1.4283\n",
            "current step is 5\n",
            "gen_loss is 0.7095\n",
            "disc_loss is 1.5042\n",
            "current step is 6\n",
            "gen_loss is 0.6946\n",
            "disc_loss is 1.4741\n",
            "current step is 7\n",
            "gen_loss is 0.6975\n",
            "disc_loss is 1.5112\n",
            "current step is 8\n",
            "gen_loss is 0.6670\n",
            "disc_loss is 1.5729\n",
            "current step is 9\n",
            "gen_loss is 0.6835\n",
            "disc_loss is 1.5308\n",
            "current step is 10\n",
            "gen_loss is 0.6756\n",
            "disc_loss is 1.5834\n",
            "current step is 11\n",
            "gen_loss is 0.6490\n",
            "disc_loss is 1.5515\n",
            "current step is 12\n",
            "gen_loss is 0.6333\n",
            "disc_loss is 1.5550\n",
            "current step is 13\n",
            "gen_loss is 0.6110\n",
            "disc_loss is 1.5889\n",
            "current step is 14\n",
            "gen_loss is 0.6077\n",
            "disc_loss is 1.5633\n",
            "current step is 15\n",
            "gen_loss is 0.5978\n",
            "disc_loss is 1.5469\n",
            "current step is 16\n",
            "gen_loss is 0.5992\n",
            "disc_loss is 1.5428\n",
            "current step is 17\n",
            "gen_loss is 0.6123\n",
            "disc_loss is 1.5262\n",
            "current step is 18\n",
            "gen_loss is 0.6381\n",
            "disc_loss is 1.5336\n",
            "current step is 19\n",
            "gen_loss is 0.6271\n",
            "disc_loss is 1.5498\n",
            "current step is 20\n",
            "gen_loss is 0.6622\n",
            "disc_loss is 1.5424\n",
            "current step is 21\n",
            "gen_loss is 0.6679\n",
            "disc_loss is 1.5190\n",
            "current step is 22\n",
            "gen_loss is 0.7039\n",
            "disc_loss is 1.5395\n",
            "current step is 23\n",
            "gen_loss is 0.6974\n",
            "disc_loss is 1.5046\n",
            "current step is 24\n",
            "gen_loss is 0.6852\n",
            "disc_loss is 1.5012\n",
            "current step is 25\n",
            "gen_loss is 0.6981\n",
            "disc_loss is 1.5082\n",
            "current step is 26\n",
            "gen_loss is 0.6870\n",
            "disc_loss is 1.5013\n",
            "current step is 27\n",
            "gen_loss is 0.6751\n",
            "disc_loss is 1.4844\n",
            "current step is 28\n",
            "gen_loss is 0.6835\n",
            "disc_loss is 1.4990\n",
            "current step is 29\n",
            "gen_loss is 0.6958\n",
            "disc_loss is 1.4466\n",
            "current step is 30\n",
            "gen_loss is 0.6966\n",
            "disc_loss is 1.4152\n",
            "current step is 31\n",
            "gen_loss is 0.7096\n",
            "disc_loss is 1.4327\n",
            "current step is 32\n",
            "gen_loss is 0.7082\n",
            "disc_loss is 1.4279\n",
            "current step is 33\n",
            "gen_loss is 0.7238\n",
            "disc_loss is 1.4095\n",
            "current step is 34\n",
            "gen_loss is 0.7189\n",
            "disc_loss is 1.4019\n",
            "current step is 35\n",
            "gen_loss is 0.7354\n",
            "disc_loss is 1.3938\n",
            "current step is 36\n",
            "gen_loss is 0.7609\n",
            "disc_loss is 1.3603\n",
            "current step is 37\n",
            "gen_loss is 0.7602\n",
            "disc_loss is 1.3587\n",
            "current step is 38\n",
            "gen_loss is 0.7819\n",
            "disc_loss is 1.3204\n",
            "current step is 39\n",
            "gen_loss is 0.7698\n",
            "disc_loss is 1.3849\n",
            "current step is 40\n",
            "gen_loss is 0.8052\n",
            "disc_loss is 1.3197\n",
            "current step is 41\n",
            "gen_loss is 0.8084\n",
            "disc_loss is 1.2794\n",
            "current step is 42\n",
            "gen_loss is 0.8213\n",
            "disc_loss is 1.2981\n",
            "current step is 43\n",
            "gen_loss is 0.8292\n",
            "disc_loss is 1.2726\n",
            "current step is 44\n",
            "gen_loss is 0.8197\n",
            "disc_loss is 1.2778\n",
            "current step is 45\n",
            "gen_loss is 0.8363\n",
            "disc_loss is 1.2750\n",
            "current step is 46\n",
            "gen_loss is 0.8280\n",
            "disc_loss is 1.2548\n",
            "current step is 47\n",
            "gen_loss is 0.8356\n",
            "disc_loss is 1.2529\n",
            "current step is 48\n",
            "gen_loss is 0.8447\n",
            "disc_loss is 1.2562\n",
            "current step is 49\n",
            "gen_loss is 0.8498\n",
            "disc_loss is 1.2314\n",
            "current step is 50\n",
            "gen_loss is 0.8524\n",
            "disc_loss is 1.2289\n",
            "current step is 51\n",
            "gen_loss is 0.8433\n",
            "disc_loss is 1.2209\n",
            "current step is 52\n",
            "gen_loss is 0.8475\n",
            "disc_loss is 1.2161\n",
            "current step is 53\n",
            "gen_loss is 0.8259\n",
            "disc_loss is 1.2376\n",
            "current step is 54\n",
            "gen_loss is 0.8199\n",
            "disc_loss is 1.2452\n",
            "current step is 55\n",
            "gen_loss is 0.8184\n",
            "disc_loss is 1.2242\n",
            "current step is 56\n",
            "gen_loss is 0.8174\n",
            "disc_loss is 1.2423\n",
            "current step is 57\n",
            "gen_loss is 0.8325\n",
            "disc_loss is 1.2355\n",
            "current step is 58\n",
            "gen_loss is 0.8182\n",
            "disc_loss is 1.2517\n",
            "current step is 59\n",
            "gen_loss is 0.8331\n",
            "disc_loss is 1.2587\n",
            "current step is 60\n",
            "gen_loss is 0.8180\n",
            "disc_loss is 1.2570\n",
            "current step is 61\n",
            "gen_loss is 0.8198\n",
            "disc_loss is 1.2655\n",
            "current step is 62\n",
            "gen_loss is 0.8253\n",
            "disc_loss is 1.2704\n",
            "current step is 63\n",
            "gen_loss is 0.8230\n",
            "disc_loss is 1.2928\n",
            "current step is 64\n",
            "gen_loss is 0.8278\n",
            "disc_loss is 1.2934\n",
            "current step is 65\n",
            "gen_loss is 0.8231\n",
            "disc_loss is 1.3058\n",
            "current step is 66\n",
            "gen_loss is 0.8190\n",
            "disc_loss is 1.3485\n",
            "current step is 67\n",
            "gen_loss is 0.8124\n",
            "disc_loss is 1.3229\n",
            "current step is 68\n",
            "gen_loss is 0.8082\n",
            "disc_loss is 1.3242\n",
            "current step is 69\n",
            "gen_loss is 0.8202\n",
            "disc_loss is 1.3395\n",
            "current step is 70\n",
            "gen_loss is 0.8039\n",
            "disc_loss is 1.3434\n",
            "current step is 71\n",
            "gen_loss is 0.7831\n",
            "disc_loss is 1.3884\n",
            "current step is 72\n",
            "gen_loss is 0.7787\n",
            "disc_loss is 1.3685\n",
            "current step is 73\n",
            "gen_loss is 0.7643\n",
            "disc_loss is 1.3713\n",
            "current step is 74\n",
            "gen_loss is 0.7532\n",
            "disc_loss is 1.3658\n",
            "current step is 75\n",
            "gen_loss is 0.7130\n",
            "disc_loss is 1.3980\n",
            "current step is 76\n",
            "gen_loss is 0.7409\n",
            "disc_loss is 1.3549\n",
            "current step is 77\n",
            "gen_loss is 0.7244\n",
            "disc_loss is 1.3649\n",
            "current step is 78\n",
            "gen_loss is 0.7176\n",
            "disc_loss is 1.4079\n",
            "current step is 79\n",
            "gen_loss is 0.7389\n",
            "disc_loss is 1.3826\n",
            "current step is 80\n",
            "gen_loss is 0.7386\n",
            "disc_loss is 1.3892\n",
            "current step is 81\n",
            "gen_loss is 0.7429\n",
            "disc_loss is 1.3975\n",
            "current step is 82\n",
            "gen_loss is 0.7475\n",
            "disc_loss is 1.3799\n",
            "current step is 83\n",
            "gen_loss is 0.7530\n",
            "disc_loss is 1.3699\n",
            "current step is 84\n",
            "gen_loss is 0.7418\n",
            "disc_loss is 1.3873\n",
            "current step is 85\n",
            "gen_loss is 0.7338\n",
            "disc_loss is 1.3829\n",
            "current step is 86\n",
            "gen_loss is 0.7501\n",
            "disc_loss is 1.3888\n",
            "current step is 87\n",
            "gen_loss is 0.7711\n",
            "disc_loss is 1.3822\n",
            "current step is 88\n",
            "gen_loss is 0.7554\n",
            "disc_loss is 1.4174\n",
            "current step is 89\n",
            "gen_loss is 0.7614\n",
            "disc_loss is 1.3799\n",
            "current step is 90\n",
            "gen_loss is 0.7605\n",
            "disc_loss is 1.3808\n",
            "current step is 91\n",
            "gen_loss is 0.7642\n",
            "disc_loss is 1.3742\n",
            "current step is 92\n",
            "gen_loss is 0.7491\n",
            "disc_loss is 1.3925\n",
            "current step is 93\n",
            "gen_loss is 0.7382\n",
            "disc_loss is 1.3819\n",
            "current step is 94\n",
            "gen_loss is 0.7463\n",
            "disc_loss is 1.4167\n",
            "current step is 95\n",
            "gen_loss is 0.7429\n",
            "disc_loss is 1.4170\n",
            "current step is 96\n",
            "gen_loss is 0.7269\n",
            "disc_loss is 1.3808\n",
            "current step is 97\n",
            "gen_loss is 0.7256\n",
            "disc_loss is 1.3841\n",
            "current step is 98\n",
            "gen_loss is 0.7138\n",
            "disc_loss is 1.3870\n",
            "current step is 99\n",
            "gen_loss is 0.7423\n",
            "disc_loss is 1.3801\n",
            "current step is 100\n",
            "gen_loss is 0.7446\n",
            "disc_loss is 1.3974\n",
            "current step is 101\n",
            "gen_loss is 0.7345\n",
            "disc_loss is 1.4090\n",
            "current step is 102\n",
            "gen_loss is 0.7482\n",
            "disc_loss is 1.3872\n",
            "current step is 103\n",
            "gen_loss is 0.7468\n",
            "disc_loss is 1.4348\n",
            "current step is 104\n",
            "gen_loss is 0.7518\n",
            "disc_loss is 1.3723\n",
            "current step is 105\n",
            "gen_loss is 0.7485\n",
            "disc_loss is 1.4328\n",
            "current step is 106\n",
            "gen_loss is 0.7545\n",
            "disc_loss is 1.4132\n",
            "current step is 107\n",
            "gen_loss is 0.7241\n",
            "disc_loss is 1.3974\n",
            "current step is 108\n",
            "gen_loss is 0.7437\n",
            "disc_loss is 1.4278\n",
            "current step is 109\n",
            "gen_loss is 0.7245\n",
            "disc_loss is 1.3931\n",
            "current step is 110\n",
            "gen_loss is 0.7386\n",
            "disc_loss is 1.4044\n",
            "current step is 111\n",
            "gen_loss is 0.7293\n",
            "disc_loss is 1.4182\n",
            "current step is 112\n",
            "gen_loss is 0.7427\n",
            "disc_loss is 1.3923\n",
            "current step is 113\n",
            "gen_loss is 0.7178\n",
            "disc_loss is 1.4311\n",
            "current step is 114\n",
            "gen_loss is 0.7208\n",
            "disc_loss is 1.4281\n",
            "current step is 115\n",
            "gen_loss is 0.7374\n",
            "disc_loss is 1.4034\n",
            "current step is 116\n",
            "gen_loss is 0.7377\n",
            "disc_loss is 1.4114\n",
            "current step is 117\n",
            "gen_loss is 0.7337\n",
            "disc_loss is 1.4050\n",
            "current step is 118\n",
            "gen_loss is 0.7433\n",
            "disc_loss is 1.4058\n",
            "current step is 119\n",
            "gen_loss is 0.7424\n",
            "disc_loss is 1.3872\n",
            "current step is 120\n",
            "gen_loss is 0.7436\n",
            "disc_loss is 1.3768\n",
            "current step is 121\n",
            "gen_loss is 0.7528\n",
            "disc_loss is 1.3871\n",
            "current step is 122\n",
            "gen_loss is 0.7826\n",
            "disc_loss is 1.3546\n",
            "current step is 123\n",
            "gen_loss is 0.7833\n",
            "disc_loss is 1.3498\n",
            "current step is 124\n",
            "gen_loss is 0.7825\n",
            "disc_loss is 1.3507\n",
            "current step is 125\n",
            "gen_loss is 0.7714\n",
            "disc_loss is 1.3629\n",
            "current step is 126\n",
            "gen_loss is 0.7841\n",
            "disc_loss is 1.3488\n",
            "current step is 127\n",
            "gen_loss is 0.7913\n",
            "disc_loss is 1.3492\n",
            "current step is 128\n",
            "gen_loss is 0.7718\n",
            "disc_loss is 1.3411\n",
            "current step is 129\n",
            "gen_loss is 0.7779\n",
            "disc_loss is 1.3109\n",
            "current step is 130\n",
            "gen_loss is 0.7748\n",
            "disc_loss is 1.3090\n",
            "current step is 131\n",
            "gen_loss is 0.7874\n",
            "disc_loss is 1.3100\n",
            "current step is 132\n",
            "gen_loss is 0.7699\n",
            "disc_loss is 1.3152\n",
            "current step is 133\n",
            "gen_loss is 0.7973\n",
            "disc_loss is 1.2853\n",
            "current step is 134\n",
            "gen_loss is 0.8116\n",
            "disc_loss is 1.2852\n",
            "current step is 135\n",
            "gen_loss is 0.8184\n",
            "disc_loss is 1.2446\n",
            "current step is 136\n",
            "gen_loss is 0.8325\n",
            "disc_loss is 1.2551\n",
            "current step is 137\n",
            "gen_loss is 0.8177\n",
            "disc_loss is 1.2617\n",
            "current step is 138\n",
            "gen_loss is 0.8246\n",
            "disc_loss is 1.2828\n",
            "current step is 139\n",
            "gen_loss is 0.8599\n",
            "disc_loss is 1.2312\n",
            "current step is 140\n",
            "gen_loss is 0.8297\n",
            "disc_loss is 1.2649\n",
            "current step is 141\n",
            "gen_loss is 0.8300\n",
            "disc_loss is 1.2220\n",
            "current step is 142\n",
            "gen_loss is 0.8449\n",
            "disc_loss is 1.2035\n",
            "current step is 143\n",
            "gen_loss is 0.8220\n",
            "disc_loss is 1.2368\n",
            "current step is 144\n",
            "gen_loss is 0.8314\n",
            "disc_loss is 1.2285\n",
            "current step is 145\n",
            "gen_loss is 0.8324\n",
            "disc_loss is 1.2080\n",
            "current step is 146\n",
            "gen_loss is 0.8290\n",
            "disc_loss is 1.2309\n",
            "current step is 147\n",
            "gen_loss is 0.8230\n",
            "disc_loss is 1.2195\n",
            "current step is 148\n",
            "gen_loss is 0.8149\n",
            "disc_loss is 1.2130\n",
            "current step is 149\n",
            "gen_loss is 0.7998\n",
            "disc_loss is 1.2206\n",
            "current step is 150\n",
            "gen_loss is 0.8161\n",
            "disc_loss is 1.2019\n",
            "current step is 151\n",
            "gen_loss is 0.8102\n",
            "disc_loss is 1.2327\n",
            "current step is 152\n",
            "gen_loss is 0.8025\n",
            "disc_loss is 1.2777\n",
            "current step is 153\n",
            "gen_loss is 0.7932\n",
            "disc_loss is 1.2577\n",
            "current step is 154\n",
            "gen_loss is 0.7770\n",
            "disc_loss is 1.2334\n",
            "current step is 155\n",
            "gen_loss is 0.7887\n",
            "disc_loss is 1.2729\n",
            "current step is 156\n",
            "gen_loss is 0.7846\n",
            "disc_loss is 1.2310\n",
            "current step is 157\n",
            "gen_loss is 0.7896\n",
            "disc_loss is 1.2566\n",
            "current step is 158\n",
            "gen_loss is 0.7717\n",
            "disc_loss is 1.2718\n",
            "current step is 159\n",
            "gen_loss is 0.7745\n",
            "disc_loss is 1.2432\n",
            "current step is 160\n",
            "gen_loss is 0.7740\n",
            "disc_loss is 1.3207\n",
            "current step is 161\n",
            "gen_loss is 0.7527\n",
            "disc_loss is 1.3244\n",
            "current step is 162\n",
            "gen_loss is 0.7601\n",
            "disc_loss is 1.3037\n",
            "current step is 163\n",
            "gen_loss is 0.7296\n",
            "disc_loss is 1.3478\n",
            "current step is 164\n",
            "gen_loss is 0.7397\n",
            "disc_loss is 1.3637\n",
            "current step is 165\n",
            "gen_loss is 0.7252\n",
            "disc_loss is 1.3642\n",
            "current step is 166\n",
            "gen_loss is 0.7173\n",
            "disc_loss is 1.4100\n",
            "current step is 167\n",
            "gen_loss is 0.6908\n",
            "disc_loss is 1.4214\n",
            "current step is 168\n",
            "gen_loss is 0.6961\n",
            "disc_loss is 1.4024\n",
            "current step is 169\n",
            "gen_loss is 0.6695\n",
            "disc_loss is 1.4416\n",
            "current step is 170\n",
            "gen_loss is 0.6872\n",
            "disc_loss is 1.4311\n",
            "current step is 171\n",
            "gen_loss is 0.6644\n",
            "disc_loss is 1.4620\n",
            "current step is 172\n",
            "gen_loss is 0.6699\n",
            "disc_loss is 1.4637\n",
            "current step is 173\n",
            "gen_loss is 0.6743\n",
            "disc_loss is 1.4984\n",
            "current step is 174\n",
            "gen_loss is 0.6709\n",
            "disc_loss is 1.4851\n",
            "current step is 175\n",
            "gen_loss is 0.6563\n",
            "disc_loss is 1.4910\n",
            "current step is 176\n",
            "gen_loss is 0.6549\n",
            "disc_loss is 1.4810\n",
            "current step is 177\n",
            "gen_loss is 0.6531\n",
            "disc_loss is 1.4645\n",
            "current step is 178\n",
            "gen_loss is 0.6659\n",
            "disc_loss is 1.4560\n",
            "current step is 179\n",
            "gen_loss is 0.6754\n",
            "disc_loss is 1.5027\n",
            "current step is 180\n",
            "gen_loss is 0.6810\n",
            "disc_loss is 1.4984\n",
            "current step is 181\n",
            "gen_loss is 0.6769\n",
            "disc_loss is 1.4794\n",
            "current step is 182\n",
            "gen_loss is 0.6857\n",
            "disc_loss is 1.4803\n",
            "current step is 183\n",
            "gen_loss is 0.6989\n",
            "disc_loss is 1.4644\n",
            "current step is 184\n",
            "gen_loss is 0.7041\n",
            "disc_loss is 1.4892\n",
            "current step is 185\n",
            "gen_loss is 0.7232\n",
            "disc_loss is 1.4636\n",
            "current step is 186\n",
            "gen_loss is 0.7129\n",
            "disc_loss is 1.4716\n",
            "current step is 187\n",
            "gen_loss is 0.7240\n",
            "disc_loss is 1.4603\n",
            "current step is 188\n",
            "gen_loss is 0.7163\n",
            "disc_loss is 1.4754\n",
            "current step is 189\n",
            "gen_loss is 0.7297\n",
            "disc_loss is 1.4415\n",
            "current step is 190\n",
            "gen_loss is 0.7219\n",
            "disc_loss is 1.4371\n",
            "current step is 191\n",
            "gen_loss is 0.7136\n",
            "disc_loss is 1.4754\n",
            "current step is 192\n",
            "gen_loss is 0.7156\n",
            "disc_loss is 1.4530\n",
            "current step is 193\n",
            "gen_loss is 0.7278\n",
            "disc_loss is 1.4548\n",
            "current step is 194\n",
            "gen_loss is 0.7344\n",
            "disc_loss is 1.4478\n",
            "current step is 195\n",
            "gen_loss is 0.7423\n",
            "disc_loss is 1.4144\n",
            "current step is 196\n",
            "gen_loss is 0.7418\n",
            "disc_loss is 1.4600\n",
            "current step is 197\n",
            "gen_loss is 0.7465\n",
            "disc_loss is 1.4437\n",
            "current step is 198\n",
            "gen_loss is 0.7454\n",
            "disc_loss is 1.3988\n",
            "current step is 199\n",
            "gen_loss is 0.7576\n",
            "disc_loss is 1.3962\n",
            "current step is 200\n",
            "gen_loss is 0.7631\n",
            "disc_loss is 1.3821\n",
            "current step is 201\n",
            "gen_loss is 0.7555\n",
            "disc_loss is 1.4046\n",
            "current step is 202\n",
            "gen_loss is 0.7586\n",
            "disc_loss is 1.4585\n",
            "current step is 203\n",
            "gen_loss is 0.7596\n",
            "disc_loss is 1.3478\n",
            "current step is 204\n",
            "gen_loss is 0.7698\n",
            "disc_loss is 1.3916\n",
            "current step is 205\n",
            "gen_loss is 0.7743\n",
            "disc_loss is 1.3933\n",
            "current step is 206\n",
            "gen_loss is 0.7773\n",
            "disc_loss is 1.3451\n",
            "current step is 207\n",
            "gen_loss is 0.7906\n",
            "disc_loss is 1.3539\n",
            "current step is 208\n",
            "gen_loss is 0.8020\n",
            "disc_loss is 1.3485\n",
            "current step is 209\n",
            "gen_loss is 0.8047\n",
            "disc_loss is 1.3336\n",
            "current step is 210\n",
            "gen_loss is 0.8264\n",
            "disc_loss is 1.2888\n",
            "current step is 211\n",
            "gen_loss is 0.8391\n",
            "disc_loss is 1.3268\n",
            "current step is 212\n",
            "gen_loss is 0.8303\n",
            "disc_loss is 1.2889\n",
            "current step is 213\n",
            "gen_loss is 0.8431\n",
            "disc_loss is 1.2758\n",
            "current step is 214\n",
            "gen_loss is 0.8465\n",
            "disc_loss is 1.2459\n",
            "current step is 215\n",
            "gen_loss is 0.8612\n",
            "disc_loss is 1.2461\n",
            "current step is 216\n",
            "gen_loss is 0.8480\n",
            "disc_loss is 1.2490\n",
            "current step is 217\n",
            "gen_loss is 0.8668\n",
            "disc_loss is 1.2025\n",
            "current step is 218\n",
            "gen_loss is 0.8774\n",
            "disc_loss is 1.2685\n",
            "current step is 219\n",
            "gen_loss is 0.9032\n",
            "disc_loss is 1.1860\n",
            "current step is 220\n",
            "gen_loss is 0.8967\n",
            "disc_loss is 1.2344\n",
            "current step is 221\n",
            "gen_loss is 0.9027\n",
            "disc_loss is 1.1799\n",
            "current step is 222\n",
            "gen_loss is 0.9095\n",
            "disc_loss is 1.1774\n",
            "current step is 223\n",
            "gen_loss is 0.9206\n",
            "disc_loss is 1.1689\n",
            "current step is 224\n",
            "gen_loss is 0.9447\n",
            "disc_loss is 1.1674\n",
            "current step is 225\n",
            "gen_loss is 0.9476\n",
            "disc_loss is 1.1201\n",
            "current step is 226\n",
            "gen_loss is 0.9551\n",
            "disc_loss is 1.1542\n",
            "current step is 227\n",
            "gen_loss is 0.9741\n",
            "disc_loss is 1.0690\n",
            "current step is 228\n",
            "gen_loss is 0.9621\n",
            "disc_loss is 1.1414\n",
            "current step is 229\n",
            "gen_loss is 0.9952\n",
            "disc_loss is 1.0422\n",
            "current step is 230\n",
            "gen_loss is 0.9766\n",
            "disc_loss is 1.0796\n",
            "current step is 231\n",
            "gen_loss is 0.9946\n",
            "disc_loss is 1.1016\n",
            "current step is 232\n",
            "gen_loss is 1.0007\n",
            "disc_loss is 1.0699\n",
            "current step is 233\n",
            "gen_loss is 0.9615\n",
            "disc_loss is 1.0307\n",
            "current step is 234\n",
            "gen_loss is 0.9592\n",
            "disc_loss is 1.1574\n",
            "Current epoch 5 is\n",
            "The train loss value for epoch 5 is 0.7673\n",
            "The test loss value for epoch 5 is 1.3572\n",
            "Time for epoch 5 is 1084.9673383235931 sec\n",
            "current step is 0\n",
            "gen_loss is 0.9849\n",
            "disc_loss is 1.0173\n",
            "current step is 1\n",
            "gen_loss is 0.9849\n",
            "disc_loss is 1.0896\n",
            "current step is 2\n",
            "gen_loss is 0.9784\n",
            "disc_loss is 1.0494\n",
            "current step is 3\n",
            "gen_loss is 1.0029\n",
            "disc_loss is 1.0931\n",
            "current step is 4\n",
            "gen_loss is 0.9713\n",
            "disc_loss is 1.0595\n",
            "current step is 5\n",
            "gen_loss is 0.9943\n",
            "disc_loss is 1.1094\n",
            "current step is 6\n",
            "gen_loss is 0.9599\n",
            "disc_loss is 1.0803\n",
            "current step is 7\n",
            "gen_loss is 0.9613\n",
            "disc_loss is 1.0664\n",
            "current step is 8\n",
            "gen_loss is 0.9437\n",
            "disc_loss is 1.1523\n",
            "current step is 9\n",
            "gen_loss is 0.9202\n",
            "disc_loss is 1.1438\n",
            "current step is 10\n",
            "gen_loss is 0.9088\n",
            "disc_loss is 1.1907\n",
            "current step is 11\n",
            "gen_loss is 0.8876\n",
            "disc_loss is 1.1380\n",
            "current step is 12\n",
            "gen_loss is 0.8753\n",
            "disc_loss is 1.2262\n",
            "current step is 13\n",
            "gen_loss is 0.8358\n",
            "disc_loss is 1.1777\n",
            "current step is 14\n",
            "gen_loss is 0.8292\n",
            "disc_loss is 1.2568\n",
            "current step is 15\n",
            "gen_loss is 0.8122\n",
            "disc_loss is 1.2066\n",
            "current step is 16\n",
            "gen_loss is 0.8099\n",
            "disc_loss is 1.2465\n",
            "current step is 17\n",
            "gen_loss is 0.7843\n",
            "disc_loss is 1.2862\n",
            "current step is 18\n",
            "gen_loss is 0.7741\n",
            "disc_loss is 1.2987\n",
            "current step is 19\n",
            "gen_loss is 0.7746\n",
            "disc_loss is 1.3143\n",
            "current step is 20\n",
            "gen_loss is 0.7619\n",
            "disc_loss is 1.3359\n",
            "current step is 21\n",
            "gen_loss is 0.7916\n",
            "disc_loss is 1.3156\n",
            "current step is 22\n",
            "gen_loss is 0.7673\n",
            "disc_loss is 1.3512\n",
            "current step is 23\n",
            "gen_loss is 0.7592\n",
            "disc_loss is 1.3980\n",
            "current step is 24\n",
            "gen_loss is 0.7632\n",
            "disc_loss is 1.3816\n",
            "current step is 25\n",
            "gen_loss is 0.7374\n",
            "disc_loss is 1.4120\n",
            "current step is 26\n",
            "gen_loss is 0.7436\n",
            "disc_loss is 1.3742\n",
            "current step is 27\n",
            "gen_loss is 0.7031\n",
            "disc_loss is 1.3636\n",
            "current step is 28\n",
            "gen_loss is 0.7164\n",
            "disc_loss is 1.4001\n",
            "current step is 29\n",
            "gen_loss is 0.6837\n",
            "disc_loss is 1.4230\n",
            "current step is 30\n",
            "gen_loss is 0.6750\n",
            "disc_loss is 1.4256\n",
            "current step is 31\n",
            "gen_loss is 0.7082\n",
            "disc_loss is 1.4455\n",
            "current step is 32\n",
            "gen_loss is 0.6965\n",
            "disc_loss is 1.4243\n",
            "current step is 33\n",
            "gen_loss is 0.6935\n",
            "disc_loss is 1.3960\n",
            "current step is 34\n",
            "gen_loss is 0.6959\n",
            "disc_loss is 1.4221\n",
            "current step is 35\n",
            "gen_loss is 0.6860\n",
            "disc_loss is 1.4256\n",
            "current step is 36\n",
            "gen_loss is 0.7071\n",
            "disc_loss is 1.4109\n",
            "current step is 37\n",
            "gen_loss is 0.7124\n",
            "disc_loss is 1.4118\n",
            "current step is 38\n",
            "gen_loss is 0.7210\n",
            "disc_loss is 1.4037\n",
            "current step is 39\n",
            "gen_loss is 0.7209\n",
            "disc_loss is 1.4179\n",
            "current step is 40\n",
            "gen_loss is 0.7287\n",
            "disc_loss is 1.3684\n",
            "current step is 41\n",
            "gen_loss is 0.7403\n",
            "disc_loss is 1.3788\n",
            "current step is 42\n",
            "gen_loss is 0.7448\n",
            "disc_loss is 1.4319\n",
            "current step is 43\n",
            "gen_loss is 0.7581\n",
            "disc_loss is 1.3437\n",
            "current step is 44\n",
            "gen_loss is 0.7605\n",
            "disc_loss is 1.3383\n",
            "current step is 45\n",
            "gen_loss is 0.7711\n",
            "disc_loss is 1.3251\n",
            "current step is 46\n",
            "gen_loss is 0.7933\n",
            "disc_loss is 1.3172\n",
            "current step is 47\n",
            "gen_loss is 0.7837\n",
            "disc_loss is 1.3177\n",
            "current step is 48\n",
            "gen_loss is 0.8139\n",
            "disc_loss is 1.3009\n",
            "current step is 49\n",
            "gen_loss is 0.8099\n",
            "disc_loss is 1.2969\n",
            "current step is 50\n",
            "gen_loss is 0.8257\n",
            "disc_loss is 1.2973\n",
            "current step is 51\n",
            "gen_loss is 0.8398\n",
            "disc_loss is 1.2649\n",
            "current step is 52\n",
            "gen_loss is 0.8583\n",
            "disc_loss is 1.2351\n",
            "current step is 53\n",
            "gen_loss is 0.8535\n",
            "disc_loss is 1.2781\n",
            "current step is 54\n",
            "gen_loss is 0.8560\n",
            "disc_loss is 1.2514\n",
            "current step is 55\n",
            "gen_loss is 0.8623\n",
            "disc_loss is 1.1893\n",
            "current step is 56\n",
            "gen_loss is 0.8853\n",
            "disc_loss is 1.2068\n",
            "current step is 57\n",
            "gen_loss is 0.8857\n",
            "disc_loss is 1.1807\n",
            "current step is 58\n",
            "gen_loss is 0.9096\n",
            "disc_loss is 1.1863\n",
            "current step is 59\n",
            "gen_loss is 0.9191\n",
            "disc_loss is 1.1663\n",
            "current step is 60\n",
            "gen_loss is 0.9380\n",
            "disc_loss is 1.1256\n",
            "current step is 61\n",
            "gen_loss is 0.9710\n",
            "disc_loss is 1.1296\n",
            "current step is 62\n",
            "gen_loss is 0.9724\n",
            "disc_loss is 1.0961\n",
            "current step is 63\n",
            "gen_loss is 1.0121\n",
            "disc_loss is 1.0969\n",
            "current step is 64\n",
            "gen_loss is 1.0085\n",
            "disc_loss is 1.0749\n",
            "current step is 65\n",
            "gen_loss is 1.0400\n",
            "disc_loss is 1.0665\n",
            "current step is 66\n",
            "gen_loss is 1.0377\n",
            "disc_loss is 1.0535\n",
            "current step is 67\n",
            "gen_loss is 1.0633\n",
            "disc_loss is 1.0525\n",
            "current step is 68\n",
            "gen_loss is 1.0781\n",
            "disc_loss is 1.0352\n",
            "current step is 69\n",
            "gen_loss is 1.0568\n",
            "disc_loss is 1.0102\n",
            "current step is 70\n",
            "gen_loss is 1.0711\n",
            "disc_loss is 1.0083\n",
            "current step is 71\n",
            "gen_loss is 1.0795\n",
            "disc_loss is 0.9802\n",
            "current step is 72\n",
            "gen_loss is 1.0674\n",
            "disc_loss is 0.9716\n",
            "current step is 73\n",
            "gen_loss is 1.0850\n",
            "disc_loss is 0.9507\n",
            "current step is 74\n",
            "gen_loss is 1.0904\n",
            "disc_loss is 0.9461\n",
            "current step is 75\n",
            "gen_loss is 1.0764\n",
            "disc_loss is 0.9730\n",
            "current step is 76\n",
            "gen_loss is 1.0472\n",
            "disc_loss is 0.9171\n",
            "current step is 77\n",
            "gen_loss is 1.0554\n",
            "disc_loss is 0.9250\n",
            "current step is 78\n",
            "gen_loss is 1.0813\n",
            "disc_loss is 0.9524\n",
            "current step is 79\n",
            "gen_loss is 1.1052\n",
            "disc_loss is 0.9286\n",
            "current step is 80\n",
            "gen_loss is 1.0875\n",
            "disc_loss is 0.9749\n",
            "current step is 81\n",
            "gen_loss is 1.0721\n",
            "disc_loss is 0.9803\n",
            "current step is 82\n",
            "gen_loss is 1.0999\n",
            "disc_loss is 0.9723\n",
            "current step is 83\n",
            "gen_loss is 1.0546\n",
            "disc_loss is 0.9733\n",
            "current step is 84\n",
            "gen_loss is 1.0525\n",
            "disc_loss is 1.0034\n",
            "current step is 85\n",
            "gen_loss is 1.0764\n",
            "disc_loss is 1.0255\n",
            "current step is 86\n",
            "gen_loss is 1.0576\n",
            "disc_loss is 1.0550\n",
            "current step is 87\n",
            "gen_loss is 0.9995\n",
            "disc_loss is 1.0226\n",
            "current step is 88\n",
            "gen_loss is 1.0033\n",
            "disc_loss is 1.0575\n",
            "current step is 89\n",
            "gen_loss is 0.9789\n",
            "disc_loss is 1.1167\n",
            "current step is 90\n",
            "gen_loss is 0.9332\n",
            "disc_loss is 1.1048\n",
            "current step is 91\n",
            "gen_loss is 0.9650\n",
            "disc_loss is 1.1379\n",
            "current step is 92\n",
            "gen_loss is 0.9457\n",
            "disc_loss is 1.1510\n",
            "current step is 93\n",
            "gen_loss is 0.8910\n",
            "disc_loss is 1.2465\n",
            "current step is 94\n",
            "gen_loss is 0.8652\n",
            "disc_loss is 1.2962\n",
            "current step is 95\n",
            "gen_loss is 0.8562\n",
            "disc_loss is 1.3119\n",
            "current step is 96\n",
            "gen_loss is 0.8366\n",
            "disc_loss is 1.3563\n",
            "current step is 97\n",
            "gen_loss is 0.8158\n",
            "disc_loss is 1.4170\n",
            "current step is 98\n",
            "gen_loss is 0.7783\n",
            "disc_loss is 1.4519\n",
            "current step is 99\n",
            "gen_loss is 0.7421\n",
            "disc_loss is 1.4941\n",
            "current step is 100\n",
            "gen_loss is 0.6904\n",
            "disc_loss is 1.5819\n",
            "current step is 101\n",
            "gen_loss is 0.6321\n",
            "disc_loss is 1.6566\n",
            "current step is 102\n",
            "gen_loss is 0.6895\n",
            "disc_loss is 1.6569\n",
            "current step is 103\n",
            "gen_loss is 0.6828\n",
            "disc_loss is 1.6701\n",
            "current step is 104\n",
            "gen_loss is 0.6345\n",
            "disc_loss is 1.7854\n",
            "current step is 105\n",
            "gen_loss is 0.5877\n",
            "disc_loss is 1.8214\n",
            "current step is 106\n",
            "gen_loss is 0.5759\n",
            "disc_loss is 1.8463\n",
            "current step is 107\n",
            "gen_loss is 0.5525\n",
            "disc_loss is 1.8970\n",
            "current step is 108\n",
            "gen_loss is 0.5180\n",
            "disc_loss is 1.9303\n",
            "current step is 109\n",
            "gen_loss is 0.4999\n",
            "disc_loss is 2.0058\n",
            "current step is 110\n",
            "gen_loss is 0.4447\n",
            "disc_loss is 2.1424\n",
            "current step is 111\n",
            "gen_loss is 0.4748\n",
            "disc_loss is 2.0935\n",
            "current step is 112\n",
            "gen_loss is 0.4608\n",
            "disc_loss is 2.0812\n",
            "current step is 113\n",
            "gen_loss is 0.4752\n",
            "disc_loss is 2.0813\n",
            "current step is 114\n",
            "gen_loss is 0.4282\n",
            "disc_loss is 2.0896\n",
            "current step is 115\n",
            "gen_loss is 0.4626\n",
            "disc_loss is 2.0136\n",
            "current step is 116\n",
            "gen_loss is 0.4410\n",
            "disc_loss is 2.1587\n",
            "current step is 117\n",
            "gen_loss is 0.4439\n",
            "disc_loss is 2.1082\n",
            "current step is 118\n",
            "gen_loss is 0.4504\n",
            "disc_loss is 2.0513\n",
            "current step is 119\n",
            "gen_loss is 0.4792\n",
            "disc_loss is 1.9498\n",
            "current step is 120\n",
            "gen_loss is 0.4731\n",
            "disc_loss is 1.9556\n",
            "current step is 121\n",
            "gen_loss is 0.5012\n",
            "disc_loss is 1.9577\n",
            "current step is 122\n",
            "gen_loss is 0.5186\n",
            "disc_loss is 1.8986\n",
            "current step is 123\n",
            "gen_loss is 0.5324\n",
            "disc_loss is 1.8540\n",
            "current step is 124\n",
            "gen_loss is 0.5582\n",
            "disc_loss is 1.7986\n",
            "current step is 125\n",
            "gen_loss is 0.5931\n",
            "disc_loss is 1.7816\n",
            "current step is 126\n",
            "gen_loss is 0.5976\n",
            "disc_loss is 1.7569\n",
            "current step is 127\n",
            "gen_loss is 0.6057\n",
            "disc_loss is 1.6887\n",
            "current step is 128\n",
            "gen_loss is 0.6343\n",
            "disc_loss is 1.7021\n",
            "current step is 129\n",
            "gen_loss is 0.6544\n",
            "disc_loss is 1.6319\n",
            "current step is 130\n",
            "gen_loss is 0.6997\n",
            "disc_loss is 1.5147\n",
            "current step is 131\n",
            "gen_loss is 0.7103\n",
            "disc_loss is 1.5403\n",
            "current step is 132\n",
            "gen_loss is 0.7329\n",
            "disc_loss is 1.4470\n",
            "current step is 133\n",
            "gen_loss is 0.7625\n",
            "disc_loss is 1.3929\n",
            "current step is 134\n",
            "gen_loss is 0.7644\n",
            "disc_loss is 1.4253\n",
            "current step is 135\n",
            "gen_loss is 0.7909\n",
            "disc_loss is 1.3097\n",
            "current step is 136\n",
            "gen_loss is 0.8143\n",
            "disc_loss is 1.3085\n",
            "current step is 137\n",
            "gen_loss is 0.8480\n",
            "disc_loss is 1.2491\n",
            "current step is 138\n",
            "gen_loss is 0.8508\n",
            "disc_loss is 1.2949\n",
            "current step is 139\n",
            "gen_loss is 0.8796\n",
            "disc_loss is 1.2117\n",
            "current step is 140\n",
            "gen_loss is 0.8923\n",
            "disc_loss is 1.2095\n",
            "current step is 141\n",
            "gen_loss is 0.9152\n",
            "disc_loss is 1.1429\n",
            "current step is 142\n",
            "gen_loss is 0.9049\n",
            "disc_loss is 1.1463\n",
            "current step is 143\n",
            "gen_loss is 0.9270\n",
            "disc_loss is 1.1542\n",
            "current step is 144\n",
            "gen_loss is 0.9256\n",
            "disc_loss is 1.1269\n",
            "current step is 145\n",
            "gen_loss is 0.9362\n",
            "disc_loss is 1.1148\n",
            "current step is 146\n",
            "gen_loss is 0.9370\n",
            "disc_loss is 1.1463\n",
            "current step is 147\n",
            "gen_loss is 0.9608\n",
            "disc_loss is 1.0937\n",
            "current step is 148\n",
            "gen_loss is 0.9469\n",
            "disc_loss is 1.1273\n",
            "current step is 149\n",
            "gen_loss is 0.9549\n",
            "disc_loss is 1.1039\n",
            "current step is 150\n",
            "gen_loss is 0.9515\n",
            "disc_loss is 1.0982\n",
            "current step is 151\n",
            "gen_loss is 0.9904\n",
            "disc_loss is 1.0656\n",
            "current step is 152\n",
            "gen_loss is 0.9512\n",
            "disc_loss is 1.1146\n",
            "current step is 153\n",
            "gen_loss is 0.9796\n",
            "disc_loss is 1.0519\n",
            "current step is 154\n",
            "gen_loss is 0.9985\n",
            "disc_loss is 1.0817\n",
            "current step is 155\n",
            "gen_loss is 0.9930\n",
            "disc_loss is 1.0580\n",
            "current step is 156\n",
            "gen_loss is 0.9905\n",
            "disc_loss is 1.1221\n",
            "current step is 157\n",
            "gen_loss is 0.9939\n",
            "disc_loss is 1.0768\n",
            "current step is 158\n",
            "gen_loss is 0.9806\n",
            "disc_loss is 1.1044\n",
            "current step is 159\n",
            "gen_loss is 0.9658\n",
            "disc_loss is 1.1259\n",
            "current step is 160\n",
            "gen_loss is 0.9687\n",
            "disc_loss is 1.0997\n",
            "current step is 161\n",
            "gen_loss is 0.9910\n",
            "disc_loss is 1.1256\n",
            "current step is 162\n",
            "gen_loss is 0.9927\n",
            "disc_loss is 1.0863\n",
            "current step is 163\n",
            "gen_loss is 0.9799\n",
            "disc_loss is 1.0720\n",
            "current step is 164\n",
            "gen_loss is 0.9999\n",
            "disc_loss is 1.0577\n",
            "current step is 165\n",
            "gen_loss is 0.9705\n",
            "disc_loss is 1.0599\n",
            "current step is 166\n",
            "gen_loss is 0.9888\n",
            "disc_loss is 1.1407\n",
            "current step is 167\n",
            "gen_loss is 0.9635\n",
            "disc_loss is 1.1318\n",
            "current step is 168\n",
            "gen_loss is 0.9553\n",
            "disc_loss is 1.1165\n",
            "current step is 169\n",
            "gen_loss is 0.9640\n",
            "disc_loss is 1.1063\n",
            "current step is 170\n",
            "gen_loss is 0.9630\n",
            "disc_loss is 1.0924\n",
            "current step is 171\n",
            "gen_loss is 0.9500\n",
            "disc_loss is 1.0887\n",
            "current step is 172\n",
            "gen_loss is 0.9361\n",
            "disc_loss is 1.1355\n",
            "current step is 173\n",
            "gen_loss is 0.9524\n",
            "disc_loss is 1.1435\n",
            "current step is 174\n",
            "gen_loss is 0.9283\n",
            "disc_loss is 1.1416\n",
            "current step is 175\n",
            "gen_loss is 0.9327\n",
            "disc_loss is 1.1287\n",
            "current step is 176\n",
            "gen_loss is 0.9542\n",
            "disc_loss is 1.0856\n",
            "current step is 177\n",
            "gen_loss is 0.9354\n",
            "disc_loss is 1.1325\n",
            "current step is 178\n",
            "gen_loss is 0.9546\n",
            "disc_loss is 1.0905\n",
            "current step is 179\n",
            "gen_loss is 0.9601\n",
            "disc_loss is 1.2188\n",
            "current step is 180\n",
            "gen_loss is 0.9650\n",
            "disc_loss is 1.1312\n",
            "current step is 181\n",
            "gen_loss is 0.9670\n",
            "disc_loss is 1.1220\n",
            "current step is 182\n",
            "gen_loss is 0.9528\n",
            "disc_loss is 1.1778\n",
            "current step is 183\n",
            "gen_loss is 0.9529\n",
            "disc_loss is 1.1746\n",
            "current step is 184\n",
            "gen_loss is 0.9524\n",
            "disc_loss is 1.1551\n",
            "current step is 185\n",
            "gen_loss is 0.9608\n",
            "disc_loss is 1.1693\n",
            "current step is 186\n",
            "gen_loss is 0.9472\n",
            "disc_loss is 1.1430\n",
            "current step is 187\n",
            "gen_loss is 0.9271\n",
            "disc_loss is 1.1935\n",
            "current step is 188\n",
            "gen_loss is 0.9434\n",
            "disc_loss is 1.1404\n",
            "current step is 189\n",
            "gen_loss is 0.9311\n",
            "disc_loss is 1.1924\n",
            "current step is 190\n",
            "gen_loss is 0.9039\n",
            "disc_loss is 1.2084\n",
            "current step is 191\n",
            "gen_loss is 0.9373\n",
            "disc_loss is 1.1703\n",
            "current step is 192\n",
            "gen_loss is 0.9093\n",
            "disc_loss is 1.2182\n",
            "current step is 193\n",
            "gen_loss is 0.9147\n",
            "disc_loss is 1.2393\n",
            "current step is 194\n",
            "gen_loss is 0.9205\n",
            "disc_loss is 1.1998\n",
            "current step is 195\n",
            "gen_loss is 0.9110\n",
            "disc_loss is 1.1972\n",
            "current step is 196\n",
            "gen_loss is 0.8657\n",
            "disc_loss is 1.2619\n",
            "current step is 197\n",
            "gen_loss is 0.8725\n",
            "disc_loss is 1.2155\n",
            "current step is 198\n",
            "gen_loss is 0.8878\n",
            "disc_loss is 1.1972\n",
            "current step is 199\n",
            "gen_loss is 0.8700\n",
            "disc_loss is 1.2650\n",
            "current step is 200\n",
            "gen_loss is 0.8734\n",
            "disc_loss is 1.2103\n",
            "current step is 201\n",
            "gen_loss is 0.8740\n",
            "disc_loss is 1.2460\n",
            "current step is 202\n",
            "gen_loss is 0.8683\n",
            "disc_loss is 1.3293\n",
            "current step is 203\n",
            "gen_loss is 0.8294\n",
            "disc_loss is 1.2568\n",
            "current step is 204\n",
            "gen_loss is 0.8508\n",
            "disc_loss is 1.2378\n",
            "current step is 205\n",
            "gen_loss is 0.8559\n",
            "disc_loss is 1.3286\n",
            "current step is 206\n",
            "gen_loss is 0.8461\n",
            "disc_loss is 1.2943\n",
            "current step is 207\n",
            "gen_loss is 0.8461\n",
            "disc_loss is 1.2942\n",
            "current step is 208\n",
            "gen_loss is 0.8570\n",
            "disc_loss is 1.3025\n",
            "current step is 209\n",
            "gen_loss is 0.8428\n",
            "disc_loss is 1.3059\n",
            "current step is 210\n",
            "gen_loss is 0.8482\n",
            "disc_loss is 1.3106\n",
            "current step is 211\n",
            "gen_loss is 0.8491\n",
            "disc_loss is 1.3546\n",
            "current step is 212\n",
            "gen_loss is 0.8460\n",
            "disc_loss is 1.3032\n",
            "current step is 213\n",
            "gen_loss is 0.8346\n",
            "disc_loss is 1.3263\n",
            "current step is 214\n",
            "gen_loss is 0.8203\n",
            "disc_loss is 1.3363\n",
            "current step is 215\n",
            "gen_loss is 0.8180\n",
            "disc_loss is 1.3308\n",
            "current step is 216\n",
            "gen_loss is 0.8018\n",
            "disc_loss is 1.3651\n",
            "current step is 217\n",
            "gen_loss is 0.7934\n",
            "disc_loss is 1.3195\n",
            "current step is 218\n",
            "gen_loss is 0.8091\n",
            "disc_loss is 1.3850\n",
            "current step is 219\n",
            "gen_loss is 0.8061\n",
            "disc_loss is 1.3437\n",
            "current step is 220\n",
            "gen_loss is 0.7855\n",
            "disc_loss is 1.4562\n",
            "current step is 221\n",
            "gen_loss is 0.7755\n",
            "disc_loss is 1.3866\n",
            "current step is 222\n",
            "gen_loss is 0.7878\n",
            "disc_loss is 1.4186\n",
            "current step is 223\n",
            "gen_loss is 0.7927\n",
            "disc_loss is 1.4123\n",
            "current step is 224\n",
            "gen_loss is 0.7901\n",
            "disc_loss is 1.4308\n",
            "current step is 225\n",
            "gen_loss is 0.7579\n",
            "disc_loss is 1.4386\n",
            "current step is 226\n",
            "gen_loss is 0.7834\n",
            "disc_loss is 1.4672\n",
            "current step is 227\n",
            "gen_loss is 0.7773\n",
            "disc_loss is 1.4602\n",
            "current step is 228\n",
            "gen_loss is 0.7465\n",
            "disc_loss is 1.5085\n",
            "current step is 229\n",
            "gen_loss is 0.7560\n",
            "disc_loss is 1.5021\n",
            "current step is 230\n",
            "gen_loss is 0.7232\n",
            "disc_loss is 1.5267\n",
            "current step is 231\n",
            "gen_loss is 0.7085\n",
            "disc_loss is 1.5092\n",
            "current step is 232\n",
            "gen_loss is 0.7073\n",
            "disc_loss is 1.4892\n",
            "current step is 233\n",
            "gen_loss is 0.6969\n",
            "disc_loss is 1.4837\n",
            "current step is 234\n",
            "gen_loss is 0.6882\n",
            "disc_loss is 1.4825\n",
            "Current epoch 6 is\n",
            "The train loss value for epoch 6 is 0.8422\n",
            "The test loss value for epoch 6 is 1.3066\n",
            "Time for epoch 6 is 1082.8391003608704 sec\n",
            "current step is 0\n",
            "gen_loss is 0.6938\n",
            "disc_loss is 1.5143\n",
            "current step is 1\n",
            "gen_loss is 0.6842\n",
            "disc_loss is 1.5413\n",
            "current step is 2\n",
            "gen_loss is 0.6952\n",
            "disc_loss is 1.4779\n",
            "current step is 3\n",
            "gen_loss is 0.7052\n",
            "disc_loss is 1.5038\n",
            "current step is 4\n",
            "gen_loss is 0.7048\n",
            "disc_loss is 1.5114\n",
            "current step is 5\n",
            "gen_loss is 0.7335\n",
            "disc_loss is 1.4928\n",
            "current step is 6\n",
            "gen_loss is 0.7219\n",
            "disc_loss is 1.5099\n",
            "current step is 7\n",
            "gen_loss is 0.7223\n",
            "disc_loss is 1.4673\n",
            "current step is 8\n",
            "gen_loss is 0.7447\n",
            "disc_loss is 1.4475\n",
            "current step is 9\n",
            "gen_loss is 0.7243\n",
            "disc_loss is 1.4701\n",
            "current step is 10\n",
            "gen_loss is 0.7263\n",
            "disc_loss is 1.5113\n",
            "current step is 11\n",
            "gen_loss is 0.7088\n",
            "disc_loss is 1.4559\n",
            "current step is 12\n",
            "gen_loss is 0.7136\n",
            "disc_loss is 1.4769\n",
            "current step is 13\n",
            "gen_loss is 0.7016\n",
            "disc_loss is 1.4382\n",
            "current step is 14\n",
            "gen_loss is 0.7050\n",
            "disc_loss is 1.4254\n",
            "current step is 15\n",
            "gen_loss is 0.6964\n",
            "disc_loss is 1.4560\n",
            "current step is 16\n",
            "gen_loss is 0.6975\n",
            "disc_loss is 1.4174\n",
            "current step is 17\n",
            "gen_loss is 0.6921\n",
            "disc_loss is 1.4616\n",
            "current step is 18\n",
            "gen_loss is 0.7136\n",
            "disc_loss is 1.3746\n",
            "current step is 19\n",
            "gen_loss is 0.7251\n",
            "disc_loss is 1.4012\n",
            "current step is 20\n",
            "gen_loss is 0.7285\n",
            "disc_loss is 1.3857\n",
            "current step is 21\n",
            "gen_loss is 0.7440\n",
            "disc_loss is 1.3789\n",
            "current step is 22\n",
            "gen_loss is 0.7328\n",
            "disc_loss is 1.3620\n",
            "current step is 23\n",
            "gen_loss is 0.7513\n",
            "disc_loss is 1.3864\n",
            "current step is 24\n",
            "gen_loss is 0.7614\n",
            "disc_loss is 1.3728\n",
            "current step is 25\n",
            "gen_loss is 0.7656\n",
            "disc_loss is 1.3592\n",
            "current step is 26\n",
            "gen_loss is 0.7567\n",
            "disc_loss is 1.3661\n",
            "current step is 27\n",
            "gen_loss is 0.7723\n",
            "disc_loss is 1.3347\n",
            "current step is 28\n",
            "gen_loss is 0.7741\n",
            "disc_loss is 1.3273\n",
            "current step is 29\n",
            "gen_loss is 0.7805\n",
            "disc_loss is 1.2987\n",
            "current step is 30\n",
            "gen_loss is 0.7623\n",
            "disc_loss is 1.3235\n",
            "current step is 31\n",
            "gen_loss is 0.7742\n",
            "disc_loss is 1.2784\n",
            "current step is 32\n",
            "gen_loss is 0.7932\n",
            "disc_loss is 1.2794\n",
            "current step is 33\n",
            "gen_loss is 0.8005\n",
            "disc_loss is 1.2657\n",
            "current step is 34\n",
            "gen_loss is 0.8002\n",
            "disc_loss is 1.2395\n",
            "current step is 35\n",
            "gen_loss is 0.8103\n",
            "disc_loss is 1.2475\n",
            "current step is 36\n",
            "gen_loss is 0.8286\n",
            "disc_loss is 1.2657\n",
            "current step is 37\n",
            "gen_loss is 0.8486\n",
            "disc_loss is 1.2442\n",
            "current step is 38\n",
            "gen_loss is 0.8586\n",
            "disc_loss is 1.2282\n",
            "current step is 39\n",
            "gen_loss is 0.8541\n",
            "disc_loss is 1.2417\n",
            "current step is 40\n",
            "gen_loss is 0.8500\n",
            "disc_loss is 1.2154\n",
            "current step is 41\n",
            "gen_loss is 0.8726\n",
            "disc_loss is 1.2042\n",
            "current step is 42\n",
            "gen_loss is 0.8855\n",
            "disc_loss is 1.1784\n",
            "current step is 43\n",
            "gen_loss is 0.8901\n",
            "disc_loss is 1.1538\n",
            "current step is 44\n",
            "gen_loss is 0.8904\n",
            "disc_loss is 1.1138\n",
            "current step is 45\n",
            "gen_loss is 0.9043\n",
            "disc_loss is 1.1399\n",
            "current step is 46\n",
            "gen_loss is 0.9358\n",
            "disc_loss is 1.0926\n",
            "current step is 47\n",
            "gen_loss is 0.9129\n",
            "disc_loss is 1.1583\n",
            "current step is 48\n",
            "gen_loss is 0.9547\n",
            "disc_loss is 1.1360\n",
            "current step is 49\n",
            "gen_loss is 0.9570\n",
            "disc_loss is 1.1170\n",
            "current step is 50\n",
            "gen_loss is 0.9759\n",
            "disc_loss is 1.1080\n",
            "current step is 51\n",
            "gen_loss is 0.9656\n",
            "disc_loss is 1.0729\n",
            "current step is 52\n",
            "gen_loss is 0.9683\n",
            "disc_loss is 1.0478\n",
            "current step is 53\n",
            "gen_loss is 0.9721\n",
            "disc_loss is 1.0822\n",
            "current step is 54\n",
            "gen_loss is 0.9827\n",
            "disc_loss is 1.0735\n",
            "current step is 55\n",
            "gen_loss is 0.9623\n",
            "disc_loss is 1.0441\n",
            "current step is 56\n",
            "gen_loss is 0.9487\n",
            "disc_loss is 1.0981\n",
            "current step is 57\n",
            "gen_loss is 0.9678\n",
            "disc_loss is 1.0343\n",
            "current step is 58\n",
            "gen_loss is 0.9560\n",
            "disc_loss is 1.0629\n",
            "current step is 59\n",
            "gen_loss is 0.9818\n",
            "disc_loss is 1.0219\n",
            "current step is 60\n",
            "gen_loss is 0.9914\n",
            "disc_loss is 1.0325\n",
            "current step is 61\n",
            "gen_loss is 1.0116\n",
            "disc_loss is 1.0400\n",
            "current step is 62\n",
            "gen_loss is 0.9931\n",
            "disc_loss is 1.0125\n",
            "current step is 63\n",
            "gen_loss is 1.0360\n",
            "disc_loss is 1.0320\n",
            "current step is 64\n",
            "gen_loss is 0.9934\n",
            "disc_loss is 1.0340\n",
            "current step is 65\n",
            "gen_loss is 1.0045\n",
            "disc_loss is 1.0731\n",
            "current step is 66\n",
            "gen_loss is 0.9793\n",
            "disc_loss is 1.0994\n",
            "current step is 67\n",
            "gen_loss is 0.9796\n",
            "disc_loss is 1.1043\n",
            "current step is 68\n",
            "gen_loss is 0.9466\n",
            "disc_loss is 1.1414\n",
            "current step is 69\n",
            "gen_loss is 0.9530\n",
            "disc_loss is 1.1185\n",
            "current step is 70\n",
            "gen_loss is 0.9056\n",
            "disc_loss is 1.1871\n",
            "current step is 71\n",
            "gen_loss is 0.8534\n",
            "disc_loss is 1.2283\n",
            "current step is 72\n",
            "gen_loss is 0.8774\n",
            "disc_loss is 1.2580\n",
            "current step is 73\n",
            "gen_loss is 0.8521\n",
            "disc_loss is 1.2589\n",
            "current step is 74\n",
            "gen_loss is 0.8390\n",
            "disc_loss is 1.3314\n",
            "current step is 75\n",
            "gen_loss is 0.8069\n",
            "disc_loss is 1.3467\n",
            "current step is 76\n",
            "gen_loss is 0.7579\n",
            "disc_loss is 1.3815\n",
            "current step is 77\n",
            "gen_loss is 0.7488\n",
            "disc_loss is 1.3492\n",
            "current step is 78\n",
            "gen_loss is 0.7631\n",
            "disc_loss is 1.4442\n",
            "current step is 79\n",
            "gen_loss is 0.8166\n",
            "disc_loss is 1.3434\n",
            "current step is 80\n",
            "gen_loss is 0.7801\n",
            "disc_loss is 1.4556\n",
            "current step is 81\n",
            "gen_loss is 0.7875\n",
            "disc_loss is 1.4146\n",
            "current step is 82\n",
            "gen_loss is 0.7872\n",
            "disc_loss is 1.4424\n",
            "current step is 83\n",
            "gen_loss is 0.7812\n",
            "disc_loss is 1.4398\n",
            "current step is 84\n",
            "gen_loss is 0.7574\n",
            "disc_loss is 1.4738\n",
            "current step is 85\n",
            "gen_loss is 0.8007\n",
            "disc_loss is 1.4115\n",
            "current step is 86\n",
            "gen_loss is 0.7838\n",
            "disc_loss is 1.4411\n",
            "current step is 87\n",
            "gen_loss is 0.7923\n",
            "disc_loss is 1.3699\n",
            "current step is 88\n",
            "gen_loss is 0.8178\n",
            "disc_loss is 1.4015\n",
            "current step is 89\n",
            "gen_loss is 0.7915\n",
            "disc_loss is 1.3587\n",
            "current step is 90\n",
            "gen_loss is 0.8243\n",
            "disc_loss is 1.3169\n",
            "current step is 91\n",
            "gen_loss is 0.8662\n",
            "disc_loss is 1.2988\n",
            "current step is 92\n",
            "gen_loss is 0.8746\n",
            "disc_loss is 1.3008\n",
            "current step is 93\n",
            "gen_loss is 0.9044\n",
            "disc_loss is 1.2995\n",
            "current step is 94\n",
            "gen_loss is 0.9152\n",
            "disc_loss is 1.2194\n",
            "current step is 95\n",
            "gen_loss is 0.9722\n",
            "disc_loss is 1.1866\n",
            "current step is 96\n",
            "gen_loss is 0.9667\n",
            "disc_loss is 1.1740\n",
            "current step is 97\n",
            "gen_loss is 1.0017\n",
            "disc_loss is 1.1657\n",
            "current step is 98\n",
            "gen_loss is 0.9874\n",
            "disc_loss is 1.1591\n",
            "current step is 99\n",
            "gen_loss is 1.0258\n",
            "disc_loss is 1.1199\n",
            "current step is 100\n",
            "gen_loss is 1.0460\n",
            "disc_loss is 1.0421\n",
            "current step is 101\n",
            "gen_loss is 1.0596\n",
            "disc_loss is 1.0440\n",
            "current step is 102\n",
            "gen_loss is 1.0800\n",
            "disc_loss is 1.0870\n",
            "current step is 103\n",
            "gen_loss is 1.1210\n",
            "disc_loss is 1.0271\n",
            "current step is 104\n",
            "gen_loss is 1.1226\n",
            "disc_loss is 1.0575\n",
            "current step is 105\n",
            "gen_loss is 1.1182\n",
            "disc_loss is 1.0186\n",
            "current step is 106\n",
            "gen_loss is 1.1246\n",
            "disc_loss is 0.9804\n",
            "current step is 107\n",
            "gen_loss is 1.1384\n",
            "disc_loss is 1.0524\n",
            "current step is 108\n",
            "gen_loss is 1.1399\n",
            "disc_loss is 0.9705\n",
            "current step is 109\n",
            "gen_loss is 1.1510\n",
            "disc_loss is 1.0323\n",
            "current step is 110\n",
            "gen_loss is 1.1290\n",
            "disc_loss is 1.0226\n",
            "current step is 111\n",
            "gen_loss is 1.1263\n",
            "disc_loss is 0.9943\n",
            "current step is 112\n",
            "gen_loss is 1.1236\n",
            "disc_loss is 0.9754\n",
            "current step is 113\n",
            "gen_loss is 1.1234\n",
            "disc_loss is 0.9980\n",
            "current step is 114\n",
            "gen_loss is 1.0827\n",
            "disc_loss is 1.0084\n",
            "current step is 115\n",
            "gen_loss is 1.0498\n",
            "disc_loss is 1.0340\n",
            "current step is 116\n",
            "gen_loss is 1.0592\n",
            "disc_loss is 1.0746\n",
            "current step is 117\n",
            "gen_loss is 1.0368\n",
            "disc_loss is 1.0568\n",
            "current step is 118\n",
            "gen_loss is 1.0193\n",
            "disc_loss is 1.0502\n",
            "current step is 119\n",
            "gen_loss is 1.0350\n",
            "disc_loss is 1.0329\n",
            "current step is 120\n",
            "gen_loss is 1.0068\n",
            "disc_loss is 1.1359\n",
            "current step is 121\n",
            "gen_loss is 0.9745\n",
            "disc_loss is 1.1320\n",
            "current step is 122\n",
            "gen_loss is 0.9799\n",
            "disc_loss is 1.1239\n",
            "current step is 123\n",
            "gen_loss is 0.9677\n",
            "disc_loss is 1.1858\n",
            "current step is 124\n",
            "gen_loss is 0.9262\n",
            "disc_loss is 1.1633\n",
            "current step is 125\n",
            "gen_loss is 0.9131\n",
            "disc_loss is 1.1751\n",
            "current step is 126\n",
            "gen_loss is 0.9161\n",
            "disc_loss is 1.2180\n",
            "current step is 127\n",
            "gen_loss is 0.8913\n",
            "disc_loss is 1.2461\n",
            "current step is 128\n",
            "gen_loss is 0.8803\n",
            "disc_loss is 1.3233\n",
            "current step is 129\n",
            "gen_loss is 0.8386\n",
            "disc_loss is 1.3713\n",
            "current step is 130\n",
            "gen_loss is 0.8706\n",
            "disc_loss is 1.2951\n",
            "current step is 131\n",
            "gen_loss is 0.8355\n",
            "disc_loss is 1.3708\n",
            "current step is 132\n",
            "gen_loss is 0.8214\n",
            "disc_loss is 1.4129\n",
            "current step is 133\n",
            "gen_loss is 0.8156\n",
            "disc_loss is 1.3728\n",
            "current step is 134\n",
            "gen_loss is 0.8109\n",
            "disc_loss is 1.3629\n",
            "current step is 135\n",
            "gen_loss is 0.8105\n",
            "disc_loss is 1.4298\n",
            "current step is 136\n",
            "gen_loss is 0.7881\n",
            "disc_loss is 1.5085\n",
            "current step is 137\n",
            "gen_loss is 0.7623\n",
            "disc_loss is 1.4571\n",
            "current step is 138\n",
            "gen_loss is 0.7508\n",
            "disc_loss is 1.5105\n",
            "current step is 139\n",
            "gen_loss is 0.7687\n",
            "disc_loss is 1.4724\n",
            "current step is 140\n",
            "gen_loss is 0.7736\n",
            "disc_loss is 1.5626\n",
            "current step is 141\n",
            "gen_loss is 0.7593\n",
            "disc_loss is 1.6547\n",
            "current step is 142\n",
            "gen_loss is 0.7814\n",
            "disc_loss is 1.5542\n",
            "current step is 143\n",
            "gen_loss is 0.7282\n",
            "disc_loss is 1.6856\n",
            "current step is 144\n",
            "gen_loss is 0.7715\n",
            "disc_loss is 1.4840\n",
            "current step is 145\n",
            "gen_loss is 0.7371\n",
            "disc_loss is 1.6170\n",
            "current step is 146\n",
            "gen_loss is 0.7319\n",
            "disc_loss is 1.6086\n",
            "current step is 147\n",
            "gen_loss is 0.7341\n",
            "disc_loss is 1.6077\n",
            "current step is 148\n",
            "gen_loss is 0.7119\n",
            "disc_loss is 1.6318\n",
            "current step is 149\n",
            "gen_loss is 0.7697\n",
            "disc_loss is 1.5603\n",
            "current step is 150\n",
            "gen_loss is 0.7674\n",
            "disc_loss is 1.6865\n",
            "current step is 151\n",
            "gen_loss is 0.7532\n",
            "disc_loss is 1.5610\n",
            "current step is 152\n",
            "gen_loss is 0.7887\n",
            "disc_loss is 1.5499\n",
            "current step is 153\n",
            "gen_loss is 0.7967\n",
            "disc_loss is 1.5614\n",
            "current step is 154\n",
            "gen_loss is 0.7925\n",
            "disc_loss is 1.5778\n",
            "current step is 155\n",
            "gen_loss is 0.7865\n",
            "disc_loss is 1.6130\n",
            "current step is 156\n",
            "gen_loss is 0.8061\n",
            "disc_loss is 1.5238\n",
            "current step is 157\n",
            "gen_loss is 0.7812\n",
            "disc_loss is 1.5797\n",
            "current step is 158\n",
            "gen_loss is 0.8028\n",
            "disc_loss is 1.5612\n",
            "current step is 159\n",
            "gen_loss is 0.7771\n",
            "disc_loss is 1.6006\n",
            "current step is 160\n",
            "gen_loss is 0.8240\n",
            "disc_loss is 1.5350\n",
            "current step is 161\n",
            "gen_loss is 0.8047\n",
            "disc_loss is 1.4898\n",
            "current step is 162\n",
            "gen_loss is 0.8120\n",
            "disc_loss is 1.4807\n",
            "current step is 163\n",
            "gen_loss is 0.7820\n",
            "disc_loss is 1.5104\n",
            "current step is 164\n",
            "gen_loss is 0.8258\n",
            "disc_loss is 1.4574\n",
            "current step is 165\n",
            "gen_loss is 0.8121\n",
            "disc_loss is 1.4430\n",
            "current step is 166\n",
            "gen_loss is 0.8114\n",
            "disc_loss is 1.4423\n",
            "current step is 167\n",
            "gen_loss is 0.8271\n",
            "disc_loss is 1.4176\n",
            "current step is 168\n",
            "gen_loss is 0.8524\n",
            "disc_loss is 1.4026\n",
            "current step is 169\n",
            "gen_loss is 0.8671\n",
            "disc_loss is 1.3846\n",
            "current step is 170\n",
            "gen_loss is 0.8136\n",
            "disc_loss is 1.3937\n",
            "current step is 171\n",
            "gen_loss is 0.8826\n",
            "disc_loss is 1.3377\n",
            "current step is 172\n",
            "gen_loss is 0.8626\n",
            "disc_loss is 1.3466\n",
            "current step is 173\n",
            "gen_loss is 0.8939\n",
            "disc_loss is 1.3647\n",
            "current step is 174\n",
            "gen_loss is 0.9013\n",
            "disc_loss is 1.3232\n",
            "current step is 175\n",
            "gen_loss is 0.8949\n",
            "disc_loss is 1.3150\n",
            "current step is 176\n",
            "gen_loss is 0.9102\n",
            "disc_loss is 1.2885\n",
            "current step is 177\n",
            "gen_loss is 0.9257\n",
            "disc_loss is 1.2227\n",
            "current step is 178\n",
            "gen_loss is 0.9087\n",
            "disc_loss is 1.2631\n",
            "current step is 179\n",
            "gen_loss is 0.9012\n",
            "disc_loss is 1.2501\n",
            "current step is 180\n",
            "gen_loss is 0.9020\n",
            "disc_loss is 1.3032\n",
            "current step is 181\n",
            "gen_loss is 0.9042\n",
            "disc_loss is 1.2513\n",
            "current step is 182\n",
            "gen_loss is 0.8993\n",
            "disc_loss is 1.2075\n",
            "current step is 183\n",
            "gen_loss is 0.9226\n",
            "disc_loss is 1.1803\n",
            "current step is 184\n",
            "gen_loss is 0.9178\n",
            "disc_loss is 1.1875\n",
            "current step is 185\n",
            "gen_loss is 0.9207\n",
            "disc_loss is 1.2028\n",
            "current step is 186\n",
            "gen_loss is 0.9329\n",
            "disc_loss is 1.2049\n",
            "current step is 187\n",
            "gen_loss is 0.9674\n",
            "disc_loss is 1.1302\n",
            "current step is 188\n",
            "gen_loss is 0.9684\n",
            "disc_loss is 1.1645\n",
            "current step is 189\n",
            "gen_loss is 0.9810\n",
            "disc_loss is 1.1211\n",
            "current step is 190\n",
            "gen_loss is 1.0047\n",
            "disc_loss is 1.1282\n",
            "current step is 191\n",
            "gen_loss is 1.0084\n",
            "disc_loss is 1.1274\n",
            "current step is 192\n",
            "gen_loss is 0.9919\n",
            "disc_loss is 1.1083\n",
            "current step is 193\n",
            "gen_loss is 0.9858\n",
            "disc_loss is 1.1437\n",
            "current step is 194\n",
            "gen_loss is 0.9631\n",
            "disc_loss is 1.1305\n",
            "current step is 195\n",
            "gen_loss is 0.9692\n",
            "disc_loss is 1.0934\n",
            "current step is 196\n",
            "gen_loss is 0.9861\n",
            "disc_loss is 1.1249\n",
            "current step is 197\n",
            "gen_loss is 0.9461\n",
            "disc_loss is 1.1711\n",
            "current step is 198\n",
            "gen_loss is 0.9759\n",
            "disc_loss is 1.1370\n",
            "current step is 199\n",
            "gen_loss is 0.9275\n",
            "disc_loss is 1.1227\n",
            "current step is 200\n",
            "gen_loss is 0.9436\n",
            "disc_loss is 1.2055\n",
            "current step is 201\n",
            "gen_loss is 0.9197\n",
            "disc_loss is 1.1795\n",
            "current step is 202\n",
            "gen_loss is 0.9240\n",
            "disc_loss is 1.1405\n",
            "current step is 203\n",
            "gen_loss is 0.9228\n",
            "disc_loss is 1.1375\n",
            "current step is 204\n",
            "gen_loss is 0.8858\n",
            "disc_loss is 1.1641\n",
            "current step is 205\n",
            "gen_loss is 0.8859\n",
            "disc_loss is 1.1461\n",
            "current step is 206\n",
            "gen_loss is 0.9090\n",
            "disc_loss is 1.1187\n",
            "current step is 207\n",
            "gen_loss is 0.9097\n",
            "disc_loss is 1.1424\n",
            "current step is 208\n",
            "gen_loss is 0.9394\n",
            "disc_loss is 1.1671\n",
            "current step is 209\n",
            "gen_loss is 0.9438\n",
            "disc_loss is 1.1588\n",
            "current step is 210\n",
            "gen_loss is 0.9858\n",
            "disc_loss is 1.2202\n",
            "current step is 211\n",
            "gen_loss is 0.9701\n",
            "disc_loss is 1.1795\n",
            "current step is 212\n",
            "gen_loss is 0.9677\n",
            "disc_loss is 1.1759\n",
            "current step is 213\n",
            "gen_loss is 0.9251\n",
            "disc_loss is 1.2040\n",
            "current step is 214\n",
            "gen_loss is 0.9269\n",
            "disc_loss is 1.2289\n",
            "current step is 215\n",
            "gen_loss is 0.9062\n",
            "disc_loss is 1.2813\n",
            "current step is 216\n",
            "gen_loss is 0.9012\n",
            "disc_loss is 1.1942\n",
            "current step is 217\n",
            "gen_loss is 0.8667\n",
            "disc_loss is 1.2389\n",
            "current step is 218\n",
            "gen_loss is 0.8679\n",
            "disc_loss is 1.2481\n",
            "current step is 219\n",
            "gen_loss is 0.8569\n",
            "disc_loss is 1.2759\n",
            "current step is 220\n",
            "gen_loss is 0.8646\n",
            "disc_loss is 1.3218\n",
            "current step is 221\n",
            "gen_loss is 0.8448\n",
            "disc_loss is 1.2634\n",
            "current step is 222\n",
            "gen_loss is 0.8870\n",
            "disc_loss is 1.3159\n",
            "current step is 223\n",
            "gen_loss is 0.8454\n",
            "disc_loss is 1.3169\n",
            "current step is 224\n",
            "gen_loss is 0.8557\n",
            "disc_loss is 1.3600\n",
            "current step is 225\n",
            "gen_loss is 0.8605\n",
            "disc_loss is 1.3318\n",
            "current step is 226\n",
            "gen_loss is 0.8627\n",
            "disc_loss is 1.3446\n",
            "current step is 227\n",
            "gen_loss is 0.8215\n",
            "disc_loss is 1.4300\n",
            "current step is 228\n",
            "gen_loss is 0.8153\n",
            "disc_loss is 1.3805\n",
            "current step is 229\n",
            "gen_loss is 0.7991\n",
            "disc_loss is 1.4141\n",
            "current step is 230\n",
            "gen_loss is 0.7957\n",
            "disc_loss is 1.3736\n",
            "current step is 231\n",
            "gen_loss is 0.7854\n",
            "disc_loss is 1.4151\n",
            "current step is 232\n",
            "gen_loss is 0.7728\n",
            "disc_loss is 1.4304\n",
            "current step is 233\n",
            "gen_loss is 0.7868\n",
            "disc_loss is 1.4539\n",
            "current step is 234\n",
            "gen_loss is 0.7623\n",
            "disc_loss is 1.4108\n",
            "Current epoch 7 is\n",
            "The train loss value for epoch 7 is 0.8742\n",
            "The test loss value for epoch 7 is 1.2845\n",
            "Time for epoch 7 is 1082.916030883789 sec\n",
            "current step is 0\n",
            "gen_loss is 0.7720\n",
            "disc_loss is 1.4334\n",
            "current step is 1\n",
            "gen_loss is 0.7881\n",
            "disc_loss is 1.4164\n",
            "current step is 2\n",
            "gen_loss is 0.8072\n",
            "disc_loss is 1.3671\n",
            "current step is 3\n",
            "gen_loss is 0.8267\n",
            "disc_loss is 1.3766\n",
            "current step is 4\n",
            "gen_loss is 0.8527\n",
            "disc_loss is 1.4010\n",
            "current step is 5\n",
            "gen_loss is 0.8839\n",
            "disc_loss is 1.3085\n",
            "current step is 6\n",
            "gen_loss is 0.8628\n",
            "disc_loss is 1.4012\n",
            "current step is 7\n",
            "gen_loss is 0.8932\n",
            "disc_loss is 1.4082\n",
            "current step is 8\n",
            "gen_loss is 0.8830\n",
            "disc_loss is 1.3406\n",
            "current step is 9\n",
            "gen_loss is 0.8808\n",
            "disc_loss is 1.3603\n",
            "current step is 10\n",
            "gen_loss is 0.8804\n",
            "disc_loss is 1.2890\n",
            "current step is 11\n",
            "gen_loss is 0.8580\n",
            "disc_loss is 1.3773\n",
            "current step is 12\n",
            "gen_loss is 0.8628\n",
            "disc_loss is 1.3166\n",
            "current step is 13\n",
            "gen_loss is 0.8496\n",
            "disc_loss is 1.3406\n",
            "current step is 14\n",
            "gen_loss is 0.8385\n",
            "disc_loss is 1.2707\n",
            "current step is 15\n",
            "gen_loss is 0.8504\n",
            "disc_loss is 1.2583\n",
            "current step is 16\n",
            "gen_loss is 0.8795\n",
            "disc_loss is 1.2747\n",
            "current step is 17\n",
            "gen_loss is 0.9475\n",
            "disc_loss is 1.2057\n",
            "current step is 18\n",
            "gen_loss is 0.9555\n",
            "disc_loss is 1.2413\n",
            "current step is 19\n",
            "gen_loss is 0.9769\n",
            "disc_loss is 1.2026\n",
            "current step is 20\n",
            "gen_loss is 1.0092\n",
            "disc_loss is 1.1927\n",
            "current step is 21\n",
            "gen_loss is 1.0284\n",
            "disc_loss is 1.1606\n",
            "current step is 22\n",
            "gen_loss is 1.0326\n",
            "disc_loss is 1.1568\n",
            "current step is 23\n",
            "gen_loss is 1.0252\n",
            "disc_loss is 1.1502\n",
            "current step is 24\n",
            "gen_loss is 1.0169\n",
            "disc_loss is 1.1498\n",
            "current step is 25\n",
            "gen_loss is 1.0045\n",
            "disc_loss is 1.1264\n",
            "current step is 26\n",
            "gen_loss is 0.9866\n",
            "disc_loss is 1.1855\n",
            "current step is 27\n",
            "gen_loss is 0.9581\n",
            "disc_loss is 1.2238\n",
            "current step is 28\n",
            "gen_loss is 0.9573\n",
            "disc_loss is 1.1710\n",
            "current step is 29\n",
            "gen_loss is 0.9579\n",
            "disc_loss is 1.1808\n",
            "current step is 30\n",
            "gen_loss is 0.9449\n",
            "disc_loss is 1.1516\n",
            "current step is 31\n",
            "gen_loss is 0.9373\n",
            "disc_loss is 1.0960\n",
            "current step is 32\n",
            "gen_loss is 0.9827\n",
            "disc_loss is 1.1047\n",
            "current step is 33\n",
            "gen_loss is 0.9587\n",
            "disc_loss is 1.1378\n",
            "current step is 34\n",
            "gen_loss is 0.9780\n",
            "disc_loss is 1.1437\n",
            "current step is 35\n",
            "gen_loss is 1.0089\n",
            "disc_loss is 1.1349\n",
            "current step is 36\n",
            "gen_loss is 1.0073\n",
            "disc_loss is 1.1238\n",
            "current step is 37\n",
            "gen_loss is 1.0165\n",
            "disc_loss is 1.1408\n",
            "current step is 38\n",
            "gen_loss is 1.0012\n",
            "disc_loss is 1.1432\n",
            "current step is 39\n",
            "gen_loss is 0.9922\n",
            "disc_loss is 1.1083\n",
            "current step is 40\n",
            "gen_loss is 1.0326\n",
            "disc_loss is 1.0974\n",
            "current step is 41\n",
            "gen_loss is 0.9842\n",
            "disc_loss is 1.1240\n",
            "current step is 42\n",
            "gen_loss is 0.9976\n",
            "disc_loss is 1.1387\n",
            "current step is 43\n",
            "gen_loss is 1.0072\n",
            "disc_loss is 1.0587\n",
            "current step is 44\n",
            "gen_loss is 1.0069\n",
            "disc_loss is 1.1240\n",
            "current step is 45\n",
            "gen_loss is 1.0347\n",
            "disc_loss is 1.1137\n",
            "current step is 46\n",
            "gen_loss is 0.9984\n",
            "disc_loss is 1.1371\n",
            "current step is 47\n",
            "gen_loss is 1.0466\n",
            "disc_loss is 1.0807\n",
            "current step is 48\n",
            "gen_loss is 1.0727\n",
            "disc_loss is 1.1061\n",
            "current step is 49\n",
            "gen_loss is 1.1025\n",
            "disc_loss is 1.0148\n",
            "current step is 50\n",
            "gen_loss is 1.0651\n",
            "disc_loss is 1.0502\n",
            "current step is 51\n",
            "gen_loss is 1.0752\n",
            "disc_loss is 1.0925\n",
            "current step is 52\n",
            "gen_loss is 1.0719\n",
            "disc_loss is 1.1435\n",
            "current step is 53\n",
            "gen_loss is 1.0640\n",
            "disc_loss is 1.0423\n",
            "current step is 54\n",
            "gen_loss is 1.0703\n",
            "disc_loss is 1.1244\n",
            "current step is 55\n",
            "gen_loss is 1.0599\n",
            "disc_loss is 1.0670\n",
            "current step is 56\n",
            "gen_loss is 1.0427\n",
            "disc_loss is 1.1396\n",
            "current step is 57\n",
            "gen_loss is 1.0573\n",
            "disc_loss is 1.1621\n",
            "current step is 58\n",
            "gen_loss is 1.0854\n",
            "disc_loss is 1.0850\n",
            "current step is 59\n",
            "gen_loss is 1.0724\n",
            "disc_loss is 1.1241\n",
            "current step is 60\n",
            "gen_loss is 1.0399\n",
            "disc_loss is 1.1600\n",
            "current step is 61\n",
            "gen_loss is 1.0727\n",
            "disc_loss is 1.1365\n",
            "current step is 62\n",
            "gen_loss is 1.0501\n",
            "disc_loss is 1.1593\n",
            "current step is 63\n",
            "gen_loss is 1.0497\n",
            "disc_loss is 1.0675\n",
            "current step is 64\n",
            "gen_loss is 1.0358\n",
            "disc_loss is 1.1617\n",
            "current step is 65\n",
            "gen_loss is 1.0349\n",
            "disc_loss is 1.1873\n",
            "current step is 66\n",
            "gen_loss is 1.0398\n",
            "disc_loss is 1.2015\n",
            "current step is 67\n",
            "gen_loss is 1.0345\n",
            "disc_loss is 1.1755\n",
            "current step is 68\n",
            "gen_loss is 0.9798\n",
            "disc_loss is 1.1172\n",
            "current step is 69\n",
            "gen_loss is 1.0152\n",
            "disc_loss is 1.1466\n",
            "current step is 70\n",
            "gen_loss is 1.0195\n",
            "disc_loss is 1.1082\n",
            "current step is 71\n",
            "gen_loss is 1.0173\n",
            "disc_loss is 1.1890\n",
            "current step is 72\n",
            "gen_loss is 1.0341\n",
            "disc_loss is 1.1641\n",
            "current step is 73\n",
            "gen_loss is 1.0418\n",
            "disc_loss is 1.1844\n",
            "current step is 74\n",
            "gen_loss is 0.9654\n",
            "disc_loss is 1.1252\n",
            "current step is 75\n",
            "gen_loss is 1.0126\n",
            "disc_loss is 1.0853\n",
            "current step is 76\n",
            "gen_loss is 1.0368\n",
            "disc_loss is 1.0707\n",
            "current step is 77\n",
            "gen_loss is 1.0366\n",
            "disc_loss is 1.0918\n",
            "current step is 78\n",
            "gen_loss is 1.0461\n",
            "disc_loss is 1.0949\n",
            "current step is 79\n",
            "gen_loss is 1.0703\n",
            "disc_loss is 1.0384\n",
            "current step is 80\n",
            "gen_loss is 1.0506\n",
            "disc_loss is 1.0634\n",
            "current step is 81\n",
            "gen_loss is 1.0809\n",
            "disc_loss is 1.0508\n",
            "current step is 82\n",
            "gen_loss is 1.1020\n",
            "disc_loss is 1.0892\n",
            "current step is 83\n",
            "gen_loss is 1.1336\n",
            "disc_loss is 1.0207\n",
            "current step is 84\n",
            "gen_loss is 1.1372\n",
            "disc_loss is 1.0788\n",
            "current step is 85\n",
            "gen_loss is 1.1223\n",
            "disc_loss is 1.0854\n",
            "current step is 86\n",
            "gen_loss is 1.0752\n",
            "disc_loss is 1.0482\n",
            "current step is 87\n",
            "gen_loss is 1.0956\n",
            "disc_loss is 1.1152\n",
            "current step is 88\n",
            "gen_loss is 1.0153\n",
            "disc_loss is 1.1141\n",
            "current step is 89\n",
            "gen_loss is 1.0482\n",
            "disc_loss is 1.0999\n",
            "current step is 90\n",
            "gen_loss is 1.0321\n",
            "disc_loss is 1.1090\n",
            "current step is 91\n",
            "gen_loss is 1.0165\n",
            "disc_loss is 1.2018\n",
            "current step is 92\n",
            "gen_loss is 1.0249\n",
            "disc_loss is 1.0923\n",
            "current step is 93\n",
            "gen_loss is 0.9943\n",
            "disc_loss is 1.1405\n",
            "current step is 94\n",
            "gen_loss is 1.0409\n",
            "disc_loss is 1.1032\n",
            "current step is 95\n",
            "gen_loss is 1.0841\n",
            "disc_loss is 1.0738\n",
            "current step is 96\n",
            "gen_loss is 1.0718\n",
            "disc_loss is 1.0916\n",
            "current step is 97\n",
            "gen_loss is 1.0733\n",
            "disc_loss is 1.1310\n",
            "current step is 98\n",
            "gen_loss is 1.1089\n",
            "disc_loss is 1.1389\n",
            "current step is 99\n",
            "gen_loss is 1.0463\n",
            "disc_loss is 1.1488\n",
            "current step is 100\n",
            "gen_loss is 1.0254\n",
            "disc_loss is 1.0966\n",
            "current step is 101\n",
            "gen_loss is 1.0492\n",
            "disc_loss is 1.1507\n",
            "current step is 102\n",
            "gen_loss is 1.0796\n",
            "disc_loss is 1.1115\n",
            "current step is 103\n",
            "gen_loss is 1.0275\n",
            "disc_loss is 1.0787\n",
            "current step is 104\n",
            "gen_loss is 1.0103\n",
            "disc_loss is 1.1400\n",
            "current step is 105\n",
            "gen_loss is 1.0458\n",
            "disc_loss is 1.0831\n",
            "current step is 106\n",
            "gen_loss is 1.0368\n",
            "disc_loss is 1.0565\n",
            "current step is 107\n",
            "gen_loss is 1.1030\n",
            "disc_loss is 1.1018\n",
            "current step is 108\n",
            "gen_loss is 1.0659\n",
            "disc_loss is 1.1339\n",
            "current step is 109\n",
            "gen_loss is 1.0938\n",
            "disc_loss is 0.9673\n",
            "current step is 110\n",
            "gen_loss is 1.0838\n",
            "disc_loss is 0.9829\n",
            "current step is 111\n",
            "gen_loss is 1.0504\n",
            "disc_loss is 1.1405\n",
            "current step is 112\n",
            "gen_loss is 1.0969\n",
            "disc_loss is 0.9888\n",
            "current step is 113\n",
            "gen_loss is 1.1201\n",
            "disc_loss is 0.9923\n",
            "current step is 114\n",
            "gen_loss is 1.0471\n",
            "disc_loss is 1.0058\n",
            "current step is 115\n",
            "gen_loss is 1.0787\n",
            "disc_loss is 0.9688\n",
            "current step is 116\n",
            "gen_loss is 1.1353\n",
            "disc_loss is 0.9976\n",
            "current step is 117\n",
            "gen_loss is 1.1214\n",
            "disc_loss is 0.9598\n",
            "current step is 118\n",
            "gen_loss is 1.1305\n",
            "disc_loss is 0.9961\n",
            "current step is 119\n",
            "gen_loss is 1.1632\n",
            "disc_loss is 1.0074\n",
            "current step is 120\n",
            "gen_loss is 1.1034\n",
            "disc_loss is 1.0301\n",
            "current step is 121\n",
            "gen_loss is 1.1335\n",
            "disc_loss is 0.9795\n",
            "current step is 122\n",
            "gen_loss is 1.2016\n",
            "disc_loss is 0.9806\n",
            "current step is 123\n",
            "gen_loss is 1.1447\n",
            "disc_loss is 0.9811\n",
            "current step is 124\n",
            "gen_loss is 1.1353\n",
            "disc_loss is 0.9753\n",
            "current step is 125\n",
            "gen_loss is 1.1419\n",
            "disc_loss is 0.9483\n",
            "current step is 126\n",
            "gen_loss is 1.1199\n",
            "disc_loss is 0.9684\n",
            "current step is 127\n",
            "gen_loss is 1.1329\n",
            "disc_loss is 1.0297\n",
            "current step is 128\n",
            "gen_loss is 1.1571\n",
            "disc_loss is 0.9509\n",
            "current step is 129\n",
            "gen_loss is 1.1635\n",
            "disc_loss is 1.0271\n",
            "current step is 130\n",
            "gen_loss is 1.1649\n",
            "disc_loss is 1.0068\n",
            "current step is 131\n",
            "gen_loss is 1.1585\n",
            "disc_loss is 0.9826\n",
            "current step is 132\n",
            "gen_loss is 1.1359\n",
            "disc_loss is 1.0237\n",
            "current step is 133\n",
            "gen_loss is 1.1559\n",
            "disc_loss is 0.9961\n",
            "current step is 134\n",
            "gen_loss is 1.1004\n",
            "disc_loss is 1.0343\n",
            "current step is 135\n",
            "gen_loss is 1.1636\n",
            "disc_loss is 1.0021\n",
            "current step is 136\n",
            "gen_loss is 1.1061\n",
            "disc_loss is 1.0722\n",
            "current step is 137\n",
            "gen_loss is 1.0969\n",
            "disc_loss is 1.0409\n",
            "current step is 138\n",
            "gen_loss is 1.0890\n",
            "disc_loss is 1.0879\n",
            "current step is 139\n",
            "gen_loss is 1.0908\n",
            "disc_loss is 1.1214\n",
            "current step is 140\n",
            "gen_loss is 1.1024\n",
            "disc_loss is 1.0875\n",
            "current step is 141\n",
            "gen_loss is 1.0475\n",
            "disc_loss is 1.1551\n",
            "current step is 142\n",
            "gen_loss is 1.0788\n",
            "disc_loss is 1.1170\n",
            "current step is 143\n",
            "gen_loss is 0.9955\n",
            "disc_loss is 1.1549\n",
            "current step is 144\n",
            "gen_loss is 0.9958\n",
            "disc_loss is 1.1424\n",
            "current step is 145\n",
            "gen_loss is 0.9653\n",
            "disc_loss is 1.1460\n",
            "current step is 146\n",
            "gen_loss is 0.9924\n",
            "disc_loss is 1.1378\n",
            "current step is 147\n",
            "gen_loss is 1.0078\n",
            "disc_loss is 1.1782\n",
            "current step is 148\n",
            "gen_loss is 1.0081\n",
            "disc_loss is 1.2059\n",
            "current step is 149\n",
            "gen_loss is 1.0068\n",
            "disc_loss is 1.2418\n",
            "current step is 150\n",
            "gen_loss is 0.9934\n",
            "disc_loss is 1.2600\n",
            "current step is 151\n",
            "gen_loss is 0.9969\n",
            "disc_loss is 1.2663\n",
            "current step is 152\n",
            "gen_loss is 0.9492\n",
            "disc_loss is 1.2275\n",
            "current step is 153\n",
            "gen_loss is 0.9770\n",
            "disc_loss is 1.2385\n",
            "current step is 154\n",
            "gen_loss is 0.9629\n",
            "disc_loss is 1.2541\n",
            "current step is 155\n",
            "gen_loss is 0.9376\n",
            "disc_loss is 1.3197\n",
            "current step is 156\n",
            "gen_loss is 0.9199\n",
            "disc_loss is 1.2209\n",
            "current step is 157\n",
            "gen_loss is 0.9033\n",
            "disc_loss is 1.2419\n",
            "current step is 158\n",
            "gen_loss is 0.9050\n",
            "disc_loss is 1.2766\n",
            "current step is 159\n",
            "gen_loss is 0.9156\n",
            "disc_loss is 1.2688\n",
            "current step is 160\n",
            "gen_loss is 0.9297\n",
            "disc_loss is 1.2958\n",
            "current step is 161\n",
            "gen_loss is 0.9080\n",
            "disc_loss is 1.2696\n",
            "current step is 162\n",
            "gen_loss is 0.9110\n",
            "disc_loss is 1.2883\n",
            "current step is 163\n",
            "gen_loss is 0.8669\n",
            "disc_loss is 1.3216\n",
            "current step is 164\n",
            "gen_loss is 0.8762\n",
            "disc_loss is 1.3484\n",
            "current step is 165\n",
            "gen_loss is 0.8953\n",
            "disc_loss is 1.3365\n",
            "current step is 166\n",
            "gen_loss is 0.8897\n",
            "disc_loss is 1.2937\n",
            "current step is 167\n",
            "gen_loss is 0.8684\n",
            "disc_loss is 1.3223\n",
            "current step is 168\n",
            "gen_loss is 0.8538\n",
            "disc_loss is 1.3397\n",
            "current step is 169\n",
            "gen_loss is 0.8712\n",
            "disc_loss is 1.3136\n",
            "current step is 170\n",
            "gen_loss is 0.8806\n",
            "disc_loss is 1.3215\n",
            "current step is 171\n",
            "gen_loss is 0.8602\n",
            "disc_loss is 1.3450\n",
            "current step is 172\n",
            "gen_loss is 0.8504\n",
            "disc_loss is 1.3474\n",
            "current step is 173\n",
            "gen_loss is 0.8529\n",
            "disc_loss is 1.3579\n",
            "current step is 174\n",
            "gen_loss is 0.8722\n",
            "disc_loss is 1.3230\n",
            "current step is 175\n",
            "gen_loss is 0.8588\n",
            "disc_loss is 1.2995\n",
            "current step is 176\n",
            "gen_loss is 0.8185\n",
            "disc_loss is 1.3017\n",
            "current step is 177\n",
            "gen_loss is 0.8459\n",
            "disc_loss is 1.3146\n",
            "current step is 178\n",
            "gen_loss is 0.8337\n",
            "disc_loss is 1.3376\n",
            "current step is 179\n",
            "gen_loss is 0.8284\n",
            "disc_loss is 1.3389\n",
            "current step is 180\n",
            "gen_loss is 0.8283\n",
            "disc_loss is 1.3744\n",
            "current step is 181\n",
            "gen_loss is 0.8826\n",
            "disc_loss is 1.2730\n",
            "current step is 182\n",
            "gen_loss is 0.8640\n",
            "disc_loss is 1.3669\n",
            "current step is 183\n",
            "gen_loss is 0.8624\n",
            "disc_loss is 1.3358\n",
            "current step is 184\n",
            "gen_loss is 0.8954\n",
            "disc_loss is 1.3302\n",
            "current step is 185\n",
            "gen_loss is 0.8887\n",
            "disc_loss is 1.3270\n",
            "current step is 186\n",
            "gen_loss is 0.8764\n",
            "disc_loss is 1.2866\n",
            "current step is 187\n",
            "gen_loss is 0.8639\n",
            "disc_loss is 1.3212\n",
            "current step is 188\n",
            "gen_loss is 0.8633\n",
            "disc_loss is 1.3275\n",
            "current step is 189\n",
            "gen_loss is 0.8606\n",
            "disc_loss is 1.3367\n",
            "current step is 190\n",
            "gen_loss is 0.8509\n",
            "disc_loss is 1.3043\n",
            "current step is 191\n",
            "gen_loss is 0.8294\n",
            "disc_loss is 1.3269\n",
            "current step is 192\n",
            "gen_loss is 0.8577\n",
            "disc_loss is 1.3349\n",
            "current step is 193\n",
            "gen_loss is 0.8335\n",
            "disc_loss is 1.3445\n",
            "current step is 194\n",
            "gen_loss is 0.8450\n",
            "disc_loss is 1.3080\n",
            "current step is 195\n",
            "gen_loss is 0.8421\n",
            "disc_loss is 1.3434\n",
            "current step is 196\n",
            "gen_loss is 0.8485\n",
            "disc_loss is 1.3352\n",
            "current step is 197\n",
            "gen_loss is 0.8043\n",
            "disc_loss is 1.3893\n",
            "current step is 198\n",
            "gen_loss is 0.8232\n",
            "disc_loss is 1.4097\n",
            "current step is 199\n",
            "gen_loss is 0.8450\n",
            "disc_loss is 1.3308\n",
            "current step is 200\n",
            "gen_loss is 0.8078\n",
            "disc_loss is 1.3949\n",
            "current step is 201\n",
            "gen_loss is 0.8148\n",
            "disc_loss is 1.3065\n",
            "current step is 202\n",
            "gen_loss is 0.8000\n",
            "disc_loss is 1.3632\n",
            "current step is 203\n",
            "gen_loss is 0.7933\n",
            "disc_loss is 1.3967\n",
            "current step is 204\n",
            "gen_loss is 0.8217\n",
            "disc_loss is 1.3700\n",
            "current step is 205\n",
            "gen_loss is 0.8208\n",
            "disc_loss is 1.3633\n",
            "current step is 206\n",
            "gen_loss is 0.8218\n",
            "disc_loss is 1.3516\n",
            "current step is 207\n",
            "gen_loss is 0.8057\n",
            "disc_loss is 1.4045\n",
            "current step is 208\n",
            "gen_loss is 0.8334\n",
            "disc_loss is 1.4334\n",
            "current step is 209\n",
            "gen_loss is 0.8212\n",
            "disc_loss is 1.3436\n",
            "current step is 210\n",
            "gen_loss is 0.8142\n",
            "disc_loss is 1.4013\n",
            "current step is 211\n",
            "gen_loss is 0.8179\n",
            "disc_loss is 1.3917\n",
            "current step is 212\n",
            "gen_loss is 0.8020\n",
            "disc_loss is 1.4535\n",
            "current step is 213\n",
            "gen_loss is 0.8183\n",
            "disc_loss is 1.4351\n",
            "current step is 214\n",
            "gen_loss is 0.8194\n",
            "disc_loss is 1.4330\n",
            "current step is 215\n",
            "gen_loss is 0.8331\n",
            "disc_loss is 1.4017\n",
            "current step is 216\n",
            "gen_loss is 0.7953\n",
            "disc_loss is 1.4376\n",
            "current step is 217\n",
            "gen_loss is 0.8097\n",
            "disc_loss is 1.3885\n",
            "current step is 218\n",
            "gen_loss is 0.8071\n",
            "disc_loss is 1.3703\n",
            "current step is 219\n",
            "gen_loss is 0.8121\n",
            "disc_loss is 1.3858\n",
            "current step is 220\n",
            "gen_loss is 0.8209\n",
            "disc_loss is 1.3189\n",
            "current step is 221\n",
            "gen_loss is 0.8054\n",
            "disc_loss is 1.4207\n",
            "current step is 222\n",
            "gen_loss is 0.8235\n",
            "disc_loss is 1.3270\n",
            "current step is 223\n",
            "gen_loss is 0.8245\n",
            "disc_loss is 1.3643\n",
            "current step is 224\n",
            "gen_loss is 0.8331\n",
            "disc_loss is 1.3340\n",
            "current step is 225\n",
            "gen_loss is 0.8400\n",
            "disc_loss is 1.3136\n",
            "current step is 226\n",
            "gen_loss is 0.8922\n",
            "disc_loss is 1.2495\n",
            "current step is 227\n",
            "gen_loss is 0.8761\n",
            "disc_loss is 1.2482\n",
            "current step is 228\n",
            "gen_loss is 0.8844\n",
            "disc_loss is 1.2626\n",
            "current step is 229\n",
            "gen_loss is 0.8886\n",
            "disc_loss is 1.2077\n",
            "current step is 230\n",
            "gen_loss is 0.8948\n",
            "disc_loss is 1.2232\n",
            "current step is 231\n",
            "gen_loss is 0.8932\n",
            "disc_loss is 1.2041\n",
            "current step is 232\n",
            "gen_loss is 0.8677\n",
            "disc_loss is 1.1840\n",
            "current step is 233\n",
            "gen_loss is 0.8845\n",
            "disc_loss is 1.1497\n",
            "current step is 234\n",
            "gen_loss is 0.9148\n",
            "disc_loss is 1.1918\n",
            "Current epoch 8 is\n",
            "The train loss value for epoch 8 is 0.9688\n",
            "The test loss value for epoch 8 is 1.1981\n",
            "Time for epoch 8 is 1087.8867826461792 sec\n",
            "current step is 0\n",
            "gen_loss is 0.8976\n",
            "disc_loss is 1.1502\n",
            "current step is 1\n",
            "gen_loss is 0.9212\n",
            "disc_loss is 1.1320\n",
            "current step is 2\n",
            "gen_loss is 0.9528\n",
            "disc_loss is 1.1179\n",
            "current step is 3\n",
            "gen_loss is 0.9570\n",
            "disc_loss is 1.1502\n",
            "current step is 4\n",
            "gen_loss is 0.9629\n",
            "disc_loss is 1.0876\n",
            "current step is 5\n",
            "gen_loss is 0.9683\n",
            "disc_loss is 1.1339\n",
            "current step is 6\n",
            "gen_loss is 0.9844\n",
            "disc_loss is 1.0334\n",
            "current step is 7\n",
            "gen_loss is 0.9577\n",
            "disc_loss is 1.0500\n",
            "current step is 8\n",
            "gen_loss is 0.9753\n",
            "disc_loss is 1.0801\n",
            "current step is 9\n",
            "gen_loss is 0.9966\n",
            "disc_loss is 1.0234\n",
            "current step is 10\n",
            "gen_loss is 0.9918\n",
            "disc_loss is 1.0920\n",
            "current step is 11\n",
            "gen_loss is 0.9913\n",
            "disc_loss is 1.0552\n",
            "current step is 12\n",
            "gen_loss is 0.9625\n",
            "disc_loss is 1.0839\n",
            "current step is 13\n",
            "gen_loss is 0.9773\n",
            "disc_loss is 1.0388\n",
            "current step is 14\n",
            "gen_loss is 0.9879\n",
            "disc_loss is 1.0485\n",
            "current step is 15\n",
            "gen_loss is 0.9741\n",
            "disc_loss is 1.0410\n",
            "current step is 16\n",
            "gen_loss is 0.9695\n",
            "disc_loss is 1.0398\n",
            "current step is 17\n",
            "gen_loss is 0.9650\n",
            "disc_loss is 1.0798\n",
            "current step is 18\n",
            "gen_loss is 0.9606\n",
            "disc_loss is 1.0082\n",
            "current step is 19\n",
            "gen_loss is 0.9539\n",
            "disc_loss is 1.0790\n",
            "current step is 20\n",
            "gen_loss is 0.9599\n",
            "disc_loss is 1.0476\n",
            "current step is 21\n",
            "gen_loss is 0.9597\n",
            "disc_loss is 1.0392\n",
            "current step is 22\n",
            "gen_loss is 0.9660\n",
            "disc_loss is 1.0541\n",
            "current step is 23\n",
            "gen_loss is 0.9906\n",
            "disc_loss is 1.0311\n",
            "current step is 24\n",
            "gen_loss is 1.0053\n",
            "disc_loss is 1.0353\n",
            "current step is 25\n",
            "gen_loss is 0.9715\n",
            "disc_loss is 1.1124\n",
            "current step is 26\n",
            "gen_loss is 0.9595\n",
            "disc_loss is 1.0464\n",
            "current step is 27\n",
            "gen_loss is 0.9929\n",
            "disc_loss is 1.0184\n",
            "current step is 28\n",
            "gen_loss is 0.9693\n",
            "disc_loss is 1.0512\n",
            "current step is 29\n",
            "gen_loss is 0.9824\n",
            "disc_loss is 1.0440\n",
            "current step is 30\n",
            "gen_loss is 0.9683\n",
            "disc_loss is 1.0583\n",
            "current step is 31\n",
            "gen_loss is 0.9744\n",
            "disc_loss is 1.0878\n",
            "current step is 32\n",
            "gen_loss is 0.9799\n",
            "disc_loss is 1.0717\n",
            "current step is 33\n",
            "gen_loss is 0.9525\n",
            "disc_loss is 1.0597\n",
            "current step is 34\n",
            "gen_loss is 0.9351\n",
            "disc_loss is 1.0905\n",
            "current step is 35\n",
            "gen_loss is 0.9338\n",
            "disc_loss is 1.0590\n",
            "current step is 36\n",
            "gen_loss is 0.9486\n",
            "disc_loss is 1.0429\n",
            "current step is 37\n",
            "gen_loss is 0.9358\n",
            "disc_loss is 1.1115\n",
            "current step is 38\n",
            "gen_loss is 0.9294\n",
            "disc_loss is 1.1055\n",
            "current step is 39\n",
            "gen_loss is 0.9280\n",
            "disc_loss is 1.1418\n",
            "current step is 40\n",
            "gen_loss is 0.9189\n",
            "disc_loss is 1.0736\n",
            "current step is 41\n",
            "gen_loss is 0.9241\n",
            "disc_loss is 1.1345\n",
            "current step is 42\n",
            "gen_loss is 0.9332\n",
            "disc_loss is 1.1467\n",
            "current step is 43\n",
            "gen_loss is 0.9442\n",
            "disc_loss is 1.1045\n",
            "current step is 44\n",
            "gen_loss is 0.8901\n",
            "disc_loss is 1.1672\n",
            "current step is 45\n",
            "gen_loss is 0.9188\n",
            "disc_loss is 1.1565\n",
            "current step is 46\n",
            "gen_loss is 0.9296\n",
            "disc_loss is 1.1227\n",
            "current step is 47\n",
            "gen_loss is 0.9400\n",
            "disc_loss is 1.1197\n",
            "current step is 48\n",
            "gen_loss is 0.9158\n",
            "disc_loss is 1.2089\n",
            "current step is 49\n",
            "gen_loss is 0.8982\n",
            "disc_loss is 1.1683\n",
            "current step is 50\n",
            "gen_loss is 0.8996\n",
            "disc_loss is 1.1981\n",
            "current step is 51\n",
            "gen_loss is 0.8681\n",
            "disc_loss is 1.2211\n",
            "current step is 52\n",
            "gen_loss is 0.8683\n",
            "disc_loss is 1.2873\n",
            "current step is 53\n",
            "gen_loss is 0.8206\n",
            "disc_loss is 1.2427\n",
            "current step is 54\n",
            "gen_loss is 0.7871\n",
            "disc_loss is 1.2448\n",
            "current step is 55\n",
            "gen_loss is 0.8001\n",
            "disc_loss is 1.2391\n",
            "current step is 56\n",
            "gen_loss is 0.7671\n",
            "disc_loss is 1.2851\n",
            "current step is 57\n",
            "gen_loss is 0.7894\n",
            "disc_loss is 1.3059\n",
            "current step is 58\n",
            "gen_loss is 0.7701\n",
            "disc_loss is 1.3174\n",
            "current step is 59\n",
            "gen_loss is 0.7573\n",
            "disc_loss is 1.3414\n",
            "current step is 60\n",
            "gen_loss is 0.7798\n",
            "disc_loss is 1.3781\n",
            "current step is 61\n",
            "gen_loss is 0.7304\n",
            "disc_loss is 1.3717\n",
            "current step is 62\n",
            "gen_loss is 0.7885\n",
            "disc_loss is 1.3679\n",
            "current step is 63\n",
            "gen_loss is 0.7580\n",
            "disc_loss is 1.3055\n",
            "current step is 64\n",
            "gen_loss is 0.7562\n",
            "disc_loss is 1.4290\n",
            "current step is 65\n",
            "gen_loss is 0.7531\n",
            "disc_loss is 1.4340\n",
            "current step is 66\n",
            "gen_loss is 0.7544\n",
            "disc_loss is 1.4702\n",
            "current step is 67\n",
            "gen_loss is 0.7431\n",
            "disc_loss is 1.4179\n",
            "current step is 68\n",
            "gen_loss is 0.7384\n",
            "disc_loss is 1.4158\n",
            "current step is 69\n",
            "gen_loss is 0.7164\n",
            "disc_loss is 1.4600\n",
            "current step is 70\n",
            "gen_loss is 0.7199\n",
            "disc_loss is 1.4436\n",
            "current step is 71\n",
            "gen_loss is 0.7215\n",
            "disc_loss is 1.5372\n",
            "current step is 72\n",
            "gen_loss is 0.7032\n",
            "disc_loss is 1.4773\n",
            "current step is 73\n",
            "gen_loss is 0.6944\n",
            "disc_loss is 1.5571\n",
            "current step is 74\n",
            "gen_loss is 0.6778\n",
            "disc_loss is 1.5043\n",
            "current step is 75\n",
            "gen_loss is 0.6796\n",
            "disc_loss is 1.4623\n",
            "current step is 76\n",
            "gen_loss is 0.6781\n",
            "disc_loss is 1.5318\n",
            "current step is 77\n",
            "gen_loss is 0.6581\n",
            "disc_loss is 1.5418\n",
            "current step is 78\n",
            "gen_loss is 0.6472\n",
            "disc_loss is 1.5385\n",
            "current step is 79\n",
            "gen_loss is 0.6546\n",
            "disc_loss is 1.4871\n",
            "current step is 80\n",
            "gen_loss is 0.6767\n",
            "disc_loss is 1.5003\n",
            "current step is 81\n",
            "gen_loss is 0.6857\n",
            "disc_loss is 1.4993\n",
            "current step is 82\n",
            "gen_loss is 0.6689\n",
            "disc_loss is 1.4969\n",
            "current step is 83\n",
            "gen_loss is 0.6753\n",
            "disc_loss is 1.5033\n",
            "current step is 84\n",
            "gen_loss is 0.6848\n",
            "disc_loss is 1.5160\n",
            "current step is 85\n",
            "gen_loss is 0.6964\n",
            "disc_loss is 1.5284\n",
            "current step is 86\n",
            "gen_loss is 0.6808\n",
            "disc_loss is 1.5449\n",
            "current step is 87\n",
            "gen_loss is 0.6843\n",
            "disc_loss is 1.5274\n",
            "current step is 88\n",
            "gen_loss is 0.6901\n",
            "disc_loss is 1.5125\n",
            "current step is 89\n",
            "gen_loss is 0.6992\n",
            "disc_loss is 1.5091\n",
            "current step is 90\n",
            "gen_loss is 0.6976\n",
            "disc_loss is 1.4981\n",
            "current step is 91\n",
            "gen_loss is 0.7043\n",
            "disc_loss is 1.5139\n",
            "current step is 92\n",
            "gen_loss is 0.6882\n",
            "disc_loss is 1.4689\n",
            "current step is 93\n",
            "gen_loss is 0.7101\n",
            "disc_loss is 1.4792\n",
            "current step is 94\n",
            "gen_loss is 0.7174\n",
            "disc_loss is 1.4708\n",
            "current step is 95\n",
            "gen_loss is 0.7094\n",
            "disc_loss is 1.4891\n",
            "current step is 96\n",
            "gen_loss is 0.7062\n",
            "disc_loss is 1.4188\n",
            "current step is 97\n",
            "gen_loss is 0.7217\n",
            "disc_loss is 1.4032\n",
            "current step is 98\n",
            "gen_loss is 0.7141\n",
            "disc_loss is 1.4234\n",
            "current step is 99\n",
            "gen_loss is 0.7085\n",
            "disc_loss is 1.4327\n",
            "current step is 100\n",
            "gen_loss is 0.7214\n",
            "disc_loss is 1.4087\n",
            "current step is 101\n",
            "gen_loss is 0.7385\n",
            "disc_loss is 1.3584\n",
            "current step is 102\n",
            "gen_loss is 0.7361\n",
            "disc_loss is 1.3898\n",
            "current step is 103\n",
            "gen_loss is 0.7471\n",
            "disc_loss is 1.3670\n",
            "current step is 104\n",
            "gen_loss is 0.7411\n",
            "disc_loss is 1.3578\n",
            "current step is 105\n",
            "gen_loss is 0.7717\n",
            "disc_loss is 1.3498\n",
            "current step is 106\n",
            "gen_loss is 0.7886\n",
            "disc_loss is 1.3366\n",
            "current step is 107\n",
            "gen_loss is 0.8035\n",
            "disc_loss is 1.2701\n",
            "current step is 108\n",
            "gen_loss is 0.8182\n",
            "disc_loss is 1.3063\n",
            "current step is 109\n",
            "gen_loss is 0.8275\n",
            "disc_loss is 1.2474\n",
            "current step is 110\n",
            "gen_loss is 0.8337\n",
            "disc_loss is 1.2490\n",
            "current step is 111\n",
            "gen_loss is 0.8549\n",
            "disc_loss is 1.2787\n",
            "current step is 112\n",
            "gen_loss is 0.8449\n",
            "disc_loss is 1.2545\n",
            "current step is 113\n",
            "gen_loss is 0.8555\n",
            "disc_loss is 1.2137\n",
            "current step is 114\n",
            "gen_loss is 0.8584\n",
            "disc_loss is 1.2077\n",
            "current step is 115\n",
            "gen_loss is 0.8469\n",
            "disc_loss is 1.2404\n",
            "current step is 116\n",
            "gen_loss is 0.8942\n",
            "disc_loss is 1.1802\n",
            "current step is 117\n",
            "gen_loss is 0.9037\n",
            "disc_loss is 1.1500\n",
            "current step is 118\n",
            "gen_loss is 0.8778\n",
            "disc_loss is 1.1687\n",
            "current step is 119\n",
            "gen_loss is 0.9120\n",
            "disc_loss is 1.1777\n",
            "current step is 120\n",
            "gen_loss is 0.9255\n",
            "disc_loss is 1.1415\n",
            "current step is 121\n",
            "gen_loss is 0.9173\n",
            "disc_loss is 1.1422\n",
            "current step is 122\n",
            "gen_loss is 0.9435\n",
            "disc_loss is 1.1195\n",
            "current step is 123\n",
            "gen_loss is 0.9633\n",
            "disc_loss is 1.0838\n",
            "current step is 124\n",
            "gen_loss is 0.9602\n",
            "disc_loss is 1.1108\n",
            "current step is 125\n",
            "gen_loss is 0.9642\n",
            "disc_loss is 1.1304\n",
            "current step is 126\n",
            "gen_loss is 1.0038\n",
            "disc_loss is 1.0955\n",
            "current step is 127\n",
            "gen_loss is 0.9848\n",
            "disc_loss is 1.1429\n",
            "current step is 128\n",
            "gen_loss is 0.9762\n",
            "disc_loss is 1.0413\n",
            "current step is 129\n",
            "gen_loss is 0.9899\n",
            "disc_loss is 1.0535\n",
            "current step is 130\n",
            "gen_loss is 0.9750\n",
            "disc_loss is 1.1024\n",
            "current step is 131\n",
            "gen_loss is 1.0090\n",
            "disc_loss is 1.0450\n",
            "current step is 132\n",
            "gen_loss is 0.9920\n",
            "disc_loss is 1.0839\n",
            "current step is 133\n",
            "gen_loss is 0.9697\n",
            "disc_loss is 1.0814\n",
            "current step is 134\n",
            "gen_loss is 0.9608\n",
            "disc_loss is 1.1063\n",
            "current step is 135\n",
            "gen_loss is 0.9880\n",
            "disc_loss is 1.0648\n",
            "current step is 136\n",
            "gen_loss is 0.9875\n",
            "disc_loss is 1.0620\n",
            "current step is 137\n",
            "gen_loss is 0.9735\n",
            "disc_loss is 1.0854\n",
            "current step is 138\n",
            "gen_loss is 0.9819\n",
            "disc_loss is 1.0846\n",
            "current step is 139\n",
            "gen_loss is 0.9793\n",
            "disc_loss is 1.1032\n",
            "current step is 140\n",
            "gen_loss is 0.9808\n",
            "disc_loss is 1.0480\n",
            "current step is 141\n",
            "gen_loss is 0.9439\n",
            "disc_loss is 1.0908\n",
            "current step is 142\n",
            "gen_loss is 0.9568\n",
            "disc_loss is 1.0693\n",
            "current step is 143\n",
            "gen_loss is 0.9546\n",
            "disc_loss is 1.0736\n",
            "current step is 144\n",
            "gen_loss is 0.9166\n",
            "disc_loss is 1.1096\n",
            "current step is 145\n",
            "gen_loss is 0.9539\n",
            "disc_loss is 1.0537\n",
            "current step is 146\n",
            "gen_loss is 0.9435\n",
            "disc_loss is 1.0768\n",
            "current step is 147\n",
            "gen_loss is 0.9393\n",
            "disc_loss is 1.1134\n",
            "current step is 148\n",
            "gen_loss is 0.9187\n",
            "disc_loss is 1.1315\n",
            "current step is 149\n",
            "gen_loss is 0.8936\n",
            "disc_loss is 1.1649\n",
            "current step is 150\n",
            "gen_loss is 0.8896\n",
            "disc_loss is 1.1305\n",
            "current step is 151\n",
            "gen_loss is 0.8566\n",
            "disc_loss is 1.1891\n",
            "current step is 152\n",
            "gen_loss is 0.9032\n",
            "disc_loss is 1.1173\n",
            "current step is 153\n",
            "gen_loss is 0.8613\n",
            "disc_loss is 1.2008\n",
            "current step is 154\n",
            "gen_loss is 0.8838\n",
            "disc_loss is 1.1576\n",
            "current step is 155\n",
            "gen_loss is 0.8863\n",
            "disc_loss is 1.2017\n",
            "current step is 156\n",
            "gen_loss is 0.8736\n",
            "disc_loss is 1.1777\n",
            "current step is 157\n",
            "gen_loss is 0.8660\n",
            "disc_loss is 1.1840\n",
            "current step is 158\n",
            "gen_loss is 0.8777\n",
            "disc_loss is 1.1820\n",
            "current step is 159\n",
            "gen_loss is 0.8975\n",
            "disc_loss is 1.1851\n",
            "current step is 160\n",
            "gen_loss is 0.8905\n",
            "disc_loss is 1.1992\n",
            "current step is 161\n",
            "gen_loss is 0.9196\n",
            "disc_loss is 1.1905\n",
            "current step is 162\n",
            "gen_loss is 0.9008\n",
            "disc_loss is 1.2170\n",
            "current step is 163\n",
            "gen_loss is 0.8868\n",
            "disc_loss is 1.2515\n",
            "current step is 164\n",
            "gen_loss is 0.8823\n",
            "disc_loss is 1.2925\n",
            "current step is 165\n",
            "gen_loss is 0.8913\n",
            "disc_loss is 1.2824\n",
            "current step is 166\n",
            "gen_loss is 0.8806\n",
            "disc_loss is 1.2941\n",
            "current step is 167\n",
            "gen_loss is 0.8622\n",
            "disc_loss is 1.2969\n",
            "current step is 168\n",
            "gen_loss is 0.8253\n",
            "disc_loss is 1.3433\n",
            "current step is 169\n",
            "gen_loss is 0.8568\n",
            "disc_loss is 1.2425\n",
            "current step is 170\n",
            "gen_loss is 0.8463\n",
            "disc_loss is 1.2344\n",
            "current step is 171\n",
            "gen_loss is 0.8330\n",
            "disc_loss is 1.2971\n",
            "current step is 172\n",
            "gen_loss is 0.8338\n",
            "disc_loss is 1.3046\n",
            "current step is 173\n",
            "gen_loss is 0.7976\n",
            "disc_loss is 1.3452\n",
            "current step is 174\n",
            "gen_loss is 0.8083\n",
            "disc_loss is 1.3244\n",
            "current step is 175\n",
            "gen_loss is 0.7800\n",
            "disc_loss is 1.3287\n",
            "current step is 176\n",
            "gen_loss is 0.8050\n",
            "disc_loss is 1.3015\n",
            "current step is 177\n",
            "gen_loss is 0.8323\n",
            "disc_loss is 1.3209\n",
            "current step is 178\n",
            "gen_loss is 0.8127\n",
            "disc_loss is 1.3771\n",
            "current step is 179\n",
            "gen_loss is 0.8301\n",
            "disc_loss is 1.3813\n",
            "current step is 180\n",
            "gen_loss is 0.7892\n",
            "disc_loss is 1.4124\n",
            "current step is 181\n",
            "gen_loss is 0.8366\n",
            "disc_loss is 1.3215\n",
            "current step is 182\n",
            "gen_loss is 0.7935\n",
            "disc_loss is 1.4459\n",
            "current step is 183\n",
            "gen_loss is 0.7873\n",
            "disc_loss is 1.4119\n",
            "current step is 184\n",
            "gen_loss is 0.7982\n",
            "disc_loss is 1.4380\n",
            "current step is 185\n",
            "gen_loss is 0.8244\n",
            "disc_loss is 1.3364\n",
            "current step is 186\n",
            "gen_loss is 0.8107\n",
            "disc_loss is 1.3446\n",
            "current step is 187\n",
            "gen_loss is 0.8326\n",
            "disc_loss is 1.3347\n",
            "current step is 188\n",
            "gen_loss is 0.8314\n",
            "disc_loss is 1.3779\n",
            "current step is 189\n",
            "gen_loss is 0.8260\n",
            "disc_loss is 1.3760\n",
            "current step is 190\n",
            "gen_loss is 0.8114\n",
            "disc_loss is 1.3595\n",
            "current step is 191\n",
            "gen_loss is 0.8280\n",
            "disc_loss is 1.3790\n",
            "current step is 192\n",
            "gen_loss is 0.8102\n",
            "disc_loss is 1.3520\n",
            "current step is 193\n",
            "gen_loss is 0.8396\n",
            "disc_loss is 1.3342\n",
            "current step is 194\n",
            "gen_loss is 0.8214\n",
            "disc_loss is 1.2962\n",
            "current step is 195\n",
            "gen_loss is 0.7940\n",
            "disc_loss is 1.3602\n",
            "current step is 196\n",
            "gen_loss is 0.8043\n",
            "disc_loss is 1.3260\n",
            "current step is 197\n",
            "gen_loss is 0.8033\n",
            "disc_loss is 1.3388\n",
            "current step is 198\n",
            "gen_loss is 0.8107\n",
            "disc_loss is 1.3146\n",
            "current step is 199\n",
            "gen_loss is 0.7895\n",
            "disc_loss is 1.2953\n",
            "current step is 200\n",
            "gen_loss is 0.8024\n",
            "disc_loss is 1.2576\n",
            "current step is 201\n",
            "gen_loss is 0.7939\n",
            "disc_loss is 1.2571\n",
            "current step is 202\n",
            "gen_loss is 0.8117\n",
            "disc_loss is 1.2360\n",
            "current step is 203\n",
            "gen_loss is 0.8239\n",
            "disc_loss is 1.2566\n",
            "current step is 204\n",
            "gen_loss is 0.8311\n",
            "disc_loss is 1.2445\n",
            "current step is 205\n",
            "gen_loss is 0.8535\n",
            "disc_loss is 1.2032\n",
            "current step is 206\n",
            "gen_loss is 0.8746\n",
            "disc_loss is 1.2375\n",
            "current step is 207\n",
            "gen_loss is 0.8897\n",
            "disc_loss is 1.2019\n",
            "current step is 208\n",
            "gen_loss is 0.9095\n",
            "disc_loss is 1.2010\n",
            "current step is 209\n",
            "gen_loss is 0.9474\n",
            "disc_loss is 1.1314\n",
            "current step is 210\n",
            "gen_loss is 0.9112\n",
            "disc_loss is 1.1367\n",
            "current step is 211\n",
            "gen_loss is 0.9151\n",
            "disc_loss is 1.1538\n",
            "current step is 212\n",
            "gen_loss is 0.9413\n",
            "disc_loss is 1.1625\n",
            "current step is 213\n",
            "gen_loss is 0.9342\n",
            "disc_loss is 1.1559\n",
            "current step is 214\n",
            "gen_loss is 0.9328\n",
            "disc_loss is 1.1146\n",
            "current step is 215\n",
            "gen_loss is 0.9488\n",
            "disc_loss is 1.0846\n",
            "current step is 216\n",
            "gen_loss is 0.9698\n",
            "disc_loss is 1.0620\n",
            "current step is 217\n",
            "gen_loss is 0.9422\n",
            "disc_loss is 1.1220\n",
            "current step is 218\n",
            "gen_loss is 0.9520\n",
            "disc_loss is 1.0988\n",
            "current step is 219\n",
            "gen_loss is 0.9872\n",
            "disc_loss is 1.0607\n",
            "current step is 220\n",
            "gen_loss is 0.9529\n",
            "disc_loss is 1.0852\n",
            "current step is 221\n",
            "gen_loss is 0.9870\n",
            "disc_loss is 1.1019\n",
            "current step is 222\n",
            "gen_loss is 1.0165\n",
            "disc_loss is 1.0336\n",
            "current step is 223\n",
            "gen_loss is 1.0099\n",
            "disc_loss is 1.0731\n",
            "current step is 224\n",
            "gen_loss is 1.0352\n",
            "disc_loss is 1.0546\n",
            "current step is 225\n",
            "gen_loss is 1.0495\n",
            "disc_loss is 1.0127\n",
            "current step is 226\n",
            "gen_loss is 1.0204\n",
            "disc_loss is 1.0249\n",
            "current step is 227\n",
            "gen_loss is 1.0210\n",
            "disc_loss is 1.0028\n",
            "current step is 228\n",
            "gen_loss is 1.0336\n",
            "disc_loss is 1.0450\n",
            "current step is 229\n",
            "gen_loss is 1.0199\n",
            "disc_loss is 0.9851\n",
            "current step is 230\n",
            "gen_loss is 1.0388\n",
            "disc_loss is 0.9910\n",
            "current step is 231\n",
            "gen_loss is 1.0286\n",
            "disc_loss is 1.0080\n",
            "current step is 232\n",
            "gen_loss is 1.0178\n",
            "disc_loss is 1.0015\n",
            "current step is 233\n",
            "gen_loss is 1.0134\n",
            "disc_loss is 0.9651\n",
            "current step is 234\n",
            "gen_loss is 1.0256\n",
            "disc_loss is 1.0610\n",
            "Current epoch 9 is\n",
            "The train loss value for epoch 9 is 0.8708\n",
            "The test loss value for epoch 9 is 1.2240\n",
            "Time for epoch 9 is 1083.5759589672089 sec\n",
            "current step is 0\n",
            "gen_loss is 0.9989\n",
            "disc_loss is 0.9849\n",
            "current step is 1\n",
            "gen_loss is 1.0673\n",
            "disc_loss is 1.0159\n",
            "current step is 2\n",
            "gen_loss is 1.0327\n",
            "disc_loss is 1.0172\n",
            "current step is 3\n",
            "gen_loss is 1.0530\n",
            "disc_loss is 1.0291\n",
            "current step is 4\n",
            "gen_loss is 1.0020\n",
            "disc_loss is 1.0508\n",
            "current step is 5\n",
            "gen_loss is 1.0191\n",
            "disc_loss is 1.0903\n",
            "current step is 6\n",
            "gen_loss is 1.0530\n",
            "disc_loss is 0.9958\n",
            "current step is 7\n",
            "gen_loss is 1.0165\n",
            "disc_loss is 1.0406\n",
            "current step is 8\n",
            "gen_loss is 1.0050\n",
            "disc_loss is 1.0893\n",
            "current step is 9\n",
            "gen_loss is 1.0096\n",
            "disc_loss is 1.0835\n",
            "current step is 10\n",
            "gen_loss is 1.0004\n",
            "disc_loss is 1.1410\n",
            "current step is 11\n",
            "gen_loss is 0.9915\n",
            "disc_loss is 1.1564\n",
            "current step is 12\n",
            "gen_loss is 0.9874\n",
            "disc_loss is 1.1356\n",
            "current step is 13\n",
            "gen_loss is 0.9468\n",
            "disc_loss is 1.1514\n",
            "current step is 14\n",
            "gen_loss is 0.9450\n",
            "disc_loss is 1.1709\n",
            "current step is 15\n",
            "gen_loss is 0.9331\n",
            "disc_loss is 1.1920\n",
            "current step is 16\n",
            "gen_loss is 0.9198\n",
            "disc_loss is 1.2245\n",
            "current step is 17\n",
            "gen_loss is 0.8635\n",
            "disc_loss is 1.3373\n",
            "current step is 18\n",
            "gen_loss is 0.8768\n",
            "disc_loss is 1.2606\n",
            "current step is 19\n",
            "gen_loss is 0.8946\n",
            "disc_loss is 1.3651\n",
            "current step is 20\n",
            "gen_loss is 0.8825\n",
            "disc_loss is 1.3040\n",
            "current step is 21\n",
            "gen_loss is 0.8681\n",
            "disc_loss is 1.3685\n",
            "current step is 22\n",
            "gen_loss is 0.8662\n",
            "disc_loss is 1.3903\n",
            "current step is 23\n",
            "gen_loss is 0.8487\n",
            "disc_loss is 1.3866\n",
            "current step is 24\n",
            "gen_loss is 0.8803\n",
            "disc_loss is 1.4038\n",
            "current step is 25\n",
            "gen_loss is 0.8211\n",
            "disc_loss is 1.5328\n",
            "current step is 26\n",
            "gen_loss is 0.8291\n",
            "disc_loss is 1.4643\n",
            "current step is 27\n",
            "gen_loss is 0.8012\n",
            "disc_loss is 1.4502\n",
            "current step is 28\n",
            "gen_loss is 0.7874\n",
            "disc_loss is 1.5291\n",
            "current step is 29\n",
            "gen_loss is 0.7626\n",
            "disc_loss is 1.5481\n",
            "current step is 30\n",
            "gen_loss is 0.7918\n",
            "disc_loss is 1.5297\n",
            "current step is 31\n",
            "gen_loss is 0.7666\n",
            "disc_loss is 1.6232\n",
            "current step is 32\n",
            "gen_loss is 0.7611\n",
            "disc_loss is 1.6139\n",
            "current step is 33\n",
            "gen_loss is 0.7883\n",
            "disc_loss is 1.5470\n",
            "current step is 34\n",
            "gen_loss is 0.7958\n",
            "disc_loss is 1.5891\n",
            "current step is 35\n",
            "gen_loss is 0.7770\n",
            "disc_loss is 1.5367\n",
            "current step is 36\n",
            "gen_loss is 0.7531\n",
            "disc_loss is 1.5814\n",
            "current step is 37\n",
            "gen_loss is 0.8059\n",
            "disc_loss is 1.6259\n",
            "current step is 38\n",
            "gen_loss is 0.8124\n",
            "disc_loss is 1.5587\n",
            "current step is 39\n",
            "gen_loss is 0.8270\n",
            "disc_loss is 1.5983\n",
            "current step is 40\n",
            "gen_loss is 0.8223\n",
            "disc_loss is 1.4380\n",
            "current step is 41\n",
            "gen_loss is 0.7994\n",
            "disc_loss is 1.5191\n",
            "current step is 42\n",
            "gen_loss is 0.8288\n",
            "disc_loss is 1.5270\n",
            "current step is 43\n",
            "gen_loss is 0.8081\n",
            "disc_loss is 1.4713\n",
            "current step is 44\n",
            "gen_loss is 0.8041\n",
            "disc_loss is 1.4569\n",
            "current step is 45\n",
            "gen_loss is 0.8034\n",
            "disc_loss is 1.4496\n",
            "current step is 46\n",
            "gen_loss is 0.8083\n",
            "disc_loss is 1.4003\n",
            "current step is 47\n",
            "gen_loss is 0.8579\n",
            "disc_loss is 1.3842\n",
            "current step is 48\n",
            "gen_loss is 0.8333\n",
            "disc_loss is 1.4241\n",
            "current step is 49\n",
            "gen_loss is 0.8504\n",
            "disc_loss is 1.4074\n",
            "current step is 50\n",
            "gen_loss is 0.8537\n",
            "disc_loss is 1.4350\n",
            "current step is 51\n",
            "gen_loss is 0.8450\n",
            "disc_loss is 1.3634\n",
            "current step is 52\n",
            "gen_loss is 0.8506\n",
            "disc_loss is 1.3658\n",
            "current step is 53\n",
            "gen_loss is 0.8013\n",
            "disc_loss is 1.3554\n",
            "current step is 54\n",
            "gen_loss is 0.8264\n",
            "disc_loss is 1.2967\n",
            "current step is 55\n",
            "gen_loss is 0.8209\n",
            "disc_loss is 1.2830\n",
            "current step is 56\n",
            "gen_loss is 0.8185\n",
            "disc_loss is 1.3026\n",
            "current step is 57\n",
            "gen_loss is 0.8343\n",
            "disc_loss is 1.2746\n",
            "current step is 58\n",
            "gen_loss is 0.8498\n",
            "disc_loss is 1.2769\n",
            "current step is 59\n",
            "gen_loss is 0.8608\n",
            "disc_loss is 1.2261\n",
            "current step is 60\n",
            "gen_loss is 0.8732\n",
            "disc_loss is 1.2538\n",
            "current step is 61\n",
            "gen_loss is 0.9136\n",
            "disc_loss is 1.1801\n",
            "current step is 62\n",
            "gen_loss is 0.9024\n",
            "disc_loss is 1.2027\n",
            "current step is 63\n",
            "gen_loss is 0.9293\n",
            "disc_loss is 1.1441\n",
            "current step is 64\n",
            "gen_loss is 0.8992\n",
            "disc_loss is 1.1685\n",
            "current step is 65\n",
            "gen_loss is 0.9136\n",
            "disc_loss is 1.1836\n",
            "current step is 66\n",
            "gen_loss is 0.9277\n",
            "disc_loss is 1.2119\n",
            "current step is 67\n",
            "gen_loss is 0.9356\n",
            "disc_loss is 1.1425\n",
            "current step is 68\n",
            "gen_loss is 0.9389\n",
            "disc_loss is 1.1348\n",
            "current step is 69\n",
            "gen_loss is 0.9658\n",
            "disc_loss is 1.0960\n",
            "current step is 70\n",
            "gen_loss is 0.9319\n",
            "disc_loss is 1.1482\n",
            "current step is 71\n",
            "gen_loss is 0.9197\n",
            "disc_loss is 1.1921\n",
            "current step is 72\n",
            "gen_loss is 0.9110\n",
            "disc_loss is 1.1762\n",
            "current step is 73\n",
            "gen_loss is 0.9347\n",
            "disc_loss is 1.1927\n",
            "current step is 74\n",
            "gen_loss is 0.9157\n",
            "disc_loss is 1.1662\n",
            "current step is 75\n",
            "gen_loss is 0.9031\n",
            "disc_loss is 1.1217\n",
            "current step is 76\n",
            "gen_loss is 0.9306\n",
            "disc_loss is 1.1297\n",
            "current step is 77\n",
            "gen_loss is 0.9088\n",
            "disc_loss is 1.1120\n",
            "current step is 78\n",
            "gen_loss is 0.9380\n",
            "disc_loss is 1.1195\n",
            "current step is 79\n",
            "gen_loss is 0.9629\n",
            "disc_loss is 1.1477\n",
            "current step is 80\n",
            "gen_loss is 0.9606\n",
            "disc_loss is 1.1529\n",
            "current step is 81\n",
            "gen_loss is 0.9147\n",
            "disc_loss is 1.1416\n",
            "current step is 82\n",
            "gen_loss is 0.9388\n",
            "disc_loss is 1.1249\n",
            "current step is 83\n",
            "gen_loss is 0.9507\n",
            "disc_loss is 1.1170\n",
            "current step is 84\n",
            "gen_loss is 0.9971\n",
            "disc_loss is 1.0716\n",
            "current step is 85\n",
            "gen_loss is 0.9674\n",
            "disc_loss is 1.1428\n",
            "current step is 86\n",
            "gen_loss is 0.9756\n",
            "disc_loss is 1.1167\n",
            "current step is 87\n",
            "gen_loss is 0.9899\n",
            "disc_loss is 1.0861\n",
            "current step is 88\n",
            "gen_loss is 0.9979\n",
            "disc_loss is 1.1107\n",
            "current step is 89\n",
            "gen_loss is 0.9973\n",
            "disc_loss is 1.0888\n",
            "current step is 90\n",
            "gen_loss is 0.9863\n",
            "disc_loss is 1.1250\n",
            "current step is 91\n",
            "gen_loss is 0.9841\n",
            "disc_loss is 1.1261\n",
            "current step is 92\n",
            "gen_loss is 1.0040\n",
            "disc_loss is 1.1378\n",
            "current step is 93\n",
            "gen_loss is 1.0046\n",
            "disc_loss is 1.1334\n",
            "current step is 94\n",
            "gen_loss is 1.0000\n",
            "disc_loss is 1.0877\n",
            "current step is 95\n",
            "gen_loss is 0.9681\n",
            "disc_loss is 1.1427\n",
            "current step is 96\n",
            "gen_loss is 0.9736\n",
            "disc_loss is 1.0924\n",
            "current step is 97\n",
            "gen_loss is 0.9993\n",
            "disc_loss is 1.0777\n",
            "current step is 98\n",
            "gen_loss is 0.9717\n",
            "disc_loss is 1.1511\n",
            "current step is 99\n",
            "gen_loss is 0.9910\n",
            "disc_loss is 1.1097\n",
            "current step is 100\n",
            "gen_loss is 0.9705\n",
            "disc_loss is 1.1292\n",
            "current step is 101\n",
            "gen_loss is 0.9846\n",
            "disc_loss is 1.0754\n",
            "current step is 102\n",
            "gen_loss is 0.9722\n",
            "disc_loss is 1.1270\n",
            "current step is 103\n",
            "gen_loss is 0.9843\n",
            "disc_loss is 1.0922\n",
            "current step is 104\n",
            "gen_loss is 1.0030\n",
            "disc_loss is 1.1019\n",
            "current step is 105\n",
            "gen_loss is 0.9739\n",
            "disc_loss is 1.1613\n",
            "current step is 106\n",
            "gen_loss is 0.9804\n",
            "disc_loss is 1.1274\n",
            "current step is 107\n",
            "gen_loss is 0.9669\n",
            "disc_loss is 1.1257\n",
            "current step is 108\n",
            "gen_loss is 0.9949\n",
            "disc_loss is 1.1174\n",
            "current step is 109\n",
            "gen_loss is 0.9788\n",
            "disc_loss is 1.1238\n",
            "current step is 110\n",
            "gen_loss is 0.9650\n",
            "disc_loss is 1.0993\n",
            "current step is 111\n",
            "gen_loss is 0.9786\n",
            "disc_loss is 1.1793\n",
            "current step is 112\n",
            "gen_loss is 1.0163\n",
            "disc_loss is 1.0900\n",
            "current step is 113\n",
            "gen_loss is 1.0092\n",
            "disc_loss is 1.1323\n",
            "current step is 114\n",
            "gen_loss is 1.0039\n",
            "disc_loss is 1.0885\n",
            "current step is 115\n",
            "gen_loss is 0.9937\n",
            "disc_loss is 1.1199\n",
            "current step is 116\n",
            "gen_loss is 0.9946\n",
            "disc_loss is 1.1071\n",
            "current step is 117\n",
            "gen_loss is 0.9870\n",
            "disc_loss is 1.1190\n",
            "current step is 118\n",
            "gen_loss is 0.9849\n",
            "disc_loss is 1.1019\n",
            "current step is 119\n",
            "gen_loss is 0.9883\n",
            "disc_loss is 1.1317\n",
            "current step is 120\n",
            "gen_loss is 0.9714\n",
            "disc_loss is 1.1263\n",
            "current step is 121\n",
            "gen_loss is 1.0025\n",
            "disc_loss is 1.1180\n",
            "current step is 122\n",
            "gen_loss is 0.9838\n",
            "disc_loss is 1.1059\n",
            "current step is 123\n",
            "gen_loss is 0.9755\n",
            "disc_loss is 1.0917\n",
            "current step is 124\n",
            "gen_loss is 1.0073\n",
            "disc_loss is 1.1485\n",
            "current step is 125\n",
            "gen_loss is 0.9973\n",
            "disc_loss is 1.1564\n",
            "current step is 126\n",
            "gen_loss is 1.0326\n",
            "disc_loss is 1.1476\n",
            "current step is 127\n",
            "gen_loss is 1.0070\n",
            "disc_loss is 1.1897\n",
            "current step is 128\n",
            "gen_loss is 0.9768\n",
            "disc_loss is 1.1061\n",
            "current step is 129\n",
            "gen_loss is 0.9468\n",
            "disc_loss is 1.1990\n",
            "current step is 130\n",
            "gen_loss is 0.9661\n",
            "disc_loss is 1.1671\n",
            "current step is 131\n",
            "gen_loss is 0.9660\n",
            "disc_loss is 1.1512\n",
            "current step is 132\n",
            "gen_loss is 0.9348\n",
            "disc_loss is 1.1728\n",
            "current step is 133\n",
            "gen_loss is 0.9405\n",
            "disc_loss is 1.1752\n",
            "current step is 134\n",
            "gen_loss is 0.9179\n",
            "disc_loss is 1.2326\n",
            "current step is 135\n",
            "gen_loss is 0.9289\n",
            "disc_loss is 1.1855\n",
            "current step is 136\n",
            "gen_loss is 0.8986\n",
            "disc_loss is 1.2232\n",
            "current step is 137\n",
            "gen_loss is 0.9045\n",
            "disc_loss is 1.1887\n",
            "current step is 138\n",
            "gen_loss is 0.9155\n",
            "disc_loss is 1.2305\n",
            "current step is 139\n",
            "gen_loss is 0.9132\n",
            "disc_loss is 1.2190\n",
            "current step is 140\n",
            "gen_loss is 0.8872\n",
            "disc_loss is 1.2374\n",
            "current step is 141\n",
            "gen_loss is 0.8914\n",
            "disc_loss is 1.2210\n",
            "current step is 142\n",
            "gen_loss is 0.8769\n",
            "disc_loss is 1.2190\n",
            "current step is 143\n",
            "gen_loss is 0.8751\n",
            "disc_loss is 1.2346\n",
            "current step is 144\n",
            "gen_loss is 0.9043\n",
            "disc_loss is 1.2159\n",
            "current step is 145\n",
            "gen_loss is 0.9271\n",
            "disc_loss is 1.1963\n",
            "current step is 146\n",
            "gen_loss is 0.8985\n",
            "disc_loss is 1.2563\n",
            "current step is 147\n",
            "gen_loss is 0.9074\n",
            "disc_loss is 1.2376\n",
            "current step is 148\n",
            "gen_loss is 0.9068\n",
            "disc_loss is 1.2280\n",
            "current step is 149\n",
            "gen_loss is 0.8769\n",
            "disc_loss is 1.2932\n",
            "current step is 150\n",
            "gen_loss is 0.8614\n",
            "disc_loss is 1.1922\n",
            "current step is 151\n",
            "gen_loss is 0.8892\n",
            "disc_loss is 1.2522\n",
            "current step is 152\n",
            "gen_loss is 0.8739\n",
            "disc_loss is 1.2397\n",
            "current step is 153\n",
            "gen_loss is 0.8490\n",
            "disc_loss is 1.2599\n",
            "current step is 154\n",
            "gen_loss is 0.8737\n",
            "disc_loss is 1.2856\n",
            "current step is 155\n",
            "gen_loss is 0.8487\n",
            "disc_loss is 1.2483\n",
            "current step is 156\n",
            "gen_loss is 0.8425\n",
            "disc_loss is 1.2723\n",
            "current step is 157\n",
            "gen_loss is 0.8760\n",
            "disc_loss is 1.2203\n",
            "current step is 158\n",
            "gen_loss is 0.8526\n",
            "disc_loss is 1.2347\n",
            "current step is 159\n",
            "gen_loss is 0.8949\n",
            "disc_loss is 1.2311\n",
            "current step is 160\n",
            "gen_loss is 0.8670\n",
            "disc_loss is 1.2345\n",
            "current step is 161\n",
            "gen_loss is 0.9051\n",
            "disc_loss is 1.2216\n",
            "current step is 162\n",
            "gen_loss is 0.8891\n",
            "disc_loss is 1.2372\n",
            "current step is 163\n",
            "gen_loss is 0.9466\n",
            "disc_loss is 1.2104\n",
            "current step is 164\n",
            "gen_loss is 0.8963\n",
            "disc_loss is 1.2422\n",
            "current step is 165\n",
            "gen_loss is 0.9098\n",
            "disc_loss is 1.1929\n",
            "current step is 166\n",
            "gen_loss is 0.8952\n",
            "disc_loss is 1.2369\n",
            "current step is 167\n",
            "gen_loss is 0.8451\n",
            "disc_loss is 1.2501\n",
            "current step is 168\n",
            "gen_loss is 0.8587\n",
            "disc_loss is 1.2401\n",
            "current step is 169\n",
            "gen_loss is 0.8558\n",
            "disc_loss is 1.1402\n",
            "current step is 170\n",
            "gen_loss is 0.8339\n",
            "disc_loss is 1.2202\n",
            "current step is 171\n",
            "gen_loss is 0.8727\n",
            "disc_loss is 1.2134\n",
            "current step is 172\n",
            "gen_loss is 0.8420\n",
            "disc_loss is 1.1958\n",
            "current step is 173\n",
            "gen_loss is 0.8719\n",
            "disc_loss is 1.2082\n",
            "current step is 174\n",
            "gen_loss is 0.8429\n",
            "disc_loss is 1.1979\n",
            "current step is 175\n",
            "gen_loss is 0.8536\n",
            "disc_loss is 1.1730\n",
            "current step is 176\n",
            "gen_loss is 0.8632\n",
            "disc_loss is 1.1827\n",
            "current step is 177\n",
            "gen_loss is 0.8639\n",
            "disc_loss is 1.1698\n",
            "current step is 178\n",
            "gen_loss is 0.8528\n",
            "disc_loss is 1.1791\n",
            "current step is 179\n",
            "gen_loss is 0.8316\n",
            "disc_loss is 1.1764\n",
            "current step is 180\n",
            "gen_loss is 0.8618\n",
            "disc_loss is 1.2005\n",
            "current step is 181\n",
            "gen_loss is 0.8748\n",
            "disc_loss is 1.2007\n",
            "current step is 182\n",
            "gen_loss is 0.8837\n",
            "disc_loss is 1.1945\n",
            "current step is 183\n",
            "gen_loss is 0.8911\n",
            "disc_loss is 1.1591\n",
            "current step is 184\n",
            "gen_loss is 0.8767\n",
            "disc_loss is 1.1289\n",
            "current step is 185\n",
            "gen_loss is 0.8964\n",
            "disc_loss is 1.1568\n",
            "current step is 186\n",
            "gen_loss is 0.9024\n",
            "disc_loss is 1.1126\n",
            "current step is 187\n",
            "gen_loss is 0.8989\n",
            "disc_loss is 1.0988\n",
            "current step is 188\n",
            "gen_loss is 0.8986\n",
            "disc_loss is 1.1733\n",
            "current step is 189\n",
            "gen_loss is 0.9014\n",
            "disc_loss is 1.1369\n",
            "current step is 190\n",
            "gen_loss is 0.8772\n",
            "disc_loss is 1.1323\n",
            "current step is 191\n",
            "gen_loss is 0.8884\n",
            "disc_loss is 1.1569\n",
            "current step is 192\n",
            "gen_loss is 0.8635\n",
            "disc_loss is 1.1183\n",
            "current step is 193\n",
            "gen_loss is 0.8859\n",
            "disc_loss is 1.1280\n",
            "current step is 194\n",
            "gen_loss is 0.8885\n",
            "disc_loss is 1.1520\n",
            "current step is 195\n",
            "gen_loss is 0.9207\n",
            "disc_loss is 1.1181\n",
            "current step is 196\n",
            "gen_loss is 0.9103\n",
            "disc_loss is 1.1362\n",
            "current step is 197\n",
            "gen_loss is 0.9240\n",
            "disc_loss is 1.1174\n",
            "current step is 198\n",
            "gen_loss is 0.8981\n",
            "disc_loss is 1.1367\n",
            "current step is 199\n",
            "gen_loss is 0.9138\n",
            "disc_loss is 1.0956\n",
            "current step is 200\n",
            "gen_loss is 0.9325\n",
            "disc_loss is 1.0685\n",
            "current step is 201\n",
            "gen_loss is 0.9011\n",
            "disc_loss is 1.0693\n",
            "current step is 202\n",
            "gen_loss is 0.9269\n",
            "disc_loss is 1.0641\n",
            "current step is 203\n",
            "gen_loss is 0.9178\n",
            "disc_loss is 1.0479\n",
            "current step is 204\n",
            "gen_loss is 0.9157\n",
            "disc_loss is 1.0547\n",
            "current step is 205\n",
            "gen_loss is 0.9271\n",
            "disc_loss is 1.1308\n",
            "current step is 206\n",
            "gen_loss is 0.9812\n",
            "disc_loss is 1.0481\n",
            "current step is 207\n",
            "gen_loss is 0.9961\n",
            "disc_loss is 1.0338\n",
            "current step is 208\n",
            "gen_loss is 1.0212\n",
            "disc_loss is 1.0439\n",
            "current step is 209\n",
            "gen_loss is 1.0471\n",
            "disc_loss is 0.9847\n",
            "current step is 210\n",
            "gen_loss is 1.0253\n",
            "disc_loss is 1.0222\n",
            "current step is 211\n",
            "gen_loss is 1.0160\n",
            "disc_loss is 1.0603\n",
            "current step is 212\n",
            "gen_loss is 1.0137\n",
            "disc_loss is 1.0235\n",
            "current step is 213\n",
            "gen_loss is 1.0155\n",
            "disc_loss is 1.0062\n",
            "current step is 214\n",
            "gen_loss is 0.9804\n",
            "disc_loss is 1.0251\n",
            "current step is 215\n",
            "gen_loss is 1.0249\n",
            "disc_loss is 0.9982\n",
            "current step is 216\n",
            "gen_loss is 0.9787\n",
            "disc_loss is 1.0534\n",
            "current step is 217\n",
            "gen_loss is 1.0033\n",
            "disc_loss is 0.9797\n",
            "current step is 218\n",
            "gen_loss is 0.9973\n",
            "disc_loss is 1.0269\n",
            "current step is 219\n",
            "gen_loss is 1.0562\n",
            "disc_loss is 0.9903\n",
            "current step is 220\n",
            "gen_loss is 1.0826\n",
            "disc_loss is 0.9925\n",
            "current step is 221\n",
            "gen_loss is 1.0445\n",
            "disc_loss is 0.9834\n",
            "current step is 222\n",
            "gen_loss is 1.0879\n",
            "disc_loss is 0.9639\n",
            "current step is 223\n",
            "gen_loss is 1.0831\n",
            "disc_loss is 1.0251\n",
            "current step is 224\n",
            "gen_loss is 1.0931\n",
            "disc_loss is 0.9732\n",
            "current step is 225\n",
            "gen_loss is 1.1006\n",
            "disc_loss is 1.0088\n",
            "current step is 226\n",
            "gen_loss is 1.0628\n",
            "disc_loss is 1.0675\n",
            "current step is 227\n",
            "gen_loss is 1.0612\n",
            "disc_loss is 1.0192\n",
            "current step is 228\n",
            "gen_loss is 1.0517\n",
            "disc_loss is 1.0539\n",
            "current step is 229\n",
            "gen_loss is 1.0110\n",
            "disc_loss is 1.0494\n",
            "current step is 230\n",
            "gen_loss is 1.0264\n",
            "disc_loss is 1.0295\n",
            "current step is 231\n",
            "gen_loss is 0.9914\n",
            "disc_loss is 1.0669\n",
            "current step is 232\n",
            "gen_loss is 0.9998\n",
            "disc_loss is 1.0107\n",
            "current step is 233\n",
            "gen_loss is 0.9764\n",
            "disc_loss is 1.0004\n",
            "current step is 234\n",
            "gen_loss is 0.9918\n",
            "disc_loss is 1.1505\n",
            "Current epoch 10 is\n",
            "The train loss value for epoch 10 is 0.9259\n",
            "The test loss value for epoch 10 is 1.1893\n",
            "Time for epoch 10 is 1084.2827196121216 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuVrmgFsqUCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save all the images during the training/testing epochs\n",
        "#from google.colab import files\n",
        "#for epoch in range(1,16):\n",
        "  #files.download('image_at_epoch_{:04d}.png'.format(epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeDXvwJzuWb2",
        "colab_type": "code",
        "outputId": "7d93ddbf-6480-4948-f1c4-48606267bab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "epochs = 10\n",
        "display.clear_output(wait=True)\n",
        "generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOwAAADnCAYAAAAdFLrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZQhCGz2roZgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 288x288 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBHovSHTwz9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display a single image using the epoch number\n",
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP0UWLfXw47B",
        "colab_type": "code",
        "outputId": "0e0c6bfc-1028-4573-c38b-f49f6ca52622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "display_image(EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASAAAAEgCAYAAAAUg66AAABc/UlEQVR4nO29d4BU5fU+/szszvbC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\nDGqAFAqFY1ADpFAoHMP/B7QDGZQGCM0iAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=288x288 at 0x7F77B06B55F8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCoOtnvew-k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#save all training generated images as a gif\n",
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  last = -1\n",
        "  for i,filename in enumerate(filenames):\n",
        "    frame = 2*(i**0.5)\n",
        "    if round(frame) > round(last):\n",
        "      last = frame\n",
        "    else:\n",
        "      continue\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)\n",
        "\n",
        "import IPython\n",
        "if IPython.version_info > (6,2,0,''):\n",
        "  display.Image(filename=anim_file)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMYDuGRfxDoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "   pass\n",
        "else:\n",
        "  files.download(anim_file)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRHfVOurEb-a",
        "colab_type": "text"
      },
      "source": [
        "Plot the loss functions for the generator and discriminator. Label them accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYwo_LY7m9Ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Just in case training is interrupted, use pretrained model to plot loss \n",
        "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtGYULxwKHMV",
        "colab_type": "code",
        "outputId": "1f3d671c-7c9d-4c75-e195-f85062b3b46f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plotLoss(gen_loss_result,disc_loss_result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deZzN9f7A8dd7hrFXspStUFHWwWhD\npE1SorT8JFqUWxLdxL11Va6rW0lKRV1Fi1ZZSgshWkjGlqVNIhNFlGxjmXn//nifYcgwOOd8z5zz\nfj4e5zHnfL/nfL/vc+ac7/v7/ayiqjjnnEtcSUEH4JxzLlieCJxzLsF5InDOuQTnicA55xKcJwLn\nnEtwngiccy7BeSJwcUlE/isiI4KO40BE5EEReSroOJzzROAiTkQ257pli8i2XI87Bh1fUFT1flXt\nfrivF5EzReQDEfkjdFsiIv1F5Oh9ntdKRFRE7txn+amh5WP3WT5GRPoeblyu4PFE4CJOVUvm3ICf\ngEtzLRsddHwFkYi0AKYAU4FTVPUY4BIgGai1z9M7AxuA6/ezqWyghYg0ily0LtZ5InCBEpGSIpIp\nIkeFHv9bRLaLSLHQ40dF5L+h+8eKyKsisk5EfhSRe0RE8rmfK0RkaejMeYqInJJr3b9EZI2I/Cki\nX4tIs9DyJiIyP7T8FxF5KNdrmonI7ND25olIk1zruorIChHZJCLLRaRDHjHtLr4KnZ3vEpEbRCQj\n9B57H+AtDQKGqeogVV0HoKorVPVeVZ2Vax9HA5cD3YBUEamzz3YUeAwYkEeMx4vIh6H3uV5Eph0g\nJldAeSJwgVLVzcBXQLPQouZABnBmrsczQveHA4WBasAFwN+A/zvYPkSkLjAKuA0oH9reBBEpJCL1\ngRuAVOBo7Kw6I/TSp4CBqnoUcAowPrS9qqH79wLHAvcB40WktIiUBh4FzlPVUkBTYHE+P45kIA04\nGWgN/EdEqu/n/ZQGGgFv52ObVwFrgTHANOzqYF9PAI1EpOl+1vUBvgXKAhWAB/KxT1fAeCJwsWAG\n0FxEimAH3GGhx6WAesDnoXVXAH1UdbOqLgOGAJ3ysf1rgHGqOl1VdwADgXLYQXcXUAwrTklW1eWq\n+mPodTuBGiJSRlU3qers0PLOwFhVnaKq2ar6PrAUuDDXPuuISFFVXa2qXx/CZ3G/qmaq6hzgm9D7\n31eZ0N9fchaIyJOhs/YtInJ3rud2Bl5TG1TsVaCjiCTn3lgoGf+X/V8V7AQqAieo6g5V/eQQ3osr\nIDwRuFgwA2gBnAGkY2euzYEmwCJV/RM4Hvu+/pTrdSuBSvnYfsXQcwFQ1SzgZ6CSqi4B+gL/AdaK\nyGgROS701M7Ygfi7UDHQRaHlJwLX5aqk/QNLKhVV9XegI9AD+EVE3hGRk/P5OWSp6m+5Hm8FSu7n\neetDfyvkek89QvUEHwCFAEL7bQLk1MO8DZTGrqb29Qxwsojsu+4/wGrgYxFZJiJ35fO9uALEE4GL\nBZ8B9bFimRnAAuBU7Aw7p1joF6xi84RcrzsBO6AfzGrs4A1A6Iy4Us5rVfVFVT0bqA4UJXRmrKpf\nq+rVWHHSk8BYEUkBVgEjVPWYXLcSqvp46HXvqep5WAL6CbvCCZtQspkPtD/IU3Mqhz8SkV+A77Ak\n8ZfiIVXNxN73gH2Wb1TVO1X1ROyK7L7c9SEuPngicIFT1T+AJViZ/wxVzcauDG4mlAhUdTswDhgo\nIiVE5CTgTuCVfOziDaCdiJwjIoWxK4D1QLqI1BKRnGKpbaFbNoCIXB8qFsoCNmIVqwq8CHQQkfNE\nJFlEioXuHy8ilUTkEhEpDmwHNudsL8x6A7eJyN9FpFwo3hMIJcpQJXon4J9Y/UfO7f+AtjmV8/t4\nHisyOzdngYhcJiLVQ9vbCGRF6P24AHkicLFiBiDAvFyPS2BXCzluDf1diRUfjWBPsUeeVPUr4Cbg\nWWAdcB7QVlVz6gceA34D1mBFMf8KvbQN8K2IbAIeAq5S1Z2quhw7O34w9LqVWFJKwip8+2JXMOuB\nxsBh9xU4wHuail0xXQgsCxVPvYcVDT2LFa2Vx1oW/ZJzwyqNV2OVyPtucydWGXxsrsWnAR8Dm4BP\ngEG5WyW5+CA+MY1zziU2vyJwzrkE54nAOecSnCcC55xLcJ4InHMuwRUKOoBDVbZsWa1atWrQYTjn\nXIEyd+7c31S13P7WFbhEULVqVdLT04MOwznnChQRWZnXOi8acs65BOeJwDnnEpwnAuecS3CeCJxz\nLsF5InDOuQTnicA55xKcJwLnnEtwnghcwsvKguHDYe7coCNxLhieCFzCS0qC8eMhLQ3atIE5c4KO\nyLno8kTgEtbHH8N334EIvPYaDBgAs2bB6adD69awYEHQEToXHZ4IXMJRhYcfhvPPh3vvtWWlS9v9\nFSvgoYfgyy9h9Wpbl5UVWKjORYUnApdQ/vgD2rWDvn3hyivhhRf2Xl+qlK1buRIuvtiW3XefJY1P\nP41+vM5FgycClzB+/NHqAd57D4YMgddftwP//pQoYUVGAFWqwKJFcM450LIlzJgRvZidiwZPBC5h\nVKgAp50G06fDnXfuOdAfzG23WRIZPBi+/hpatIB+/SIZqXPR5YnAxbXMTDtob9wIRYvCu+9CkyaH\nvp3ixaFXL1i+HJ54Aq64wpYvWwYffWT1Ds4VVJ4IXNxauRKaNYN//xsmTgzPNosVgx49oH59ezxk\nCFx4oSWXSZM8IbiCyROBi0sffggNG1rz0HHjoGPHyOznscdg2DDIyIBWreCssywhOFeQeCJwcefF\nF60fQOXKkJ4Ol18euX0VKQLdulkR0bPPwi+/wPvv71nvVwiuIPBE4OLO+edD9+7WOeyUU6Kzz5QU\nuOUW+P57K4oCq5ROS4MJEzwhuNjmicDFhfR06NrVOn9VqgRPPmkVvNFWuDAcdZTdz8y0SurLL4cG\nDWDsWMjOjn5Mzh2MJwJXoKlakUyTJjB58p7ewLGgVSv45hsrqtq61VoaXXRR0FE591eeCFyBtXUr\ndOliZfTnngvz5lnnr1hSqBBcfz0sXQqvvALXXWfLd+2ySmwfvsLFAk8ErsDq0AFefhnuv996C5cp\nE3REeStUyFoude5sjydMgPbtoW5dePVVTwguWJ4IXIGTU/F6332WAB54AJKTAw3pkLVrB2++aXF3\n7Ai1allS84TgghCxRCAiL4jIWhFZnMf6tiLylYgsEJF0EWkaqVhcfNi1ywaE69vXHp911p6B4Qqa\npCS7olm4EMaMsV7PDz20Z/0HH1gfCG9t5KIhklcEo4BWB1g/FaivqqnAjcCICMbiCrhff7UevA8/\nbCOIxssBMinJKpHnz4cpU+wKYedOW1azphV3XXwxPPigz6DmIqdQpDasqp+ISNUDrN+c62EJIE5+\n2i7cZs60s+cNG2DkSKsgjjdJSVCxot1PTobZs/fcvvjCEkFSEjRqZImwRw844ww480yoV8+arTp3\nuEQjeGoVSgQTVbVOHuvbAQ8B5YFLVHVWHs+7BbgF4IQTTmi0cuXKiMTrYs/vv8OJJ0L58vD223vG\n+Ek0mzZZ0Vjp0jZzWqtWdpUEVqzUsCEMGmTFZVlZljTyO7qqSwwiMldV0/a3LtDKYlUdp6qnApcD\n/z7A855T1TRVTStXrlz0AnSB2b7d/pYubQkgPT1xkwDYvAmlS9v91FRYs8ZmU3vjDRsmG2wOBbBp\nNytWtI5sDz1kU3Ju2hRI2K6AiFjR0KEIFSNVF5Gyqvpb0PG4YH39tZWR//Of1u7+gguCjij2iNiV\n0oknwlVX7b3uhBPsM5s925qpgl0hrF4Nxx0Hi0PNN047reC1tnKREVgiEJGTgR9UVUWkIVAEWB9U\nPC42vPkm3HijDQ+RU2buDs0559gNrF7lyy9thrXjjrNlAwbYlUTJktC4sdU1NGkCbdoEF7MLVsTq\nCETkNaAFUBb4FbgfKAygqsNFpA9wPbAT2Ab0VtXPDrbdtLQ0TU9Pj0jMLjg7d0Lv3jbpy9lnW0Ko\nVCnoqOLT8uXw+ed7KqIXLoQ6dazlEtigeeXLw803+xVDPDlQHUFEK4sjwRNBfPrwQ2smeeed8Oij\n3gommrZtszqH6tWtWW69elZ8dOml1uu5ZMmgI3ThELOVxYls69agI4gN69bZ31atbKygIUM8CURb\nsWKWBMDqHhYtgqFDrdd206awalWw8bnI80QQRTkjYz74oBV//PlnsPEEbcYMOPlkSwBgQzW72NC9\nuyWCH3+Eli2t6M7FL08EUbJihQ0w9tBD1tZ7yRIbdGzHjqAjC8aiRdC2rdUDnHhi0NG4/WnVyjrz\nDR3qV2nxzhNBFGRmWnPIrCxr6nfhhTBiBEydCjfdFD/DJeTXypV2kClZ0uoGYnnU0ERXu7b9rwCG\nD4eBAxPv+5oIYqIfQbzr3t2KP955B046yZZ17mwTnt93n82tm3vAsXj2++82OcuWLfDZZ9bm3cU+\nVWth9OKL8O238NxzNl+ziw+eCCJsxAh4/nk74F966d7r/vlPWLvWWmkkilKlrLNThw7WZNEVDCI2\nztNJJ0G/ftYEddw4KFs26MhcOHjz0QgbO9a6/L/++sHbZP/xBxxzTHTiiradO+1qoHz5oCNxR+qN\nN2zgv0qVrA9CztAWLrZ589EA5OTX9u3hrbcOngQ+/BCqVrXKuXijCrfeCqef7i2l4sHVV8P06XDH\nHZ4E4oUnggjIyrJioOHD8/+aRo2gXDl73bffRi62INx7757ho486KuhoXDiccYZ1/gNrBnwo33UX\nezwRREDOHLqH0j2/XDm7KihUyFpp/PJL5OKLpqFDrSL8llvsc3Hx5/nn4W9/g549farNgsoTQZi9\n+y785z82cNrNNx/aa086CSZOtArk1q2tZU1BNnGinTVefjk884yPjx+vRo6EXr1snKjLLvPiv4LI\nE0EYLVsGnTrZJCFPPXV4B77Gja1O4aKLrOt/QdakiSWCV1/1wcviWXIyDB4Mw4bBpEn2f1/v4wgX\nKN58NIymTbOinbffPrKDeOvWdgObhap8+YJ1Nv3991Clik2k8vjjQUfjoqVbNxsy5M0390yi4woG\nvyIIo1tuge++s9Y/4fDzzzYrV79+4dleNCxfDs2aQdeuQUfignD++dbZLCnJxikaMyboiFx+eCII\ng1GjbDpAgGOPDd92K1a0VkQDBsCzz4Zvu5Gydq0Vae3caZ3lXGIbONA6Dvbv78NSxDovGjpCs2bZ\nlcDFF8O554Z32yJW7rp6tc1Lm5MYYtHmzXDJJXYVM3WqTYPoEttTT9lJwf33W5Po55+HokWDjsrt\nj18RHIG1a+2Mp0oVuyqIhEKFrCdnw4bWkSdnyOZYc/PNNsPVm2/a6KrOFSliLYoeesgaDLRsCb/5\njOQxyRPBYdq1C665xlpHvP12ZCvHSpa0fgnXXw81akRuP0eiXz94+WWf99btTQT69rW6gqQkvyKI\nVT7W0GF66SUbQfTFF+0AHU2bN9v0guXKRXe/+zNlCpx3XsFq1eSCoWrfk82bIT0dWrQIOqKDy8qy\nq5lSpQp+k24faygCOnWynsDRTgKqe5qXbt4c3X3v67HHbCRRbxni8iPnZOGBB+zkYejQQMPZr9Wr\nYfRoGzUY7Crm3nuhXTtrxt2xow0nv317sHGGnaoWqFujRo00SN99p7p8eaAh6LvvqiYlqbZurbpz\nZzAxvPyyKqh26KC6a1cwMbiCadMm1csus+/P7bcH9x3O8eGHqrfeqlqjhsUEqrVq7Vm/apXqlCmq\nXbuqHnusrf/3v23djh2q27cHE/ehAtI1j+Nq4Af2Q70FmQg2bVI97TTVmjWDP/g9+6z99266STU7\nO7r7/vBD1UKFVM89VzUzM7r7dvFh1y7Vu++27/BFF6n+8Ud09rtunepbb6neeeee3/Dtt6sedZRq\nmzaqgwapzp2b9+97xw7VDz5QXbnSHo8bp1q6tP0OJ00KPqkdiCeCMMjOVr36ajsTnzIlkBD+4r77\n7D/4yCPR2+eGDapHH61av370frwufv3vf6qVKqmuWBG5fSxerNqjh2rdunvO+EuUsKt7VfseH+4B\nPD1d9brrVEuVsu2WLWtXF5s2hS/+cDlQIvDK4nwaMsQG1nroIWsFEQtUrY32DTdAtWrR2+8HH0Bq\nKlSoEL19uvi1dSsULw7Z2fDNN1Cr1uFva+NG+OQT6+B57bU2dtdHH0HbtjYG0rnn2i0tDQoXDt97\nyMy038Wbb8LixfDVV1Yn8sYb1v+nSROrbwjSgSqLI3bmDrwArAUW57G+I/AVsAiYCdTPz3aDuCKY\nNUs1OVn18sujXwyTX1lZqkuWRG77a9aovv9+5Lbv3GOPqaakqL7yyqG9buNG1d69VdPS7IodVIsU\nUR0xwtbv3BndIsysLPubna1apYrFU6mSas+eqjNnBncMIYiiIeAcoOEBEsHZQOnQ/YuB2fnZbhCJ\nYPNmK8+M5aKQ++9XLV5c9csvw7/tjRtVU1OtHHX9+vBv3zlV+261aGFHpX/9a88BNbctW1QnT1b9\nxz9Un3zSlu3YYZW4zZqp9uun+vHHqtu2RTX0PP35p+ro0VY5npJi761PH1uXnR3dpBBIIrD9UjWv\nRLDP80oDP+dnm9FMBDt2WBIoCNasUa1aVbVcOdVly8K33cxM1ZYtrXL4gw/Ct13n9mf7dtUbb7Qj\n01VXqW7dassHD1Zt2lS1cGFbV6iQapcue163Y0cw8R6KP/5QffFF1fnz7fGsWarVq6v27WvLIp0U\nDpQIYqUfwU3AB3mtFJFbRCRdRNLXrVsXtaDuucem5Au6vX5+HH+89WvIzrYZzsLxMWVnWz+JadPg\nhRdsu85FUkqKteF/5BGYMAEWLbLlc+ZY2/1evaws/vffbfiKHOEs74+Uo4+231Nqqj0WsWG7H30U\nGjSAU0+Ff/3L6jmiLq8MEY4b+bgiAM4FvgbK5Geb0boieO01O/Po0SMquwubmTNVixZVPeecIz/D\neOstjXqrJOdyZGTsub+/YqJ4sXatNQdv2dKKuHLqM6ZPV/3mm/Dth6BaDYlIVWCiqtbJY309YBxw\nsap+l59tRqPV0JIldiWQmmpnwykpEd1d2L37rnWJP9Iu/Ko2hMT55/sQEs5Fw5YtUKKE3a9dG5Yu\ntTlJrrrKBp086aTD33ZMDjEhIicAY4FO+U0C0fDnn9C+vQ309uabBS8JgA1VnZMEZs8+9LHgX3/d\nvoAiNoSEJwHnoiMnCQBMnmzN1osXt2EuLrggcvM6RCwRiMhrwCygpohkiMhNItJNRLqFntIPKAM8\nIyILRCT4keSATZtscpk337T2vwXZJ5/AmWfCf/6T/9e89x5cdx08+GDk4nLOHVylSjbn98yZsHKl\nDXAZqZMy71C2H6rxcRasapVTr7xiFWtduhz4+V98YWPG16plHXJKlYpKmM65KIjJoqFYM20aXHGF\n1djHQxIAex/PP29l/F27wqRJeT/3m29shrGKFeH99z0JOJdIPBEAGRk2yczXXwffDTzcUlJs4pza\nteHKK22Y3f0ZONBmQ5s0yYbbdc4ljoSfs3j7djtAbtsGY8fG55nwUUfZWf6kSXnXezz3HPz005G1\nSnDOFUxxdv576O66y1rWjBplHTriVcWKNjgd2LzH69fbQFm9esGGDTaFYKxOg+mci6yEviJYt86K\nTXr3tvqBRLBpE1x4oR30jz8exo2zCuJLLw06MudcUBI6EZQrBwsWQNmyQUcSPaVKwbPPQocO1qpo\nyBBPAs4luoRMBH/8Ya1peva0s+JEc8UV8NprViT0t78FHY1zLmgJlwiys23i+UmTrKdevXpBRxSM\nq68OOgLnXKxIuEQwcCBMnAhPPZW4ScA553JLqFZDkydDv342hMJttwUdjXPOxYaESQSZmTbEQt26\nVlkaL72HnXPuSCVM0VDRotZhrGxZG83POeecSZhEADYSp3POub0lTNGQc865/fNE4JxzCc4TgXPO\nJThPBM45l+A8ETjnXILzROCccwnOE4FzziU4TwTOOZfgPBE451yC80TgnHMJzhOBc84luIglAhF5\nQUTWisjiPNafKiKzRGS7iNwdqTicc84dWCSvCEYBrQ6wfgPQAxgUwRicc84dRMQSgap+gh3s81q/\nVlXnADsjFYNzzrmDKxB1BCJyi4iki0j6unXrgg7HOefiSoFIBKr6nKqmqWpauXLlgg7HOefiSoFI\nBM455yLHE4FzziW4iE1VKSKvAS2AsiKSAdwPFAZQ1eEicjyQDhwFZItIT6CWqv4ZqZicc879VcQS\ngapee5D1vwCVI7V/55xz+ZOvoiEROUlEioTutxCRHiJyTGRDc845Fw35rSN4G8gSkZOB54AqwKsR\ni8o551zU5DcRZKvqLqAdMFRVewMVIheWc865aMlvItgpItcCnYGJoWWFIxOSc865aMpvIrgBOAv4\nj6r+KCLVgJcjF5ZzzrloyVerIVVdig0Qh4iUBkqp6sORDMw551x05LfV0HQROUpEjgXmAf8TkcGR\nDc0551w05Ldo6OhQR6/2wEuqegZwfuTCcs45Fy35TQSFRKQCcBV7Koudc87Fgfwmgv7AJOAHVZ0j\nItWB7yMXlnPOuWjJb2XxW8BbuR4vB66IVFDOOeeiJ7+VxZVFZFxoDuK1IvK2iPg4Qc45FwfyWzQ0\nEngHqBi6vRta5pxzroDLbyIop6ojVXVX6DYK8KnCnHMuDuQ3EawXketEJDl0uw5YH8nAnHPORUd+\nE8GNWNPRX4A1wJVAlwjF5JxzLorylQhUdaWqXqaq5VS1vKpejrcacs65uHAkcxbfFbYonHPOBeZI\nEoGELQrnnHOBOZJEoGGLwjnnXGAO2LNYRDax/wO+AMUiEpFzzrmoOmAiUNVS0QrEOedcMI6kaMg5\n51wc8ETgnHMJLmKJQEReCA1QtziP9SIiT4rIMhH5SkQaRioW55xzeYvkFcEooNUB1l8MnBK63QIM\ni2Aszjnn8hCxRKCqnwAbDvCUtti0l6qqXwDHhGZBc845F0VB1hFUAlblepwRWvYXInKLiKSLSPq6\ndeuiEpxzziWKAlFZrKrPqWqaqqaVK+ejXzvnXDgFmQh+Bqrkelw5tMw551wUBZkI3gGuD7UeOhPY\nqKprAozHOecSUr4mrz8cIvIa0AIoKyIZwP1AYQBVHQ68D7QGlgFbgRsiFYtzzrm8RSwRqOq1B1mv\nwO2R2r9zzrn8KRCVxc455yLHE4FzziU4TwTOOZfgPBE452KX+vxX0eCJwDkXe7ZtgyuugJo1Yfny\noKOJe54InHOxZeNGuOgiGDcOfv0VmjWDb74JOqq45onAORc7fv0VWrSAWbPg1Vfhs88gKwvOOQe+\n+iro6OKWJwLnXGxYsQKaNoVvv4V334VrroG6deGTT6BIEUsQc+YEHWVc8kTgnAve0qWWBH77DaZM\ngVa5pjKpUQM+/RSOOQbOO8+uElxYeSJwzgVr9myrB8jKghkz4Oyz//qcqlUtGVSsCBdeaMnChY0n\nAudccD76yM7yjzkGPv8c6tXL+7mVKlmiOPlkaNMGJk6MXpxxzhOBcy4YY8bAJZdA9epW3FO9+sFf\nc9xxMH26JYx27eCttyIeZiLwROCci77//Q+uugoaN7az/AqHMEvtscda0dCZZ1qF8ksvRS7OBOGJ\nIBGpQv/+cPvtsHNn0NG4RPPww3DLLdZXYPJkKF360Ldx1FHw4YfQsiV07gzDh4c/zgQSsWGoXQwb\nNAjuv9/ur1kDr78OKSnBxuTinyr06QOPPgrXXgujRh3Z965ECWtm2qED/O1vsHUr3HVX2MJNJH5F\nkGhefRXuuccuyx9/3HpvXnklbN8edGQunu3aBTffbEngttvglVfCc/JRtCi8/bYlg7//HQYM8PGJ\nDoNfESSSqVOhSxdo3tzKVYsUgcKFoXt3aN/eflBFiwYdpYs3mZnwf/9nJx39+sEDD4BI+LafkmIn\nOMWKwb/+BVu2wMCB4d1HnPNEkCgWLrSDfY0aMH68JQGweoJChaBbN2uFMXas/aCcC4dNm+Dyy2Ha\nNHjiCejRIzL7KVQIRo6E4sXhv/+1YqLHH4ekOCr0UIXsbEhODvum4+hTcnn66Sdo3RpKlYIPPrA2\n27ndeiuMGAGTJkHbtvYjcu5IrVtnlbkzZtgVaKSSQI6kJHjmGasnePJJ+15nZUV2n9Eyfbq1knru\nuYhs3q8I4t2GDdZdf8sW65lZpcr+n3fTTXamceONcOml8M47Vhnn3OFYtcp6AK9YYUVCl14anf2K\nWGOIEiXg3/+2k5pRo6wItCBauBD+8Q87gatcGcqWjchuPBHEs8xMO8P/4Qc7269b98DP79LFLrE7\nd7aOPhMnQsmSUQnVxZFvv4ULLrDhpCdNspFDo0nEmkcXL24H0W3b4LXX9hSHFgQrVlh9x+jRdgX/\n6KNWjBupYltVLVC3Ro0aqcuHXbtUr7hCFVRff/3QXvvqq6pJSapNm6r++Wdk4nPxKT1dtWxZ1fLl\nVefNCzoa1SeftN9Aq1aqW7cGHc3BrVun2rOnakqKatGiqn37qv7+e1g2DaRrHsfVwA/sh3rzRJAP\n2dmqd9xh/97Bgw9vG2+8oZqcrHrWWaobN4Y3PhefPv5YtVQp1RNPVP3uu6Cj2WPECFUR1RYtYvfE\nZvNm1X//2z6/pCTVm29WXbUqrLvwRJBoHnnE/rW9eh3ZdsaMUS1USPWMM8J2VuLi1LhxqkWKqNaq\npZqREXQ0fzV6tJ3YnHlmbH2Xd+xQHTZM9fjj7Td7+eWqS5dGZFeBJQKgFfAtsAzou5/1JwJTga+A\n6UDlg23TE8FBjB5t/9arrlLNyjry7Y0fr1q4sGpamuqGDUe+PRd/Ro60s9jTT1f97bego8nb2LH2\nXW7QwIpggpSdrfrmm6qnnGK/16ZNVT//PKK7DCQRAMnAD0B1IAVYCNTa5zlvAZ1D91sCLx9su54I\nDmDKFPuiN2+umpkZvu2++66VWTZoENs/dBd9jz1mh5Hzz1fdtCnoaA7ugw+s7L12bdXVq4OJYdo0\n1caN7XOrXdt+X9nZEd9tUIngLGBSrsf/AP6xz3OWAFVC9wX482Db9USQhwULVI86yr5Ykbj0fe89\nu/SvXz/4sykXvOxs1X/+0xz0IZQAABuwSURBVA4hV14Z3hOPSPv4Y9USJexs/KeforffBQus0hpU\nq1SxK6ldu6K2+wMlgkh2KKsErMr1OCO0LLeFQPvQ/XZAKREps++GROQWEUkXkfR169ZFJNgCbeVK\nuPjiPSMy7tthLBxat4YJE6xpYMuWsHZt+PfhCoasLBvkbeBA6NrVBi0sSE0zW7SwCXHWrrWZ0X74\nIbL7W7ECOnWCBg1sNrZBg+C776y5dgR6CR+OoHsW3w00F5H5QHPgZ+AvXQFV9TlVTVPVtHLlykU7\nxti2YYMlga1b93Q6iZSLLrLRHpctg3PPhV9/jdy+XGzascPGDXr2Wejb1/7GyMHskJx1lg17sXmz\nJYOvvw7/Ptatg549oWZNm4SnTx9YvtwGx4uxMb0imQh+BnJ3Y60cWrabqq5W1faq2gC4N7TsjwjG\nFF9ydxgbPx7q1In8Ps8/H957z85yWrSwYaxdYti82XoIv/mmdXB66KGCPbBbw4Y2dIOqDcS4cGF4\ntrtli42CetJJMHQoXH+9nTw99FBkrtbDIa8yoyO9Yb2WlwPV2FNZXHuf55QFkkL3/wP0P9h2vY4g\n5Eg6jIXDjBlWzlqjRmw2F3ThtX69Nb1MSlJ9/vmgowmv776zMvtjjlGdPfvwt7NvU9B27SLWFPRw\nEGDz0dbAd1jroXtDy/oDl4XuXwl8H3rOCKDIwbbpiUDD02EsHD77TLVkSdWTTopupZuLrp9/Vq1T\nx1qOjR0bdDSRsWKFavXq1qHrk08O7bX7NgVt1kx15szIxHkEAksEkbh5ItDwdRgLh5kzrbVStWr2\nY3Lx5fvvVatWtYQ/dWrQ0URWRobqqaeqFiumOnly/l6TuylonTqqEydGpSno4ThQIgi6stgdqtwz\njA0aFHQ0Vun20UdWad28Ofz4Y9ARuXBZuBCaNrU5BaZNs9Zi8axSJRsyu0YNaNPGRuDNy4IFNqpv\ny5bwyy82wumCBTZYYwGsN/FEUJDsO8NYrEy6cfrpFtuff1oFcqSb47nI+/RT+54VLmz3GzcOOqLo\nKF/ekl5qKlxxBbzxxt7rf/wRrrvOmoLOmbOnKWjnzgWz9VRIjBxJ3EEtXGgziO07w1isaNTIksHm\nzZYMvv8+6IjcocrOhpkzoXdvm0vguOPg88/htNOCjiy6jj3WrnLPOsuayr744t5NQceOteGtf/gh\nJpuCHg6xoqOCIy0tTdPT04MOI7pWrrQvZXIyzJoV2b4CR2rhQmtimpJiZ1Y1awYdkTuQ7dvt/zR+\nvHUY/PVXm5Pi4ott1rry5YOOMDhbt9o0mx99ZBPdbNtmEzjdf78VIxUwIjJXVdP2t84npol1uTuM\nffZZbCcBgPr14eOPrey0RQs7yCTaGWWs27jROh+OG2d/N22yCYhat7YD38UXx25792gqXtzqCW65\nxfrs9O8Pp54adFQR4Ykglu07w1g0OoyFQ5061lEndzKoXTvoqBLbmjV2xj9+vP0/du60s/1rrrGD\nf8uWcVHEEXZFi1p9XJzzRBCrsrKsUuqzz2wslxYtgo7o0NSqtXcymDoV6tULOqrE8t13dtY/fjx8\n8YUtO+kkuPNOO/ifeWaBruB04eOJIBapQq9e8PbbMHgwXH110BEdnlNPteZ4555rCWHKFGuN4SIj\nOxvS0+3AP378nvFzGjWyidzbtbMEXQCbN7rI8kQQiwYNsjFKevWyW0F2yil/TQYNGwYdVfzYudOu\nvHIqe3/+2c7ymze3EULbtoUTTgg6ShfjPBHEmljrMBYOJ520Jxmcdx5Mnpw47dIjYfNmG258/HiY\nONEqf4sVsw5O7dpZp6Zjjw06SleAeCKIJTkdxlq0iK0OY+FQrdqeOoPzz7fK7zPPDDqqgmPtWhsC\nfPx4a864fTuUKQPt21t5//nnWysX5w6DJ4JYkdNhrGZNq+CLtQ5j4VC16p5kcOGFdlZ79tlBRxW7\nli+3A/+4cdaxSxVOPNGKfC6/HJo0sTb/zh0h/xbFgpwZxo4+2tp1x3Mb7hNO2JMMLroI3n/fJgZx\nZuFC67k6bhwsWmTL6teHfv3s4F+/vlf2urDzRBC0gtZhLBwqV96TDFq1soPehRcGHVWwtm61IQz+\n9z8rEmza1FqMtW0L1asf8uZ27txJRkYGmZmZEQjWxbKiRYtSuXJlChcunO/XeCIIUkHtMBYOFSta\nMrjgAksG99xjPTdTUoKOLPoWL7YmwkuX2udw991whFOyZmRkUKpUKapWrYr4FUTCUFXWr19PRkYG\n1apVy/fr4qg2soDJ3WHspZcKXoexcDj+eOvo1LUrPPyw1Rd8+23QUUWPKgwfbi2o1q+31lQPP3zE\nSQAgMzOTMmXKeBJIMCJCmTJlDvlK0BNBEOKlw1g4lChhE6CPHWtD/DZsaMUjBWwwxEP2++/QoYNV\n/J5zjtUNXHBBWHfhSSAxHc7/3RNBEOKpw1i4tGtnlaNnnWWDfF1xhZ0lx6OZM62H9YQJ8Mgj1kDg\nuOOCjsolME8E0ZSVZT/8eOswFi4VK1rxyKOPWkepevWsb0W8yMqCgQPtCiA52ZqE9u4dX/1Fcvn1\n11/5v//7P6pXr06jRo0466yzGDduXGDxTJ8+nZkzZx7xNtq0aROmiGJHfH4DY9HixVYG3qePNQOM\ntw5j4ZKUZJWlX3wBpUpZcck998COHUFHdmRWr7aWUffeC1deCfPn28xucUpVufzyyznnnHNYvnw5\nc+fO5fXXXycjIyOi+921a1ee6w4nERxoe/HEWw1F2o4ddhY4cKD1E3jtNasT8PLbA2vYEObNg7vu\nsiuEKVNs+I2COB78Bx/A9dfDli022cuNN0b3/9+zp82nG06pqTBkSJ6rp02bRkpKCt26ddu97MQT\nT+SOO+4AICsri759+zJ9+nS2b9/O7bffzq233sr06dN54IEHKFu2LIsXL6ZRo0a88soriAhz587l\nrrvuYvPmzZQtW5ZRo0ZRoUIFWrRoQWpqKp999hnXXnstNWrUYMCAAezYsYMyZcowevRotm3bxvDh\nw0lOTuaVV15h6NChVKlShRtvvJHffvuNcuXKMXLkSE444QS6dOlC0aJFmT9/Pk2aNGHw4MEH/Tim\nTp3K3Xffza5du2jcuDHDhg2jSJEi9O3bl3feeYdChQpx4YUXMmjQIN566y0efPBBkpOTOfroo/nk\nk0+O/P9xhDwRRNKXX9qPfskSm/LuiSegbNmgoyo4ihe3VjUXX2wzQzVsaAefrl0LRiLdscOmNBw8\n2Iq5Xn89YSbpWbJkCQ0PMLjg888/z9FHH82cOXPYvn07TZo04cJQX5L58+ezZMkSKlasSJMmTfj8\n888544wzuOOOO5gwYQLlypXjjTfe4N577+WFF14AYMeOHeTMXPj777/zxRdfICKMGDGCRx55hMce\ne4xu3bpRsmRJ7r77bgAuvfRSOnfuTOfOnXnhhRfo0aMH48ePB6z57cyZM0nOxzDdmZmZdOnShalT\np1KjRg2uv/56hg0bRqdOnRg3bhzffPMNIsIff/wBQP/+/Zk0aRKVKlXavSxonggiYcsW6wk6ZAhU\nqGBjxMRhuWLUtG1rTSy7dIFbb7Uz7P/9L7aT6rJlcO21Niz0bbdZfVCxYsHEcoAz92i5/fbb+eyz\nz0hJSWHOnDlMnjyZr776ijFjxgCwceNGvv/+e1JSUjj99NOpHOpYmZqayooVKzjmmGNYvHgxF4Ra\nVmVlZVGhQoXd2786V8u7jIwMrr76atasWcOOHTvybE8/a9Ysxo4dC0CnTp245557dq/r0KFDvpIA\nwLfffku1atWoUaMGAJ07d+bpp5+me/fuFC1alJtuuok2bdrsrlto0qQJXbp04aqrrqJ9+/b52kek\neSF1uE2bZmd/gwdb65elSz0JhEPFijY20WOP2bAU9epZcVEsevVVaNDAksHYsfD008ElgYDUrl2b\nefPm7X789NNPM3XqVNatWwdYHcLQoUNZsGABCxYs4Mcff9x9RVAk1zhbycnJ7Nq1C1Wldu3au5+/\naNEiJk+evPt5JUqU2H3/jjvuoHv37ixatIhnn332sHpX597e4SpUqBBffvklV155JRMnTqRVq1YA\nDB8+nAEDBrBq1SoaNWrE+hhoHRfRRCAirUTkWxFZJiJ997P+BBH5WETmi8hXItI6kvFE1B9/WJHF\needZhef06TBsGBx1VNCRxY+kJKszmD3b6lsuuMAqlrdvDzoys3kz3HADdOxoYwLlDCSYgFq2bElm\nZibDhg3bvWzr1q2771900UUMGzaMnTt3AvDdd9+xZcuWPLdXs2ZN1q1bx6xZswAbQmPJkiX7fe7G\njRupFJpc/sUXX9y9vFSpUmzatGn347PPPpvXX38dgNGjR9PsMMe8qlmzJitWrGDZsmUAvPzyyzRv\n3pzNmzezceNGWrduzeOPP87ChQsB+OGHHzjjjDPo378/5cqVY9WqVYe133CKWCIQkWTgaeBioBZw\nrYjU2udp9wFvqmoD4BrgmUjFE1ETJtjMTy+8YC1cvvrKJgZxkZGaCnPnWmesxx6z4axzZuMKyoIF\nkJYGL74I//qXnQgk8IQwIsL48eOZMWMG1apV4/TTT6dz5848/PDDANx8883UqlWLhg0bUqdOHW69\n9dYDttBJSUlhzJgx9OnTh/r165OamppnC6AHHniADh060KhRI8rmKj689NJLGTduHKmpqXz66acM\nHTqUkSNHUq9ePV5++WWeeOKJfL23qVOnUrly5d23+fPnM3LkSDp06EDdunVJSkqiW7dubNq0iTZt\n2lCvXj2aNm26u9K5d+/e1K1blzp16nD22WdTv379/H6skaOqEbkBZwGTcj3+B/CPfZ7zLNAn1/Nn\nHmy7jRo10pjxyy+qV12lCqr16qnOmRN0RIlnwgTVsmVVixVTHTZMNTs7uvvPzlZ98knVlBTVihVV\np02L7v7zsHTp0qBDcAHa3/8fSNc8jquRLBqqBOS+5skILcvtAeA6EckA3gfuiGA84aMKL79sVwHj\nx8OAAVYpmJYWdGSJ57LL7AqsWbM94/SHyqEjbv1621+PHlZMtWCBzcLmXAETdGXxtcAoVa0MtAZe\nFpG/xCQit4hIuoikr4vWjzwvP/1kUwFef71NIrNggXUSOoQhX12YVahgLYkef9wqlOvVsx7KkfTJ\nJ1ZElbPfd98Ny2BxzgUhkongZ6BKrseVQ8tyuwl4E0BVZwFFgb+0CVTV51Q1TVXTygX1Y8vOhmee\ngdq17SDw5JPw6acJ0y485iUlWcepL7+0+XovusgqlsNdkZyVBQ8+aGf+RYvCrFm234LQr8G5PEQy\nEcwBThGRaiKSglUGv7PPc34CzgMQkdOwRBDwKf9+fPutVf7efrsNirZ4Mdxxh40X42JL/fp72u4/\n/jiccYY14Q2HjAybTOeBB6xl0Lx50KhReLbtXIAilghUdRfQHZgEfI21DloiIv1F5LLQ0/4OdBWR\nhcBrQJdQpUZs2LkT/vtfO7gsWQKjRtkEMlWrBh2ZO5Bixazt/rvvws8/28H6mWeObGjrd96x78Hc\nudYy6KWXbCwk5+JARHsWq+r7WCVw7mX9ct1fCjSJZAyHbf58G9Zg/nwbEvmpp2wiFVdwtGljQ1t3\n6WJXcx9+CM8/f2hl+ZmZ1iR46FDrJPb66xDqQepcvAi6sjj2ZGbCP/9pQxqsXg1jxtjNk0DBdPzx\n1hN5yBC7mqtb1/7mx7ffWlHg0KFw551WH+BJIN+Sk5NJTU2ldu3a1K9fn8cee4zs7GwA0tPT6dGj\nxxHvY/jw4bz00kuH9Jqzzz77sPc3atQoVq9efdivB+vnMCjWhqDPq11prN4i2o/g009Va9a0fgE3\n3KC6YUPk9uWib+FC1dq17f/bs6fqtm37f152tuqoUaolSqiWKaP67rvRjTMMYqEfQYkSJXbf//XX\nX/W8887Tfv36hW37O3fuDNu28qt58+Y65xD7C+3atWuvx/fff78++uij4QzrL2KpH0HBsWkTdO9u\nbdG3b7emhy+8AKVLBx2ZC6d69WDOHPtfDxliFcn7DlOwaRN06mTFSWlpNkxEHIwV1aLFX2/PhPrx\nb926//WjRtn6337767pDVb58eZ577jmeeuopVHWvCV5mzJhBamoqqampNGjQYPcwEA8//DB169al\nfv369O3bN/Q+WtCzZ0/S0tJ44okn9jq7btGiBb169SItLY3TTjuNOXPm0L59e0455RTuu+++3bGU\nLFkSsPkJWrRowZVXXsmpp55Kx44dczq60r9/fxo3bkydOnW45ZZbUFXGjBlDeno6HTt2JDU1lW3b\ntjF16lQaNGhA3bp1ufHGG9keaqVWtWpV+vTpQ8OGDXnrrbfy9RkNHjyYOnXqUKdOHYaEBgrcsmUL\nl1xyCfXr16dOnTq88cYbAPTt25datWpRr1693aOpHgkfffTDD21Ey1Wr7PJ/wAAIfVFcHCpWzIp6\nWrWycYHS0my+g9tvt4rga66xuZMffND6h3jLsLCpXr06WVlZrF27dq/lgwYN4umnn6ZJkyZs3ryZ\nokWL8sEHHzBhwgRmz55N8eLF2bBhw+7n5x5y+oEHHthrWykpKaSnp/PEE0/Qtm1b5s6dy7HHHstJ\nJ51Er169KFOmzF7P39+Q102bNqV79+7062fVmZ06dWLixIlceeWVPPXUUwwaNIi0tLQ8h5/u2bMn\nAGXKlNlr4L0DmTt3LiNHjmT27NmoKmeccQbNmzdn+fLlVKxYkffeew+wcZTWr1+/3+Gtj0TiJoL1\n662d+UsvWV+Azz+38mCXGC65xCqSb7jBmgKPHm2J4LjjbJygwxyALFZNn573uuLFD7y+bNkDrz9S\nTZo04a677qJjx460b9+eypUrM2XKFG644QaKFy8OwLHHHrv7+bmHnN7XZZdZg8S6detSu3bt3UNV\nV69enVWrVv0lEexvyOumTZvy8ccf88gjj7B161Y2bNhA7dq1ufTSS/d6bV7DT+ckggPFua/PPvuM\ndu3a7R71tH379nz66ae0atWKv//97/Tp04c2bdrQrFkzdu3atd/hrY9E4hUNqcJbb9nwEK++agOE\nzZ/vSSARHXccvPeedQ6cPx9at7aioDhLArFi+fLlJCcnU758+b2W9+3blxEjRrBt2zaaNGnCN998\nc8DtHGiI6JwhrJOSkvYazjopKWm/g9rtb8jrzMxMbrvtNsaMGcOiRYvo2rVrYENZ16hRg3nz5lG3\nbl3uu+8++vfvn+fw1kcisRLB6tXQvr1NHF+lip0B9u8Pub4MLsGI2BXB77/buFG5zjxd+Kxbt45u\n3brRvXt3ZJ9e2D/88AN169alT58+NG7cmG+++YYLLriAkSNH7h66OnfRUKTlHPTLli3L5s2bd0+e\nA3sPZZ3X8NOHo1mzZowfP56tW7eyZcsWxo0bR7NmzVi9ejXFixfnuuuuo3fv3sybNy/P4a2PROIU\nDb3/vk0XuX07PPII9OoFhRLn7buDSLCJY6Jh27ZtpKamsnPnTgoVKkSnTp246667/vK8IUOG8PHH\nH5OUlETt2rW5+OKLKVKkCAsWLCAtLY2UlBRat27NwIEDoxL3McccQ9euXalTpw7HH388jRs33r2u\nS5cudOvWjWLFijFr1qzdw0/nzFWce47mAxkwYMDuCmGwWdW6dOnC6aefDtgw3Q0aNGDSpEn07t2b\npKQkChcuzLBhw9i0aRNt27YlMzMTVc3XnMoHIzm15AVFWlqa5lQUHZJly+zM78kn4ZRTwh+YczHk\n66+/5jQfByth7e//LyJzVXW/QyQnzinxySfbSJHOOef2klh1BM455/7CE4FzcaqgFfu68Dic/7sn\nAufiUNGiRVm/fr0ngwSjqqxfv56iRYse0usSp47AuQRSuXJlMjIyCHxGPxd1RYsW3d1JLr88ETgX\nhwoXLky1atWCDsMVEF405JxzCc4TgXPOJThPBM45l+AKXM9iEVkHrAw6jiNUFvgt6CBiiH8ee/PP\nYw//LPZ2JJ/Hiaq633laC1wiiAcikp5XV+9E5J/H3vzz2MM/i71F6vPwoiHnnEtwngiccy7BeSII\nxnNBBxBj/PPYm38ee/hnsbeIfB5eR+CccwnOrwiccy7BeSJwzrkE54kgikSkioh8LCJLRWSJiNwZ\ndExBE5FkEZkvIhODjiVoInKMiIwRkW9E5GsROSvomIIkIr1Cv5PFIvKaiBzakJoFnIi8ICJrRWRx\nrmXHishHIvJ96G/pcOzLE0F07QL+rqq1gDOB20WkVsAxBe1O4Ougg4gRTwAfquqpQH0S+HMRkUpA\nDyBNVesAycA1wUYVdaOAVvss6wtMVdVTgKmhx0fME0EUqeoaVZ0Xur8J+6FXCjaq4IhIZeASYETQ\nsQRNRI4GzgGeB1DVHar6R7BRBa4QUExECgHFgdUBxxNVqvoJsGGfxW2BF0P3XwQuD8e+PBEERESq\nAg2A2cFGEqghwD1AdtCBxIBqwDpgZKiobISIlAg6qKCo6s/AIOAnYA2wUVUnBxtVTDhOVdeE7v8C\nHBeOjXoiCICIlATeBnqq6p9BxxMEEWkDrFXVuUHHEiMKAQ2BYaraANhCmC77C6JQ2XdbLEFWBEqI\nyHXBRhVb1Nr+h6X9vyeCKBORwlgSGK2qY4OOJ0BNgMtEZAXwOtBSRF4JNqRAZQAZqppzhTgGSwyJ\n6nzgR1Vdp6o7gbHA2QHHFAt+FZEKAKG/a8OxUU8EUSQigpUBf62qg4OOJ0iq+g9VrayqVbFKwGmq\nmrBnfKr6C7BKRGqGFp0HLA0wpKD9BJwpIsVDv5vzSODK81zeATqH7ncGJoRjo54IoqsJ0Ak7+10Q\nurUOOigXM+4ARovIV0AqMDDgeAITujIaA8wDFmHHqoQabkJEXgNmATVFJENEbgL+C1wgIt9jV03/\nDcu+fIgJ55xLbH5F4JxzCc4TgXPOJThPBM45l+A8ETjnXILzROCccwnOE4Fz+xCRrFzNexeISNh6\n+IpI1dyjSToXCwoFHYBzMWibqqYGHYRz0eJXBM7lk4isEJFHRGSRiHwpIieHllcVkWki8pWITBWR\nE0LLjxORcSKyMHTLGSIhWUT+Fxprf7KIFAvsTTmHJwLn9qfYPkVDV+dat1FV6wJPYaOnAgwFXlTV\nesBo4MnQ8ieBGapaHxs3aElo+SnA06paG/gDuCLC78e5A/Kexc7tQ0Q2q2rJ/SxfAbRU1eWhwQN/\nUdUyIvIbUEFVd4aWr1HVsiKyDqisqttzbaMq8FFoYhFEpA9QWFUHRP6dObd/fkXg3KHRPO4fiu25\n7mfhdXUuYJ4InDs0V+f6Oyt0fyZ7plHsCHwauj8V+Bvsnpv56GgF6dyh8DMR5/6qmIgsyPX4Q1XN\naUJaOjQ66Hbg2tCyO7CZxXpjs4zdEFp+J/BcaNTILCwprMG5GON1BM7lU6iOIE1Vfws6FufCyYuG\nnHMuwfkVgXPOJTi/InDOuQTnicA55xKcJwLnnEtwngiccy7BeSJwzrkE9/+iVOnMgEHDwgAAAABJ\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHleXQWiEJzc",
        "colab_type": "text"
      },
      "source": [
        "Show three example generated photos using your model. Even though the images may not be perfect, it should be perceivable as a fashion photo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_5LMqzFG0OQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Restore the latest checkpoint\n",
        "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJhL_QGOsdAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate images\n",
        "num_examples_to_generate =3 \n",
        "noise_dim =100\n",
        "latent_points = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxB6LoQavsF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_and_save_3images(model, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(1, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9nzpXuass2t",
        "colab_type": "code",
        "outputId": "70e0183f-41b7-48f3-c4be-0492c680b1a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        " display.clear_output(wait=True)\n",
        " generate_and_save_3images(generator,\n",
        "                          latent_points)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALMAAAA+CAYAAABgOir1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxOA/1ioXJC2k/U8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 288x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgCDWjv-Ejdy",
        "colab_type": "text"
      },
      "source": [
        "# Question 2\n",
        "Design a method to conditionally generate photos of different labels. Pick two classes within the MNIST fasion dataset, and use them to conditionally generate photos of those two classes. You will likely need to include another outcome variable in the discriminator to account for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPxcAy6yHYIO",
        "colab_type": "text"
      },
      "source": [
        "Plot all of your loss functions and label them accordingly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1RLNXUeHgFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For this question, we need to design models in Keras to have multiple inputs\n",
        "#We choose to use Functional API\n",
        "#Reference: https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQNbYfkIKmFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvKWAVOYKpKJ",
        "colab_type": "code",
        "outputId": "f5127798-3e59-4904-b86e-fa67c7e465ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Concatenate\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C2oO-C3K4MY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the standalone generator model\n",
        "def define_generator(latent_dim, n_classes=10):\n",
        "\t# label input\n",
        "\tin_label = Input(shape=(1,))\n",
        "\t# embedding for categorical input\n",
        "\tli = Embedding(n_classes, 50)(in_label)\n",
        "\t# linear multiplication\n",
        "\tn_nodes = 7 * 7\n",
        "\tli = Dense(n_nodes)(li)\n",
        "\t# reshape to additional channel\n",
        "\tli = Reshape((7, 7, 1))(li)\n",
        "\t# image generator input\n",
        "\tin_lat = Input(shape=(latent_dim,))\n",
        "\t# foundation for 7x7 image\n",
        "\tn_nodes = 128 * 7 * 7\n",
        "\tgen = Dense(n_nodes)(in_lat)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\tgen = Reshape((7, 7, 128))(gen)\n",
        "\t# merge image gen and label input\n",
        "\tmerge = Concatenate()([gen, li])\n",
        "\t# upsample to 14x14\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\t# upsample to 28x28\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\t# output\n",
        "\tout_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
        "\t# define model\n",
        "\tmodel = Model([in_lat, in_label], out_layer)\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUkqzvRtK7UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the standalone discriminator model\n",
        "def define_discriminator(in_shape=(28,28,1), n_classes=10):\n",
        "\t# label input\n",
        "\tin_label = Input(shape=(1,))\n",
        "\t# embedding for categorical input\n",
        "\tli = Embedding(n_classes, 50)(in_label)\n",
        "\t# scale up to image dimensions with linear activation\n",
        "\tn_nodes = in_shape[0] * in_shape[1]\n",
        "\tli = Dense(n_nodes)(li)\n",
        "\t# reshape to additional channel\n",
        "\tli = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
        "\t# image input\n",
        "\tin_image = Input(shape=in_shape)\n",
        "\t# concat label as a channel\n",
        "\tmerge = Concatenate()([in_image, li])\n",
        "\t# downsample\n",
        "\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "\t# downsample\n",
        "\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "\t# flatten feature maps\n",
        "\tfe = Flatten()(fe)\n",
        "\t# dropout\n",
        "\tfe = Dropout(0.4)(fe)\n",
        "\t# output\n",
        "\tout_layer = Dense(1, activation='sigmoid')(fe)\n",
        "\t# define model\n",
        "\tmodel = Model([in_image, in_label], out_layer)\n",
        "  # compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\treturn model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rMOq-nLLBgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load fashion mnist images\n",
        "def load_real_samples():\n",
        "\t# load dataset\n",
        "\t(trainX, trainy), (_, _) = fashion_mnist.load_data()\n",
        "\t# expand to 3d, e.g. add channels\n",
        "\tX = expand_dims(trainX, axis=-1)\n",
        "\t# convert from ints to floats\n",
        "\tX = X.astype('float32')\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\treturn [X, trainy]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kExEW3NBLEB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# split into images and labels\n",
        "\timages, labels = dataset\n",
        "\t# choose random instances\n",
        "\tix = randint(0, images.shape[0], n_samples)\n",
        "\t# select images and labels\n",
        "\tX, labels = images[ix], labels[ix]\n",
        "\t# generate class labels\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn [X, labels], y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh88RkiELGcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tz_input = x_input.reshape(n_samples, latent_dim)\n",
        "\t# generate labels\n",
        "\tlabels = randint(0, n_classes, n_samples)\n",
        "\treturn [z_input, labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWyOhxvCLJjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tz_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\timages = generator.predict([z_input, labels_input])\n",
        "\t# create class labels\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn [images, labels_input], y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6Cxu36OLMTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model):\n",
        "\t# make weights in the discriminator not trainable\n",
        "  d_model.trainable = False\n",
        "\t# get noise and label inputs from generator model\n",
        "  gen_noise, gen_label = g_model.input\n",
        "  # get image output from the generator model\n",
        "  gen_output = g_model.output\n",
        "  # connect image output and label input from generator as inputs to discriminator\n",
        "  gan_output = d_model([gen_output, gen_label])\n",
        "  # define gan model as taking noise and label and outputting a classification\n",
        "  model = Model([gen_noise, gen_label], gan_output)\n",
        "  # compile model\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTbCtJ0zLO7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotLoss(gen_loss_result,disc_loss_result):\n",
        "  epoch_count = range(1, len(gen_loss_result) + 1)\n",
        "  plt.plot(epoch_count, gen_loss_result, 'r-')\n",
        "  plt.plot(epoch_count, disc_loss_result, 'b--')\n",
        "  plt.legend(['Generator Loss', 'Discriminator Loss'])\n",
        "  plt.title('Two losses in GANs')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-3OpKdwLRqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=256):\n",
        "  bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "  half_batch = int(n_batch / 2)\n",
        "  gen_loss_result =[]\n",
        "  disc_loss_result = []\n",
        "  # manually enumerate epochs\n",
        "  for i in range(n_epochs):\n",
        "    batch_step_gen_loss =[]\n",
        "    batch_step_disc_loss = []\n",
        "    # enumerate batches over the training set\n",
        "    for j in range(bat_per_epo):\n",
        "      # get randomly selected 'real' samples\n",
        "      [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
        "      # update discriminator model weights\n",
        "      d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
        "      # generate 'fake' examples\n",
        "      [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "      # update discriminator model weights\n",
        "      d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
        "\t\t\t# prepare points in latent space as input for the generator\n",
        "      [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
        "      # create inverted labels for the fake samples\n",
        "      y_gan = ones((n_batch, 1))\n",
        "      # update the generator via the discriminator's error\n",
        "      g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
        "      d_loss = d_loss1 + d_loss2\n",
        "      # summarize loss on this batch\n",
        "      print('>%d, %d/%d, d=%.3f, g=%.3f' %\n",
        "      (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
        "      batch_step_gen_loss.append(g_loss)\n",
        "      batch_step_disc_loss.append(d_loss)\n",
        "    epoch_mean_gen_loss = np.mean(batch_step_gen_loss)\n",
        "    epoch_mean_disc_loss = np.mean(batch_step_disc_loss)\n",
        "    gen_loss_result.append(epoch_mean_gen_loss)\n",
        "    disc_loss_result.append(epoch_mean_disc_loss)\n",
        "  plotLoss(gen_loss_result,disc_loss_result)\n",
        "  # save the generator model\n",
        "  model_save_name = 'cgan_generator.h5'\n",
        "  path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "  g_model.save(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LxuN6NcLWE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = load_real_samples()\n",
        "#[X_real, labels_real], y_real = generate_real_samples(dataset, 256)\n",
        "#print([X_real.shape, labels_real])\n",
        "#print(y_real.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJNd1f1-LavO",
        "colab_type": "code",
        "outputId": "1c39ec4a-51cb-46ae-d2db-cbef91ab991a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "discriminator_cgan = define_discriminator()\n",
        "#discriminator.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-WLcNn3Lehs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the generator\n",
        "generator_cgan = define_generator(latent_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mgMHi70Lhpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the gan\n",
        "gan_model = define_gan(generator_cgan, discriminator_cgan)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj36JImhQuO9",
        "colab_type": "code",
        "outputId": "a59850d7-eb0b-494f-84c0-f396188e164f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#train model\n",
        "train(generator_cgan, discriminator_cgan, gan_model, dataset, latent_dim)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">1, 1/234, d=1.398, g=0.692\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">1, 2/234, d=1.324, g=0.688\n",
            ">1, 3/234, d=1.268, g=0.682\n",
            ">1, 4/234, d=1.214, g=0.672\n",
            ">1, 5/234, d=1.170, g=0.661\n",
            ">1, 6/234, d=1.144, g=0.644\n",
            ">1, 7/234, d=1.131, g=0.630\n",
            ">1, 8/234, d=1.121, g=0.625\n",
            ">1, 9/234, d=1.115, g=0.642\n",
            ">1, 10/234, d=1.028, g=0.702\n",
            ">1, 11/234, d=0.985, g=0.787\n",
            ">1, 12/234, d=0.902, g=0.870\n",
            ">1, 13/234, d=0.886, g=0.902\n",
            ">1, 14/234, d=0.884, g=0.884\n",
            ">1, 15/234, d=0.882, g=0.840\n",
            ">1, 16/234, d=0.921, g=0.789\n",
            ">1, 17/234, d=0.948, g=0.740\n",
            ">1, 18/234, d=0.982, g=0.685\n",
            ">1, 19/234, d=1.076, g=0.611\n",
            ">1, 20/234, d=1.129, g=0.576\n",
            ">1, 21/234, d=1.133, g=0.595\n",
            ">1, 22/234, d=1.035, g=0.680\n",
            ">1, 23/234, d=0.871, g=0.870\n",
            ">1, 24/234, d=0.655, g=1.119\n",
            ">1, 25/234, d=0.547, g=1.231\n",
            ">1, 26/234, d=0.546, g=1.154\n",
            ">1, 27/234, d=0.593, g=1.002\n",
            ">1, 28/234, d=0.685, g=0.856\n",
            ">1, 29/234, d=0.764, g=0.747\n",
            ">1, 30/234, d=0.808, g=0.681\n",
            ">1, 31/234, d=0.873, g=0.640\n",
            ">1, 32/234, d=0.900, g=0.614\n",
            ">1, 33/234, d=0.924, g=0.602\n",
            ">1, 34/234, d=0.928, g=0.602\n",
            ">1, 35/234, d=0.919, g=0.618\n",
            ">1, 36/234, d=0.913, g=0.660\n",
            ">1, 37/234, d=0.869, g=0.721\n",
            ">1, 38/234, d=0.789, g=0.834\n",
            ">1, 39/234, d=0.734, g=0.961\n",
            ">1, 40/234, d=0.664, g=1.100\n",
            ">1, 41/234, d=0.644, g=1.168\n",
            ">1, 42/234, d=0.616, g=1.171\n",
            ">1, 43/234, d=0.669, g=1.101\n",
            ">1, 44/234, d=0.653, g=1.011\n",
            ">1, 45/234, d=0.693, g=0.946\n",
            ">1, 46/234, d=0.753, g=0.870\n",
            ">1, 47/234, d=0.735, g=0.832\n",
            ">1, 48/234, d=0.755, g=0.809\n",
            ">1, 49/234, d=0.750, g=0.804\n",
            ">1, 50/234, d=0.724, g=0.806\n",
            ">1, 51/234, d=0.738, g=0.800\n",
            ">1, 52/234, d=0.731, g=0.797\n",
            ">1, 53/234, d=0.723, g=0.787\n",
            ">1, 54/234, d=0.750, g=0.778\n",
            ">1, 55/234, d=0.762, g=0.767\n",
            ">1, 56/234, d=0.823, g=0.755\n",
            ">1, 57/234, d=0.869, g=0.738\n",
            ">1, 58/234, d=0.914, g=0.716\n",
            ">1, 59/234, d=0.912, g=0.681\n",
            ">1, 60/234, d=1.076, g=0.647\n",
            ">1, 61/234, d=1.087, g=0.633\n",
            ">1, 62/234, d=1.097, g=0.618\n",
            ">1, 63/234, d=1.161, g=0.644\n",
            ">1, 64/234, d=1.241, g=0.635\n",
            ">1, 65/234, d=1.288, g=0.644\n",
            ">1, 66/234, d=1.334, g=0.674\n",
            ">1, 67/234, d=1.361, g=0.751\n",
            ">1, 68/234, d=1.356, g=0.794\n",
            ">1, 69/234, d=1.470, g=0.763\n",
            ">1, 70/234, d=1.486, g=0.751\n",
            ">1, 71/234, d=1.408, g=0.806\n",
            ">1, 72/234, d=1.419, g=0.866\n",
            ">1, 73/234, d=1.546, g=0.884\n",
            ">1, 74/234, d=1.737, g=0.978\n",
            ">1, 75/234, d=1.570, g=1.217\n",
            ">1, 76/234, d=1.476, g=1.345\n",
            ">1, 77/234, d=1.391, g=1.358\n",
            ">1, 78/234, d=1.357, g=1.138\n",
            ">1, 79/234, d=1.544, g=0.935\n",
            ">1, 80/234, d=1.907, g=0.650\n",
            ">1, 81/234, d=2.392, g=0.539\n",
            ">1, 82/234, d=1.607, g=1.115\n",
            ">1, 83/234, d=0.970, g=1.661\n",
            ">1, 84/234, d=0.888, g=1.352\n",
            ">1, 85/234, d=1.141, g=0.928\n",
            ">1, 86/234, d=1.734, g=0.661\n",
            ">1, 87/234, d=2.014, g=0.640\n",
            ">1, 88/234, d=1.874, g=0.944\n",
            ">1, 89/234, d=1.723, g=1.239\n",
            ">1, 90/234, d=1.618, g=1.313\n",
            ">1, 91/234, d=1.524, g=1.325\n",
            ">1, 92/234, d=1.307, g=1.311\n",
            ">1, 93/234, d=1.227, g=1.329\n",
            ">1, 94/234, d=1.142, g=1.326\n",
            ">1, 95/234, d=1.096, g=1.376\n",
            ">1, 96/234, d=1.158, g=1.298\n",
            ">1, 97/234, d=1.084, g=1.268\n",
            ">1, 98/234, d=1.139, g=1.118\n",
            ">1, 99/234, d=1.194, g=1.051\n",
            ">1, 100/234, d=1.271, g=1.021\n",
            ">1, 101/234, d=1.430, g=0.935\n",
            ">1, 102/234, d=1.492, g=0.841\n",
            ">1, 103/234, d=1.746, g=0.753\n",
            ">1, 104/234, d=1.657, g=0.812\n",
            ">1, 105/234, d=1.441, g=0.978\n",
            ">1, 106/234, d=1.175, g=1.395\n",
            ">1, 107/234, d=0.874, g=1.662\n",
            ">1, 108/234, d=0.788, g=1.678\n",
            ">1, 109/234, d=0.762, g=1.487\n",
            ">1, 110/234, d=0.831, g=1.376\n",
            ">1, 111/234, d=0.936, g=1.144\n",
            ">1, 112/234, d=1.345, g=0.878\n",
            ">1, 113/234, d=1.637, g=0.745\n",
            ">1, 114/234, d=1.576, g=0.767\n",
            ">1, 115/234, d=1.653, g=0.908\n",
            ">1, 116/234, d=1.575, g=1.062\n",
            ">1, 117/234, d=1.447, g=1.168\n",
            ">1, 118/234, d=1.349, g=1.227\n",
            ">1, 119/234, d=1.226, g=1.300\n",
            ">1, 120/234, d=1.164, g=1.326\n",
            ">1, 121/234, d=1.070, g=1.370\n",
            ">1, 122/234, d=1.003, g=1.402\n",
            ">1, 123/234, d=0.891, g=1.425\n",
            ">1, 124/234, d=1.041, g=1.349\n",
            ">1, 125/234, d=1.026, g=1.280\n",
            ">1, 126/234, d=1.055, g=1.189\n",
            ">1, 127/234, d=1.060, g=1.142\n",
            ">1, 128/234, d=1.151, g=1.041\n",
            ">1, 129/234, d=1.310, g=0.961\n",
            ">1, 130/234, d=1.365, g=0.891\n",
            ">1, 131/234, d=1.453, g=0.799\n",
            ">1, 132/234, d=1.567, g=0.803\n",
            ">1, 133/234, d=1.495, g=0.762\n",
            ">1, 134/234, d=1.512, g=0.821\n",
            ">1, 135/234, d=1.339, g=0.870\n",
            ">1, 136/234, d=1.326, g=1.061\n",
            ">1, 137/234, d=1.087, g=1.185\n",
            ">1, 138/234, d=0.977, g=1.214\n",
            ">1, 139/234, d=0.951, g=1.168\n",
            ">1, 140/234, d=1.036, g=0.995\n",
            ">1, 141/234, d=1.363, g=0.789\n",
            ">1, 142/234, d=1.443, g=0.654\n",
            ">1, 143/234, d=1.713, g=0.598\n",
            ">1, 144/234, d=1.724, g=0.582\n",
            ">1, 145/234, d=1.689, g=0.637\n",
            ">1, 146/234, d=1.666, g=0.721\n",
            ">1, 147/234, d=1.559, g=0.823\n",
            ">1, 148/234, d=1.534, g=0.871\n",
            ">1, 149/234, d=1.517, g=0.876\n",
            ">1, 150/234, d=1.420, g=0.882\n",
            ">1, 151/234, d=1.378, g=0.874\n",
            ">1, 152/234, d=1.441, g=0.854\n",
            ">1, 153/234, d=1.470, g=0.818\n",
            ">1, 154/234, d=1.430, g=0.812\n",
            ">1, 155/234, d=1.466, g=0.718\n",
            ">1, 156/234, d=1.519, g=0.687\n",
            ">1, 157/234, d=1.457, g=0.670\n",
            ">1, 158/234, d=1.486, g=0.671\n",
            ">1, 159/234, d=1.492, g=0.641\n",
            ">1, 160/234, d=1.498, g=0.644\n",
            ">1, 161/234, d=1.427, g=0.685\n",
            ">1, 162/234, d=1.399, g=0.684\n",
            ">1, 163/234, d=1.336, g=0.705\n",
            ">1, 164/234, d=1.319, g=0.717\n",
            ">1, 165/234, d=1.296, g=0.728\n",
            ">1, 166/234, d=1.305, g=0.727\n",
            ">1, 167/234, d=1.346, g=0.687\n",
            ">1, 168/234, d=1.372, g=0.668\n",
            ">1, 169/234, d=1.363, g=0.690\n",
            ">1, 170/234, d=1.472, g=0.673\n",
            ">1, 171/234, d=1.397, g=0.727\n",
            ">1, 172/234, d=1.389, g=0.777\n",
            ">1, 173/234, d=1.349, g=0.845\n",
            ">1, 174/234, d=1.280, g=0.903\n",
            ">1, 175/234, d=1.247, g=0.967\n",
            ">1, 176/234, d=1.206, g=1.037\n",
            ">1, 177/234, d=1.238, g=1.088\n",
            ">1, 178/234, d=1.181, g=1.089\n",
            ">1, 179/234, d=1.178, g=1.112\n",
            ">1, 180/234, d=1.182, g=1.026\n",
            ">1, 181/234, d=1.285, g=0.907\n",
            ">1, 182/234, d=1.420, g=0.746\n",
            ">1, 183/234, d=1.667, g=0.639\n",
            ">1, 184/234, d=1.791, g=0.563\n",
            ">1, 185/234, d=1.750, g=0.551\n",
            ">1, 186/234, d=1.787, g=0.563\n",
            ">1, 187/234, d=1.668, g=0.654\n",
            ">1, 188/234, d=1.583, g=0.728\n",
            ">1, 189/234, d=1.467, g=0.800\n",
            ">1, 190/234, d=1.367, g=0.872\n",
            ">1, 191/234, d=1.328, g=0.899\n",
            ">1, 192/234, d=1.359, g=0.856\n",
            ">1, 193/234, d=1.387, g=0.809\n",
            ">1, 194/234, d=1.523, g=0.741\n",
            ">1, 195/234, d=1.539, g=0.726\n",
            ">1, 196/234, d=1.578, g=0.741\n",
            ">1, 197/234, d=1.627, g=0.764\n",
            ">1, 198/234, d=1.652, g=0.806\n",
            ">1, 199/234, d=1.603, g=0.851\n",
            ">1, 200/234, d=1.625, g=0.884\n",
            ">1, 201/234, d=1.607, g=0.857\n",
            ">1, 202/234, d=1.614, g=0.846\n",
            ">1, 203/234, d=1.631, g=0.812\n",
            ">1, 204/234, d=1.520, g=0.840\n",
            ">1, 205/234, d=1.554, g=0.825\n",
            ">1, 206/234, d=1.490, g=0.826\n",
            ">1, 207/234, d=1.566, g=0.812\n",
            ">1, 208/234, d=1.545, g=0.795\n",
            ">1, 209/234, d=1.569, g=0.799\n",
            ">1, 210/234, d=1.551, g=0.789\n",
            ">1, 211/234, d=1.567, g=0.789\n",
            ">1, 212/234, d=1.499, g=0.781\n",
            ">1, 213/234, d=1.514, g=0.816\n",
            ">1, 214/234, d=1.484, g=0.809\n",
            ">1, 215/234, d=1.454, g=0.831\n",
            ">1, 216/234, d=1.479, g=0.817\n",
            ">1, 217/234, d=1.450, g=0.847\n",
            ">1, 218/234, d=1.386, g=0.855\n",
            ">1, 219/234, d=1.406, g=0.872\n",
            ">1, 220/234, d=1.381, g=0.884\n",
            ">1, 221/234, d=1.343, g=0.862\n",
            ">1, 222/234, d=1.347, g=0.935\n",
            ">1, 223/234, d=1.325, g=0.926\n",
            ">1, 224/234, d=1.274, g=0.935\n",
            ">1, 225/234, d=1.264, g=0.972\n",
            ">1, 226/234, d=1.303, g=0.972\n",
            ">1, 227/234, d=1.302, g=0.971\n",
            ">1, 228/234, d=1.276, g=0.990\n",
            ">1, 229/234, d=1.279, g=0.989\n",
            ">1, 230/234, d=1.274, g=1.000\n",
            ">1, 231/234, d=1.228, g=1.024\n",
            ">1, 232/234, d=1.225, g=1.055\n",
            ">1, 233/234, d=1.173, g=1.047\n",
            ">1, 234/234, d=1.211, g=1.063\n",
            ">2, 1/234, d=1.129, g=1.046\n",
            ">2, 2/234, d=1.222, g=0.968\n",
            ">2, 3/234, d=1.252, g=0.924\n",
            ">2, 4/234, d=1.407, g=0.831\n",
            ">2, 5/234, d=1.477, g=0.747\n",
            ">2, 6/234, d=1.561, g=0.725\n",
            ">2, 7/234, d=1.596, g=0.680\n",
            ">2, 8/234, d=1.568, g=0.717\n",
            ">2, 9/234, d=1.509, g=0.773\n",
            ">2, 10/234, d=1.411, g=0.842\n",
            ">2, 11/234, d=1.311, g=0.953\n",
            ">2, 12/234, d=1.187, g=1.034\n",
            ">2, 13/234, d=1.185, g=1.118\n",
            ">2, 14/234, d=1.092, g=1.092\n",
            ">2, 15/234, d=1.210, g=0.977\n",
            ">2, 16/234, d=1.369, g=0.790\n",
            ">2, 17/234, d=1.433, g=0.662\n",
            ">2, 18/234, d=1.615, g=0.611\n",
            ">2, 19/234, d=1.701, g=0.668\n",
            ">2, 20/234, d=1.632, g=0.722\n",
            ">2, 21/234, d=1.529, g=0.778\n",
            ">2, 22/234, d=1.553, g=0.876\n",
            ">2, 23/234, d=1.396, g=0.965\n",
            ">2, 24/234, d=1.375, g=0.990\n",
            ">2, 25/234, d=1.334, g=1.011\n",
            ">2, 26/234, d=1.332, g=1.012\n",
            ">2, 27/234, d=1.312, g=1.005\n",
            ">2, 28/234, d=1.299, g=0.978\n",
            ">2, 29/234, d=1.295, g=0.914\n",
            ">2, 30/234, d=1.309, g=0.884\n",
            ">2, 31/234, d=1.382, g=0.865\n",
            ">2, 32/234, d=1.317, g=0.838\n",
            ">2, 33/234, d=1.354, g=0.816\n",
            ">2, 34/234, d=1.396, g=0.784\n",
            ">2, 35/234, d=1.424, g=0.798\n",
            ">2, 36/234, d=1.488, g=0.778\n",
            ">2, 37/234, d=1.411, g=0.823\n",
            ">2, 38/234, d=1.372, g=0.847\n",
            ">2, 39/234, d=1.319, g=0.886\n",
            ">2, 40/234, d=1.304, g=0.901\n",
            ">2, 41/234, d=1.277, g=0.904\n",
            ">2, 42/234, d=1.300, g=0.874\n",
            ">2, 43/234, d=1.355, g=0.817\n",
            ">2, 44/234, d=1.387, g=0.790\n",
            ">2, 45/234, d=1.412, g=0.764\n",
            ">2, 46/234, d=1.442, g=0.727\n",
            ">2, 47/234, d=1.495, g=0.750\n",
            ">2, 48/234, d=1.532, g=0.758\n",
            ">2, 49/234, d=1.464, g=0.762\n",
            ">2, 50/234, d=1.480, g=0.797\n",
            ">2, 51/234, d=1.434, g=0.809\n",
            ">2, 52/234, d=1.449, g=0.820\n",
            ">2, 53/234, d=1.414, g=0.805\n",
            ">2, 54/234, d=1.445, g=0.815\n",
            ">2, 55/234, d=1.403, g=0.805\n",
            ">2, 56/234, d=1.413, g=0.772\n",
            ">2, 57/234, d=1.451, g=0.769\n",
            ">2, 58/234, d=1.453, g=0.756\n",
            ">2, 59/234, d=1.504, g=0.722\n",
            ">2, 60/234, d=1.540, g=0.704\n",
            ">2, 61/234, d=1.569, g=0.689\n",
            ">2, 62/234, d=1.636, g=0.672\n",
            ">2, 63/234, d=1.598, g=0.688\n",
            ">2, 64/234, d=1.570, g=0.690\n",
            ">2, 65/234, d=1.526, g=0.715\n",
            ">2, 66/234, d=1.481, g=0.745\n",
            ">2, 67/234, d=1.436, g=0.783\n",
            ">2, 68/234, d=1.398, g=0.809\n",
            ">2, 69/234, d=1.355, g=0.815\n",
            ">2, 70/234, d=1.304, g=0.823\n",
            ">2, 71/234, d=1.345, g=0.813\n",
            ">2, 72/234, d=1.359, g=0.792\n",
            ">2, 73/234, d=1.397, g=0.761\n",
            ">2, 74/234, d=1.413, g=0.724\n",
            ">2, 75/234, d=1.462, g=0.730\n",
            ">2, 76/234, d=1.457, g=0.702\n",
            ">2, 77/234, d=1.490, g=0.704\n",
            ">2, 78/234, d=1.494, g=0.731\n",
            ">2, 79/234, d=1.487, g=0.740\n",
            ">2, 80/234, d=1.494, g=0.751\n",
            ">2, 81/234, d=1.474, g=0.753\n",
            ">2, 82/234, d=1.500, g=0.752\n",
            ">2, 83/234, d=1.493, g=0.751\n",
            ">2, 84/234, d=1.514, g=0.734\n",
            ">2, 85/234, d=1.509, g=0.710\n",
            ">2, 86/234, d=1.570, g=0.683\n",
            ">2, 87/234, d=1.492, g=0.675\n",
            ">2, 88/234, d=1.545, g=0.682\n",
            ">2, 89/234, d=1.521, g=0.676\n",
            ">2, 90/234, d=1.464, g=0.708\n",
            ">2, 91/234, d=1.496, g=0.712\n",
            ">2, 92/234, d=1.443, g=0.739\n",
            ">2, 93/234, d=1.413, g=0.779\n",
            ">2, 94/234, d=1.427, g=0.788\n",
            ">2, 95/234, d=1.395, g=0.794\n",
            ">2, 96/234, d=1.410, g=0.790\n",
            ">2, 97/234, d=1.424, g=0.800\n",
            ">2, 98/234, d=1.433, g=0.769\n",
            ">2, 99/234, d=1.423, g=0.756\n",
            ">2, 100/234, d=1.437, g=0.730\n",
            ">2, 101/234, d=1.516, g=0.716\n",
            ">2, 102/234, d=1.555, g=0.696\n",
            ">2, 103/234, d=1.551, g=0.717\n",
            ">2, 104/234, d=1.523, g=0.728\n",
            ">2, 105/234, d=1.500, g=0.780\n",
            ">2, 106/234, d=1.459, g=0.802\n",
            ">2, 107/234, d=1.467, g=0.831\n",
            ">2, 108/234, d=1.429, g=0.839\n",
            ">2, 109/234, d=1.437, g=0.841\n",
            ">2, 110/234, d=1.381, g=0.852\n",
            ">2, 111/234, d=1.415, g=0.830\n",
            ">2, 112/234, d=1.395, g=0.843\n",
            ">2, 113/234, d=1.409, g=0.821\n",
            ">2, 114/234, d=1.426, g=0.805\n",
            ">2, 115/234, d=1.451, g=0.750\n",
            ">2, 116/234, d=1.432, g=0.752\n",
            ">2, 117/234, d=1.492, g=0.737\n",
            ">2, 118/234, d=1.436, g=0.748\n",
            ">2, 119/234, d=1.471, g=0.745\n",
            ">2, 120/234, d=1.440, g=0.771\n",
            ">2, 121/234, d=1.414, g=0.787\n",
            ">2, 122/234, d=1.384, g=0.832\n",
            ">2, 123/234, d=1.318, g=0.864\n",
            ">2, 124/234, d=1.300, g=0.875\n",
            ">2, 125/234, d=1.268, g=0.919\n",
            ">2, 126/234, d=1.277, g=0.898\n",
            ">2, 127/234, d=1.238, g=0.879\n",
            ">2, 128/234, d=1.291, g=0.840\n",
            ">2, 129/234, d=1.381, g=0.794\n",
            ">2, 130/234, d=1.452, g=0.756\n",
            ">2, 131/234, d=1.450, g=0.743\n",
            ">2, 132/234, d=1.464, g=0.706\n",
            ">2, 133/234, d=1.521, g=0.737\n",
            ">2, 134/234, d=1.508, g=0.751\n",
            ">2, 135/234, d=1.477, g=0.788\n",
            ">2, 136/234, d=1.404, g=0.859\n",
            ">2, 137/234, d=1.383, g=0.901\n",
            ">2, 138/234, d=1.347, g=0.959\n",
            ">2, 139/234, d=1.338, g=1.004\n",
            ">2, 140/234, d=1.273, g=1.029\n",
            ">2, 141/234, d=1.238, g=1.075\n",
            ">2, 142/234, d=1.236, g=1.056\n",
            ">2, 143/234, d=1.219, g=1.055\n",
            ">2, 144/234, d=1.284, g=1.007\n",
            ">2, 145/234, d=1.285, g=0.915\n",
            ">2, 146/234, d=1.286, g=0.856\n",
            ">2, 147/234, d=1.398, g=0.804\n",
            ">2, 148/234, d=1.497, g=0.747\n",
            ">2, 149/234, d=1.512, g=0.694\n",
            ">2, 150/234, d=1.538, g=0.677\n",
            ">2, 151/234, d=1.529, g=0.689\n",
            ">2, 152/234, d=1.534, g=0.715\n",
            ">2, 153/234, d=1.489, g=0.743\n",
            ">2, 154/234, d=1.408, g=0.763\n",
            ">2, 155/234, d=1.398, g=0.838\n",
            ">2, 156/234, d=1.377, g=0.885\n",
            ">2, 157/234, d=1.292, g=0.957\n",
            ">2, 158/234, d=1.201, g=0.983\n",
            ">2, 159/234, d=1.159, g=1.002\n",
            ">2, 160/234, d=1.129, g=0.991\n",
            ">2, 161/234, d=1.190, g=0.989\n",
            ">2, 162/234, d=1.240, g=0.909\n",
            ">2, 163/234, d=1.277, g=0.844\n",
            ">2, 164/234, d=1.303, g=0.743\n",
            ">2, 165/234, d=1.443, g=0.700\n",
            ">2, 166/234, d=1.548, g=0.642\n",
            ">2, 167/234, d=1.606, g=0.633\n",
            ">2, 168/234, d=1.586, g=0.641\n",
            ">2, 169/234, d=1.616, g=0.657\n",
            ">2, 170/234, d=1.625, g=0.702\n",
            ">2, 171/234, d=1.540, g=0.735\n",
            ">2, 172/234, d=1.510, g=0.781\n",
            ">2, 173/234, d=1.454, g=0.834\n",
            ">2, 174/234, d=1.415, g=0.876\n",
            ">2, 175/234, d=1.390, g=0.902\n",
            ">2, 176/234, d=1.367, g=0.909\n",
            ">2, 177/234, d=1.376, g=0.915\n",
            ">2, 178/234, d=1.327, g=0.898\n",
            ">2, 179/234, d=1.424, g=0.855\n",
            ">2, 180/234, d=1.410, g=0.789\n",
            ">2, 181/234, d=1.456, g=0.739\n",
            ">2, 182/234, d=1.521, g=0.705\n",
            ">2, 183/234, d=1.522, g=0.668\n",
            ">2, 184/234, d=1.612, g=0.632\n",
            ">2, 185/234, d=1.580, g=0.608\n",
            ">2, 186/234, d=1.624, g=0.604\n",
            ">2, 187/234, d=1.625, g=0.602\n",
            ">2, 188/234, d=1.613, g=0.623\n",
            ">2, 189/234, d=1.576, g=0.635\n",
            ">2, 190/234, d=1.521, g=0.679\n",
            ">2, 191/234, d=1.491, g=0.701\n",
            ">2, 192/234, d=1.435, g=0.722\n",
            ">2, 193/234, d=1.395, g=0.742\n",
            ">2, 194/234, d=1.337, g=0.780\n",
            ">2, 195/234, d=1.326, g=0.792\n",
            ">2, 196/234, d=1.343, g=0.800\n",
            ">2, 197/234, d=1.315, g=0.780\n",
            ">2, 198/234, d=1.312, g=0.774\n",
            ">2, 199/234, d=1.358, g=0.741\n",
            ">2, 200/234, d=1.416, g=0.702\n",
            ">2, 201/234, d=1.444, g=0.667\n",
            ">2, 202/234, d=1.488, g=0.660\n",
            ">2, 203/234, d=1.485, g=0.654\n",
            ">2, 204/234, d=1.490, g=0.660\n",
            ">2, 205/234, d=1.455, g=0.681\n",
            ">2, 206/234, d=1.441, g=0.712\n",
            ">2, 207/234, d=1.413, g=0.744\n",
            ">2, 208/234, d=1.383, g=0.784\n",
            ">2, 209/234, d=1.351, g=0.834\n",
            ">2, 210/234, d=1.306, g=0.864\n",
            ">2, 211/234, d=1.294, g=0.876\n",
            ">2, 212/234, d=1.258, g=0.875\n",
            ">2, 213/234, d=1.272, g=0.882\n",
            ">2, 214/234, d=1.282, g=0.863\n",
            ">2, 215/234, d=1.250, g=0.838\n",
            ">2, 216/234, d=1.280, g=0.814\n",
            ">2, 217/234, d=1.339, g=0.770\n",
            ">2, 218/234, d=1.344, g=0.748\n",
            ">2, 219/234, d=1.355, g=0.723\n",
            ">2, 220/234, d=1.377, g=0.690\n",
            ">2, 221/234, d=1.380, g=0.686\n",
            ">2, 222/234, d=1.444, g=0.671\n",
            ">2, 223/234, d=1.436, g=0.668\n",
            ">2, 224/234, d=1.445, g=0.667\n",
            ">2, 225/234, d=1.419, g=0.685\n",
            ">2, 226/234, d=1.432, g=0.704\n",
            ">2, 227/234, d=1.371, g=0.729\n",
            ">2, 228/234, d=1.343, g=0.775\n",
            ">2, 229/234, d=1.309, g=0.809\n",
            ">2, 230/234, d=1.297, g=0.851\n",
            ">2, 231/234, d=1.248, g=0.882\n",
            ">2, 232/234, d=1.202, g=0.867\n",
            ">2, 233/234, d=1.274, g=0.856\n",
            ">2, 234/234, d=1.270, g=0.839\n",
            ">3, 1/234, d=1.271, g=0.803\n",
            ">3, 2/234, d=1.313, g=0.764\n",
            ">3, 3/234, d=1.380, g=0.736\n",
            ">3, 4/234, d=1.453, g=0.694\n",
            ">3, 5/234, d=1.486, g=0.690\n",
            ">3, 6/234, d=1.450, g=0.687\n",
            ">3, 7/234, d=1.498, g=0.685\n",
            ">3, 8/234, d=1.470, g=0.702\n",
            ">3, 9/234, d=1.472, g=0.732\n",
            ">3, 10/234, d=1.446, g=0.756\n",
            ">3, 11/234, d=1.408, g=0.774\n",
            ">3, 12/234, d=1.366, g=0.799\n",
            ">3, 13/234, d=1.370, g=0.826\n",
            ">3, 14/234, d=1.357, g=0.835\n",
            ">3, 15/234, d=1.337, g=0.845\n",
            ">3, 16/234, d=1.369, g=0.840\n",
            ">3, 17/234, d=1.321, g=0.826\n",
            ">3, 18/234, d=1.330, g=0.827\n",
            ">3, 19/234, d=1.362, g=0.804\n",
            ">3, 20/234, d=1.354, g=0.801\n",
            ">3, 21/234, d=1.390, g=0.769\n",
            ">3, 22/234, d=1.446, g=0.727\n",
            ">3, 23/234, d=1.453, g=0.689\n",
            ">3, 24/234, d=1.440, g=0.711\n",
            ">3, 25/234, d=1.454, g=0.705\n",
            ">3, 26/234, d=1.466, g=0.703\n",
            ">3, 27/234, d=1.459, g=0.721\n",
            ">3, 28/234, d=1.400, g=0.725\n",
            ">3, 29/234, d=1.374, g=0.734\n",
            ">3, 30/234, d=1.398, g=0.777\n",
            ">3, 31/234, d=1.359, g=0.789\n",
            ">3, 32/234, d=1.298, g=0.816\n",
            ">3, 33/234, d=1.298, g=0.819\n",
            ">3, 34/234, d=1.255, g=0.863\n",
            ">3, 35/234, d=1.253, g=0.851\n",
            ">3, 36/234, d=1.285, g=0.840\n",
            ">3, 37/234, d=1.279, g=0.842\n",
            ">3, 38/234, d=1.273, g=0.798\n",
            ">3, 39/234, d=1.312, g=0.795\n",
            ">3, 40/234, d=1.369, g=0.769\n",
            ">3, 41/234, d=1.366, g=0.746\n",
            ">3, 42/234, d=1.369, g=0.725\n",
            ">3, 43/234, d=1.455, g=0.694\n",
            ">3, 44/234, d=1.451, g=0.697\n",
            ">3, 45/234, d=1.459, g=0.701\n",
            ">3, 46/234, d=1.467, g=0.700\n",
            ">3, 47/234, d=1.475, g=0.722\n",
            ">3, 48/234, d=1.417, g=0.735\n",
            ">3, 49/234, d=1.418, g=0.760\n",
            ">3, 50/234, d=1.435, g=0.777\n",
            ">3, 51/234, d=1.390, g=0.794\n",
            ">3, 52/234, d=1.371, g=0.804\n",
            ">3, 53/234, d=1.361, g=0.827\n",
            ">3, 54/234, d=1.370, g=0.805\n",
            ">3, 55/234, d=1.374, g=0.808\n",
            ">3, 56/234, d=1.380, g=0.792\n",
            ">3, 57/234, d=1.426, g=0.777\n",
            ">3, 58/234, d=1.401, g=0.767\n",
            ">3, 59/234, d=1.431, g=0.740\n",
            ">3, 60/234, d=1.448, g=0.732\n",
            ">3, 61/234, d=1.500, g=0.721\n",
            ">3, 62/234, d=1.442, g=0.700\n",
            ">3, 63/234, d=1.502, g=0.697\n",
            ">3, 64/234, d=1.472, g=0.700\n",
            ">3, 65/234, d=1.499, g=0.720\n",
            ">3, 66/234, d=1.467, g=0.717\n",
            ">3, 67/234, d=1.426, g=0.744\n",
            ">3, 68/234, d=1.443, g=0.738\n",
            ">3, 69/234, d=1.424, g=0.756\n",
            ">3, 70/234, d=1.400, g=0.772\n",
            ">3, 71/234, d=1.398, g=0.780\n",
            ">3, 72/234, d=1.355, g=0.800\n",
            ">3, 73/234, d=1.396, g=0.796\n",
            ">3, 74/234, d=1.378, g=0.776\n",
            ">3, 75/234, d=1.380, g=0.786\n",
            ">3, 76/234, d=1.356, g=0.773\n",
            ">3, 77/234, d=1.375, g=0.772\n",
            ">3, 78/234, d=1.431, g=0.753\n",
            ">3, 79/234, d=1.445, g=0.736\n",
            ">3, 80/234, d=1.461, g=0.739\n",
            ">3, 81/234, d=1.469, g=0.732\n",
            ">3, 82/234, d=1.515, g=0.706\n",
            ">3, 83/234, d=1.510, g=0.716\n",
            ">3, 84/234, d=1.467, g=0.732\n",
            ">3, 85/234, d=1.497, g=0.753\n",
            ">3, 86/234, d=1.458, g=0.756\n",
            ">3, 87/234, d=1.416, g=0.802\n",
            ">3, 88/234, d=1.378, g=0.831\n",
            ">3, 89/234, d=1.381, g=0.843\n",
            ">3, 90/234, d=1.356, g=0.881\n",
            ">3, 91/234, d=1.333, g=0.896\n",
            ">3, 92/234, d=1.328, g=0.889\n",
            ">3, 93/234, d=1.308, g=0.874\n",
            ">3, 94/234, d=1.287, g=0.876\n",
            ">3, 95/234, d=1.334, g=0.855\n",
            ">3, 96/234, d=1.381, g=0.811\n",
            ">3, 97/234, d=1.374, g=0.787\n",
            ">3, 98/234, d=1.403, g=0.738\n",
            ">3, 99/234, d=1.460, g=0.711\n",
            ">3, 100/234, d=1.529, g=0.659\n",
            ">3, 101/234, d=1.552, g=0.654\n",
            ">3, 102/234, d=1.550, g=0.662\n",
            ">3, 103/234, d=1.502, g=0.666\n",
            ">3, 104/234, d=1.502, g=0.685\n",
            ">3, 105/234, d=1.467, g=0.723\n",
            ">3, 106/234, d=1.443, g=0.776\n",
            ">3, 107/234, d=1.372, g=0.826\n",
            ">3, 108/234, d=1.316, g=0.862\n",
            ">3, 109/234, d=1.295, g=0.888\n",
            ">3, 110/234, d=1.263, g=0.925\n",
            ">3, 111/234, d=1.208, g=0.981\n",
            ">3, 112/234, d=1.217, g=0.932\n",
            ">3, 113/234, d=1.259, g=0.918\n",
            ">3, 114/234, d=1.262, g=0.877\n",
            ">3, 115/234, d=1.306, g=0.810\n",
            ">3, 116/234, d=1.393, g=0.756\n",
            ">3, 117/234, d=1.470, g=0.694\n",
            ">3, 118/234, d=1.511, g=0.676\n",
            ">3, 119/234, d=1.491, g=0.664\n",
            ">3, 120/234, d=1.530, g=0.671\n",
            ">3, 121/234, d=1.482, g=0.687\n",
            ">3, 122/234, d=1.450, g=0.725\n",
            ">3, 123/234, d=1.404, g=0.781\n",
            ">3, 124/234, d=1.376, g=0.805\n",
            ">3, 125/234, d=1.319, g=0.844\n",
            ">3, 126/234, d=1.299, g=0.888\n",
            ">3, 127/234, d=1.337, g=0.871\n",
            ">3, 128/234, d=1.294, g=0.879\n",
            ">3, 129/234, d=1.301, g=0.864\n",
            ">3, 130/234, d=1.322, g=0.834\n",
            ">3, 131/234, d=1.339, g=0.804\n",
            ">3, 132/234, d=1.400, g=0.784\n",
            ">3, 133/234, d=1.382, g=0.730\n",
            ">3, 134/234, d=1.391, g=0.707\n",
            ">3, 135/234, d=1.501, g=0.671\n",
            ">3, 136/234, d=1.494, g=0.665\n",
            ">3, 137/234, d=1.465, g=0.655\n",
            ">3, 138/234, d=1.526, g=0.636\n",
            ">3, 139/234, d=1.532, g=0.638\n",
            ">3, 140/234, d=1.499, g=0.660\n",
            ">3, 141/234, d=1.489, g=0.679\n",
            ">3, 142/234, d=1.449, g=0.711\n",
            ">3, 143/234, d=1.430, g=0.734\n",
            ">3, 144/234, d=1.383, g=0.764\n",
            ">3, 145/234, d=1.339, g=0.789\n",
            ">3, 146/234, d=1.316, g=0.793\n",
            ">3, 147/234, d=1.329, g=0.819\n",
            ">3, 148/234, d=1.343, g=0.808\n",
            ">3, 149/234, d=1.332, g=0.794\n",
            ">3, 150/234, d=1.349, g=0.775\n",
            ">3, 151/234, d=1.359, g=0.754\n",
            ">3, 152/234, d=1.342, g=0.738\n",
            ">3, 153/234, d=1.381, g=0.730\n",
            ">3, 154/234, d=1.387, g=0.726\n",
            ">3, 155/234, d=1.413, g=0.723\n",
            ">3, 156/234, d=1.427, g=0.711\n",
            ">3, 157/234, d=1.394, g=0.720\n",
            ">3, 158/234, d=1.390, g=0.724\n",
            ">3, 159/234, d=1.407, g=0.729\n",
            ">3, 160/234, d=1.386, g=0.745\n",
            ">3, 161/234, d=1.378, g=0.754\n",
            ">3, 162/234, d=1.390, g=0.764\n",
            ">3, 163/234, d=1.368, g=0.765\n",
            ">3, 164/234, d=1.351, g=0.769\n",
            ">3, 165/234, d=1.370, g=0.781\n",
            ">3, 166/234, d=1.362, g=0.781\n",
            ">3, 167/234, d=1.383, g=0.779\n",
            ">3, 168/234, d=1.360, g=0.770\n",
            ">3, 169/234, d=1.393, g=0.763\n",
            ">3, 170/234, d=1.380, g=0.750\n",
            ">3, 171/234, d=1.366, g=0.748\n",
            ">3, 172/234, d=1.375, g=0.723\n",
            ">3, 173/234, d=1.378, g=0.726\n",
            ">3, 174/234, d=1.394, g=0.721\n",
            ">3, 175/234, d=1.452, g=0.718\n",
            ">3, 176/234, d=1.434, g=0.717\n",
            ">3, 177/234, d=1.440, g=0.698\n",
            ">3, 178/234, d=1.464, g=0.710\n",
            ">3, 179/234, d=1.448, g=0.727\n",
            ">3, 180/234, d=1.429, g=0.723\n",
            ">3, 181/234, d=1.423, g=0.751\n",
            ">3, 182/234, d=1.404, g=0.756\n",
            ">3, 183/234, d=1.377, g=0.750\n",
            ">3, 184/234, d=1.398, g=0.758\n",
            ">3, 185/234, d=1.383, g=0.775\n",
            ">3, 186/234, d=1.354, g=0.771\n",
            ">3, 187/234, d=1.361, g=0.777\n",
            ">3, 188/234, d=1.380, g=0.765\n",
            ">3, 189/234, d=1.357, g=0.756\n",
            ">3, 190/234, d=1.400, g=0.753\n",
            ">3, 191/234, d=1.376, g=0.735\n",
            ">3, 192/234, d=1.407, g=0.729\n",
            ">3, 193/234, d=1.427, g=0.715\n",
            ">3, 194/234, d=1.431, g=0.706\n",
            ">3, 195/234, d=1.407, g=0.717\n",
            ">3, 196/234, d=1.407, g=0.727\n",
            ">3, 197/234, d=1.385, g=0.736\n",
            ">3, 198/234, d=1.401, g=0.747\n",
            ">3, 199/234, d=1.386, g=0.765\n",
            ">3, 200/234, d=1.360, g=0.773\n",
            ">3, 201/234, d=1.356, g=0.784\n",
            ">3, 202/234, d=1.339, g=0.797\n",
            ">3, 203/234, d=1.347, g=0.807\n",
            ">3, 204/234, d=1.330, g=0.819\n",
            ">3, 205/234, d=1.316, g=0.798\n",
            ">3, 206/234, d=1.308, g=0.820\n",
            ">3, 207/234, d=1.309, g=0.799\n",
            ">3, 208/234, d=1.330, g=0.811\n",
            ">3, 209/234, d=1.324, g=0.789\n",
            ">3, 210/234, d=1.287, g=0.771\n",
            ">3, 211/234, d=1.343, g=0.767\n",
            ">3, 212/234, d=1.385, g=0.736\n",
            ">3, 213/234, d=1.398, g=0.733\n",
            ">3, 214/234, d=1.415, g=0.695\n",
            ">3, 215/234, d=1.425, g=0.684\n",
            ">3, 216/234, d=1.451, g=0.669\n",
            ">3, 217/234, d=1.439, g=0.664\n",
            ">3, 218/234, d=1.440, g=0.685\n",
            ">3, 219/234, d=1.449, g=0.683\n",
            ">3, 220/234, d=1.430, g=0.705\n",
            ">3, 221/234, d=1.404, g=0.740\n",
            ">3, 222/234, d=1.393, g=0.779\n",
            ">3, 223/234, d=1.356, g=0.803\n",
            ">3, 224/234, d=1.326, g=0.837\n",
            ">3, 225/234, d=1.306, g=0.845\n",
            ">3, 226/234, d=1.297, g=0.861\n",
            ">3, 227/234, d=1.292, g=0.849\n",
            ">3, 228/234, d=1.326, g=0.799\n",
            ">3, 229/234, d=1.364, g=0.774\n",
            ">3, 230/234, d=1.405, g=0.735\n",
            ">3, 231/234, d=1.386, g=0.705\n",
            ">3, 232/234, d=1.448, g=0.686\n",
            ">3, 233/234, d=1.477, g=0.660\n",
            ">3, 234/234, d=1.470, g=0.658\n",
            ">4, 1/234, d=1.473, g=0.666\n",
            ">4, 2/234, d=1.460, g=0.689\n",
            ">4, 3/234, d=1.431, g=0.719\n",
            ">4, 4/234, d=1.422, g=0.739\n",
            ">4, 5/234, d=1.400, g=0.773\n",
            ">4, 6/234, d=1.343, g=0.795\n",
            ">4, 7/234, d=1.365, g=0.805\n",
            ">4, 8/234, d=1.341, g=0.816\n",
            ">4, 9/234, d=1.340, g=0.844\n",
            ">4, 10/234, d=1.299, g=0.833\n",
            ">4, 11/234, d=1.286, g=0.823\n",
            ">4, 12/234, d=1.313, g=0.803\n",
            ">4, 13/234, d=1.311, g=0.797\n",
            ">4, 14/234, d=1.324, g=0.807\n",
            ">4, 15/234, d=1.370, g=0.765\n",
            ">4, 16/234, d=1.389, g=0.715\n",
            ">4, 17/234, d=1.408, g=0.714\n",
            ">4, 18/234, d=1.439, g=0.687\n",
            ">4, 19/234, d=1.451, g=0.688\n",
            ">4, 20/234, d=1.439, g=0.676\n",
            ">4, 21/234, d=1.444, g=0.678\n",
            ">4, 22/234, d=1.417, g=0.680\n",
            ">4, 23/234, d=1.405, g=0.699\n",
            ">4, 24/234, d=1.416, g=0.717\n",
            ">4, 25/234, d=1.363, g=0.733\n",
            ">4, 26/234, d=1.390, g=0.764\n",
            ">4, 27/234, d=1.346, g=0.783\n",
            ">4, 28/234, d=1.299, g=0.790\n",
            ">4, 29/234, d=1.306, g=0.815\n",
            ">4, 30/234, d=1.320, g=0.822\n",
            ">4, 31/234, d=1.310, g=0.825\n",
            ">4, 32/234, d=1.308, g=0.794\n",
            ">4, 33/234, d=1.310, g=0.776\n",
            ">4, 34/234, d=1.375, g=0.742\n",
            ">4, 35/234, d=1.397, g=0.729\n",
            ">4, 36/234, d=1.399, g=0.715\n",
            ">4, 37/234, d=1.460, g=0.706\n",
            ">4, 38/234, d=1.427, g=0.712\n",
            ">4, 39/234, d=1.397, g=0.719\n",
            ">4, 40/234, d=1.415, g=0.736\n",
            ">4, 41/234, d=1.399, g=0.745\n",
            ">4, 42/234, d=1.389, g=0.757\n",
            ">4, 43/234, d=1.368, g=0.763\n",
            ">4, 44/234, d=1.372, g=0.769\n",
            ">4, 45/234, d=1.335, g=0.790\n",
            ">4, 46/234, d=1.326, g=0.788\n",
            ">4, 47/234, d=1.335, g=0.803\n",
            ">4, 48/234, d=1.345, g=0.797\n",
            ">4, 49/234, d=1.341, g=0.798\n",
            ">4, 50/234, d=1.340, g=0.777\n",
            ">4, 51/234, d=1.351, g=0.778\n",
            ">4, 52/234, d=1.368, g=0.765\n",
            ">4, 53/234, d=1.338, g=0.769\n",
            ">4, 54/234, d=1.346, g=0.763\n",
            ">4, 55/234, d=1.348, g=0.765\n",
            ">4, 56/234, d=1.375, g=0.742\n",
            ">4, 57/234, d=1.353, g=0.737\n",
            ">4, 58/234, d=1.373, g=0.728\n",
            ">4, 59/234, d=1.361, g=0.734\n",
            ">4, 60/234, d=1.387, g=0.725\n",
            ">4, 61/234, d=1.372, g=0.732\n",
            ">4, 62/234, d=1.368, g=0.717\n",
            ">4, 63/234, d=1.364, g=0.744\n",
            ">4, 64/234, d=1.387, g=0.728\n",
            ">4, 65/234, d=1.374, g=0.751\n",
            ">4, 66/234, d=1.368, g=0.756\n",
            ">4, 67/234, d=1.348, g=0.746\n",
            ">4, 68/234, d=1.349, g=0.760\n",
            ">4, 69/234, d=1.350, g=0.767\n",
            ">4, 70/234, d=1.354, g=0.767\n",
            ">4, 71/234, d=1.351, g=0.755\n",
            ">4, 72/234, d=1.349, g=0.749\n",
            ">4, 73/234, d=1.328, g=0.755\n",
            ">4, 74/234, d=1.347, g=0.739\n",
            ">4, 75/234, d=1.344, g=0.743\n",
            ">4, 76/234, d=1.363, g=0.734\n",
            ">4, 77/234, d=1.360, g=0.716\n",
            ">4, 78/234, d=1.356, g=0.708\n",
            ">4, 79/234, d=1.393, g=0.699\n",
            ">4, 80/234, d=1.403, g=0.691\n",
            ">4, 81/234, d=1.400, g=0.698\n",
            ">4, 82/234, d=1.422, g=0.704\n",
            ">4, 83/234, d=1.422, g=0.697\n",
            ">4, 84/234, d=1.384, g=0.720\n",
            ">4, 85/234, d=1.396, g=0.722\n",
            ">4, 86/234, d=1.391, g=0.744\n",
            ">4, 87/234, d=1.361, g=0.744\n",
            ">4, 88/234, d=1.376, g=0.743\n",
            ">4, 89/234, d=1.356, g=0.753\n",
            ">4, 90/234, d=1.342, g=0.756\n",
            ">4, 91/234, d=1.357, g=0.763\n",
            ">4, 92/234, d=1.344, g=0.756\n",
            ">4, 93/234, d=1.340, g=0.750\n",
            ">4, 94/234, d=1.374, g=0.751\n",
            ">4, 95/234, d=1.367, g=0.737\n",
            ">4, 96/234, d=1.362, g=0.727\n",
            ">4, 97/234, d=1.363, g=0.726\n",
            ">4, 98/234, d=1.376, g=0.712\n",
            ">4, 99/234, d=1.405, g=0.706\n",
            ">4, 100/234, d=1.408, g=0.702\n",
            ">4, 101/234, d=1.410, g=0.689\n",
            ">4, 102/234, d=1.398, g=0.696\n",
            ">4, 103/234, d=1.418, g=0.704\n",
            ">4, 104/234, d=1.374, g=0.702\n",
            ">4, 105/234, d=1.393, g=0.699\n",
            ">4, 106/234, d=1.384, g=0.715\n",
            ">4, 107/234, d=1.383, g=0.720\n",
            ">4, 108/234, d=1.367, g=0.738\n",
            ">4, 109/234, d=1.337, g=0.741\n",
            ">4, 110/234, d=1.357, g=0.747\n",
            ">4, 111/234, d=1.324, g=0.763\n",
            ">4, 112/234, d=1.358, g=0.754\n",
            ">4, 113/234, d=1.332, g=0.754\n",
            ">4, 114/234, d=1.337, g=0.735\n",
            ">4, 115/234, d=1.376, g=0.752\n",
            ">4, 116/234, d=1.376, g=0.715\n",
            ">4, 117/234, d=1.395, g=0.723\n",
            ">4, 118/234, d=1.390, g=0.708\n",
            ">4, 119/234, d=1.411, g=0.710\n",
            ">4, 120/234, d=1.401, g=0.727\n",
            ">4, 121/234, d=1.395, g=0.733\n",
            ">4, 122/234, d=1.384, g=0.727\n",
            ">4, 123/234, d=1.376, g=0.737\n",
            ">4, 124/234, d=1.379, g=0.754\n",
            ">4, 125/234, d=1.347, g=0.763\n",
            ">4, 126/234, d=1.356, g=0.784\n",
            ">4, 127/234, d=1.349, g=0.789\n",
            ">4, 128/234, d=1.333, g=0.804\n",
            ">4, 129/234, d=1.313, g=0.791\n",
            ">4, 130/234, d=1.356, g=0.781\n",
            ">4, 131/234, d=1.346, g=0.786\n",
            ">4, 132/234, d=1.334, g=0.788\n",
            ">4, 133/234, d=1.320, g=0.769\n",
            ">4, 134/234, d=1.327, g=0.749\n",
            ">4, 135/234, d=1.328, g=0.761\n",
            ">4, 136/234, d=1.317, g=0.744\n",
            ">4, 137/234, d=1.368, g=0.736\n",
            ">4, 138/234, d=1.383, g=0.740\n",
            ">4, 139/234, d=1.352, g=0.738\n",
            ">4, 140/234, d=1.367, g=0.747\n",
            ">4, 141/234, d=1.360, g=0.756\n",
            ">4, 142/234, d=1.358, g=0.761\n",
            ">4, 143/234, d=1.343, g=0.783\n",
            ">4, 144/234, d=1.345, g=0.779\n",
            ">4, 145/234, d=1.331, g=0.793\n",
            ">4, 146/234, d=1.302, g=0.801\n",
            ">4, 147/234, d=1.300, g=0.809\n",
            ">4, 148/234, d=1.313, g=0.827\n",
            ">4, 149/234, d=1.273, g=0.810\n",
            ">4, 150/234, d=1.320, g=0.809\n",
            ">4, 151/234, d=1.302, g=0.791\n",
            ">4, 152/234, d=1.326, g=0.784\n",
            ">4, 153/234, d=1.313, g=0.788\n",
            ">4, 154/234, d=1.320, g=0.773\n",
            ">4, 155/234, d=1.316, g=0.768\n",
            ">4, 156/234, d=1.353, g=0.751\n",
            ">4, 157/234, d=1.363, g=0.756\n",
            ">4, 158/234, d=1.339, g=0.755\n",
            ">4, 159/234, d=1.349, g=0.748\n",
            ">4, 160/234, d=1.331, g=0.748\n",
            ">4, 161/234, d=1.334, g=0.769\n",
            ">4, 162/234, d=1.344, g=0.783\n",
            ">4, 163/234, d=1.341, g=0.783\n",
            ">4, 164/234, d=1.325, g=0.784\n",
            ">4, 165/234, d=1.317, g=0.785\n",
            ">4, 166/234, d=1.300, g=0.790\n",
            ">4, 167/234, d=1.304, g=0.787\n",
            ">4, 168/234, d=1.305, g=0.779\n",
            ">4, 169/234, d=1.326, g=0.761\n",
            ">4, 170/234, d=1.324, g=0.756\n",
            ">4, 171/234, d=1.348, g=0.752\n",
            ">4, 172/234, d=1.312, g=0.738\n",
            ">4, 173/234, d=1.322, g=0.750\n",
            ">4, 174/234, d=1.355, g=0.731\n",
            ">4, 175/234, d=1.361, g=0.740\n",
            ">4, 176/234, d=1.339, g=0.731\n",
            ">4, 177/234, d=1.333, g=0.734\n",
            ">4, 178/234, d=1.351, g=0.730\n",
            ">4, 179/234, d=1.325, g=0.738\n",
            ">4, 180/234, d=1.330, g=0.753\n",
            ">4, 181/234, d=1.317, g=0.763\n",
            ">4, 182/234, d=1.337, g=0.732\n",
            ">4, 183/234, d=1.304, g=0.766\n",
            ">4, 184/234, d=1.296, g=0.754\n",
            ">4, 185/234, d=1.319, g=0.763\n",
            ">4, 186/234, d=1.330, g=0.762\n",
            ">4, 187/234, d=1.324, g=0.762\n",
            ">4, 188/234, d=1.322, g=0.752\n",
            ">4, 189/234, d=1.301, g=0.745\n",
            ">4, 190/234, d=1.328, g=0.733\n",
            ">4, 191/234, d=1.335, g=0.729\n",
            ">4, 192/234, d=1.336, g=0.722\n",
            ">4, 193/234, d=1.348, g=0.720\n",
            ">4, 194/234, d=1.339, g=0.727\n",
            ">4, 195/234, d=1.328, g=0.725\n",
            ">4, 196/234, d=1.342, g=0.733\n",
            ">4, 197/234, d=1.305, g=0.739\n",
            ">4, 198/234, d=1.326, g=0.747\n",
            ">4, 199/234, d=1.312, g=0.745\n",
            ">4, 200/234, d=1.325, g=0.770\n",
            ">4, 201/234, d=1.337, g=0.770\n",
            ">4, 202/234, d=1.333, g=0.767\n",
            ">4, 203/234, d=1.304, g=0.762\n",
            ">4, 204/234, d=1.306, g=0.750\n",
            ">4, 205/234, d=1.308, g=0.749\n",
            ">4, 206/234, d=1.332, g=0.739\n",
            ">4, 207/234, d=1.314, g=0.737\n",
            ">4, 208/234, d=1.302, g=0.746\n",
            ">4, 209/234, d=1.331, g=0.723\n",
            ">4, 210/234, d=1.334, g=0.718\n",
            ">4, 211/234, d=1.322, g=0.726\n",
            ">4, 212/234, d=1.332, g=0.721\n",
            ">4, 213/234, d=1.330, g=0.733\n",
            ">4, 214/234, d=1.325, g=0.741\n",
            ">4, 215/234, d=1.327, g=0.759\n",
            ">4, 216/234, d=1.336, g=0.781\n",
            ">4, 217/234, d=1.286, g=0.777\n",
            ">4, 218/234, d=1.271, g=0.791\n",
            ">4, 219/234, d=1.244, g=0.790\n",
            ">4, 220/234, d=1.261, g=0.817\n",
            ">4, 221/234, d=1.292, g=0.790\n",
            ">4, 222/234, d=1.282, g=0.787\n",
            ">4, 223/234, d=1.312, g=0.783\n",
            ">4, 224/234, d=1.290, g=0.770\n",
            ">4, 225/234, d=1.286, g=0.749\n",
            ">4, 226/234, d=1.298, g=0.741\n",
            ">4, 227/234, d=1.287, g=0.745\n",
            ">4, 228/234, d=1.318, g=0.770\n",
            ">4, 229/234, d=1.339, g=0.761\n",
            ">4, 230/234, d=1.308, g=0.773\n",
            ">4, 231/234, d=1.280, g=0.792\n",
            ">4, 232/234, d=1.298, g=0.797\n",
            ">4, 233/234, d=1.275, g=0.810\n",
            ">4, 234/234, d=1.266, g=0.835\n",
            ">5, 1/234, d=1.259, g=0.840\n",
            ">5, 2/234, d=1.281, g=0.826\n",
            ">5, 3/234, d=1.254, g=0.822\n",
            ">5, 4/234, d=1.283, g=0.823\n",
            ">5, 5/234, d=1.250, g=0.817\n",
            ">5, 6/234, d=1.273, g=0.796\n",
            ">5, 7/234, d=1.307, g=0.804\n",
            ">5, 8/234, d=1.300, g=0.767\n",
            ">5, 9/234, d=1.289, g=0.754\n",
            ">5, 10/234, d=1.312, g=0.766\n",
            ">5, 11/234, d=1.298, g=0.755\n",
            ">5, 12/234, d=1.328, g=0.756\n",
            ">5, 13/234, d=1.268, g=0.780\n",
            ">5, 14/234, d=1.283, g=0.778\n",
            ">5, 15/234, d=1.267, g=0.808\n",
            ">5, 16/234, d=1.233, g=0.815\n",
            ">5, 17/234, d=1.295, g=0.829\n",
            ">5, 18/234, d=1.241, g=0.803\n",
            ">5, 19/234, d=1.262, g=0.821\n",
            ">5, 20/234, d=1.263, g=0.817\n",
            ">5, 21/234, d=1.271, g=0.778\n",
            ">5, 22/234, d=1.293, g=0.790\n",
            ">5, 23/234, d=1.224, g=0.765\n",
            ">5, 24/234, d=1.265, g=0.771\n",
            ">5, 25/234, d=1.341, g=0.746\n",
            ">5, 26/234, d=1.277, g=0.779\n",
            ">5, 27/234, d=1.292, g=0.785\n",
            ">5, 28/234, d=1.280, g=0.799\n",
            ">5, 29/234, d=1.262, g=0.790\n",
            ">5, 30/234, d=1.242, g=0.834\n",
            ">5, 31/234, d=1.267, g=0.835\n",
            ">5, 32/234, d=1.250, g=0.823\n",
            ">5, 33/234, d=1.246, g=0.830\n",
            ">5, 34/234, d=1.248, g=0.831\n",
            ">5, 35/234, d=1.229, g=0.837\n",
            ">5, 36/234, d=1.239, g=0.792\n",
            ">5, 37/234, d=1.229, g=0.800\n",
            ">5, 38/234, d=1.232, g=0.804\n",
            ">5, 39/234, d=1.267, g=0.795\n",
            ">5, 40/234, d=1.250, g=0.791\n",
            ">5, 41/234, d=1.278, g=0.786\n",
            ">5, 42/234, d=1.243, g=0.774\n",
            ">5, 43/234, d=1.240, g=0.827\n",
            ">5, 44/234, d=1.220, g=0.812\n",
            ">5, 45/234, d=1.246, g=0.810\n",
            ">5, 46/234, d=1.232, g=0.847\n",
            ">5, 47/234, d=1.225, g=0.835\n",
            ">5, 48/234, d=1.197, g=0.837\n",
            ">5, 49/234, d=1.210, g=0.854\n",
            ">5, 50/234, d=1.209, g=0.828\n",
            ">5, 51/234, d=1.205, g=0.820\n",
            ">5, 52/234, d=1.212, g=0.805\n",
            ">5, 53/234, d=1.200, g=0.818\n",
            ">5, 54/234, d=1.224, g=0.816\n",
            ">5, 55/234, d=1.205, g=0.816\n",
            ">5, 56/234, d=1.215, g=0.825\n",
            ">5, 57/234, d=1.208, g=0.837\n",
            ">5, 58/234, d=1.230, g=0.835\n",
            ">5, 59/234, d=1.260, g=0.847\n",
            ">5, 60/234, d=1.196, g=0.915\n",
            ">5, 61/234, d=1.236, g=0.871\n",
            ">5, 62/234, d=1.216, g=0.879\n",
            ">5, 63/234, d=1.184, g=0.881\n",
            ">5, 64/234, d=1.185, g=0.891\n",
            ">5, 65/234, d=1.224, g=0.881\n",
            ">5, 66/234, d=1.187, g=0.849\n",
            ">5, 67/234, d=1.224, g=0.834\n",
            ">5, 68/234, d=1.207, g=0.826\n",
            ">5, 69/234, d=1.215, g=0.844\n",
            ">5, 70/234, d=1.228, g=0.835\n",
            ">5, 71/234, d=1.208, g=0.815\n",
            ">5, 72/234, d=1.234, g=0.831\n",
            ">5, 73/234, d=1.233, g=0.841\n",
            ">5, 74/234, d=1.214, g=0.874\n",
            ">5, 75/234, d=1.215, g=0.861\n",
            ">5, 76/234, d=1.151, g=0.859\n",
            ">5, 77/234, d=1.180, g=0.863\n",
            ">5, 78/234, d=1.178, g=0.855\n",
            ">5, 79/234, d=1.207, g=0.853\n",
            ">5, 80/234, d=1.177, g=0.844\n",
            ">5, 81/234, d=1.205, g=0.825\n",
            ">5, 82/234, d=1.193, g=0.876\n",
            ">5, 83/234, d=1.200, g=0.873\n",
            ">5, 84/234, d=1.169, g=0.880\n",
            ">5, 85/234, d=1.188, g=0.879\n",
            ">5, 86/234, d=1.143, g=0.884\n",
            ">5, 87/234, d=1.174, g=0.900\n",
            ">5, 88/234, d=1.142, g=0.892\n",
            ">5, 89/234, d=1.196, g=0.882\n",
            ">5, 90/234, d=1.183, g=0.869\n",
            ">5, 91/234, d=1.210, g=0.842\n",
            ">5, 92/234, d=1.217, g=0.895\n",
            ">5, 93/234, d=1.162, g=0.864\n",
            ">5, 94/234, d=1.161, g=0.871\n",
            ">5, 95/234, d=1.206, g=0.859\n",
            ">5, 96/234, d=1.161, g=0.856\n",
            ">5, 97/234, d=1.149, g=0.852\n",
            ">5, 98/234, d=1.174, g=0.860\n",
            ">5, 99/234, d=1.186, g=0.871\n",
            ">5, 100/234, d=1.189, g=0.886\n",
            ">5, 101/234, d=1.175, g=0.873\n",
            ">5, 102/234, d=1.121, g=0.845\n",
            ">5, 103/234, d=1.179, g=0.876\n",
            ">5, 104/234, d=1.092, g=0.889\n",
            ">5, 105/234, d=1.199, g=0.865\n",
            ">5, 106/234, d=1.175, g=0.858\n",
            ">5, 107/234, d=1.184, g=0.889\n",
            ">5, 108/234, d=1.200, g=0.903\n",
            ">5, 109/234, d=1.208, g=0.891\n",
            ">5, 110/234, d=1.198, g=0.876\n",
            ">5, 111/234, d=1.164, g=0.937\n",
            ">5, 112/234, d=1.150, g=0.851\n",
            ">5, 113/234, d=1.176, g=0.901\n",
            ">5, 114/234, d=1.124, g=0.872\n",
            ">5, 115/234, d=1.137, g=0.873\n",
            ">5, 116/234, d=1.160, g=0.929\n",
            ">5, 117/234, d=1.172, g=0.872\n",
            ">5, 118/234, d=1.142, g=0.909\n",
            ">5, 119/234, d=1.131, g=0.913\n",
            ">5, 120/234, d=1.087, g=0.911\n",
            ">5, 121/234, d=1.140, g=0.939\n",
            ">5, 122/234, d=1.189, g=0.911\n",
            ">5, 123/234, d=1.202, g=0.937\n",
            ">5, 124/234, d=1.155, g=0.937\n",
            ">5, 125/234, d=1.164, g=0.958\n",
            ">5, 126/234, d=1.157, g=0.930\n",
            ">5, 127/234, d=1.148, g=0.898\n",
            ">5, 128/234, d=1.139, g=0.961\n",
            ">5, 129/234, d=1.122, g=0.934\n",
            ">5, 130/234, d=1.178, g=0.924\n",
            ">5, 131/234, d=1.166, g=0.953\n",
            ">5, 132/234, d=1.178, g=0.946\n",
            ">5, 133/234, d=1.184, g=0.981\n",
            ">5, 134/234, d=1.154, g=0.924\n",
            ">5, 135/234, d=1.136, g=0.961\n",
            ">5, 136/234, d=1.083, g=0.972\n",
            ">5, 137/234, d=1.161, g=0.936\n",
            ">5, 138/234, d=1.134, g=0.916\n",
            ">5, 139/234, d=1.156, g=0.969\n",
            ">5, 140/234, d=1.205, g=0.940\n",
            ">5, 141/234, d=1.159, g=0.922\n",
            ">5, 142/234, d=1.234, g=0.901\n",
            ">5, 143/234, d=1.148, g=0.913\n",
            ">5, 144/234, d=1.163, g=0.909\n",
            ">5, 145/234, d=1.218, g=0.880\n",
            ">5, 146/234, d=1.215, g=0.890\n",
            ">5, 147/234, d=1.196, g=0.887\n",
            ">5, 148/234, d=1.177, g=0.892\n",
            ">5, 149/234, d=1.231, g=0.859\n",
            ">5, 150/234, d=1.229, g=0.901\n",
            ">5, 151/234, d=1.303, g=0.896\n",
            ">5, 152/234, d=1.335, g=0.798\n",
            ">5, 153/234, d=1.266, g=0.837\n",
            ">5, 154/234, d=1.406, g=0.823\n",
            ">5, 155/234, d=1.393, g=0.773\n",
            ">5, 156/234, d=1.354, g=0.777\n",
            ">5, 157/234, d=1.451, g=0.770\n",
            ">5, 158/234, d=1.508, g=0.765\n",
            ">5, 159/234, d=1.396, g=0.807\n",
            ">5, 160/234, d=1.345, g=0.805\n",
            ">5, 161/234, d=1.476, g=0.794\n",
            ">5, 162/234, d=1.434, g=0.764\n",
            ">5, 163/234, d=1.406, g=0.774\n",
            ">5, 164/234, d=1.482, g=0.775\n",
            ">5, 165/234, d=1.442, g=0.789\n",
            ">5, 166/234, d=1.425, g=0.833\n",
            ">5, 167/234, d=1.534, g=0.808\n",
            ">5, 168/234, d=1.413, g=0.818\n",
            ">5, 169/234, d=1.422, g=0.809\n",
            ">5, 170/234, d=1.438, g=0.781\n",
            ">5, 171/234, d=1.420, g=0.789\n",
            ">5, 172/234, d=1.390, g=0.800\n",
            ">5, 173/234, d=1.403, g=0.802\n",
            ">5, 174/234, d=1.437, g=0.788\n",
            ">5, 175/234, d=1.423, g=0.794\n",
            ">5, 176/234, d=1.402, g=0.781\n",
            ">5, 177/234, d=1.353, g=0.794\n",
            ">5, 178/234, d=1.417, g=0.782\n",
            ">5, 179/234, d=1.420, g=0.788\n",
            ">5, 180/234, d=1.411, g=0.772\n",
            ">5, 181/234, d=1.395, g=0.789\n",
            ">5, 182/234, d=1.379, g=0.776\n",
            ">5, 183/234, d=1.399, g=0.775\n",
            ">5, 184/234, d=1.379, g=0.784\n",
            ">5, 185/234, d=1.321, g=0.772\n",
            ">5, 186/234, d=1.374, g=0.791\n",
            ">5, 187/234, d=1.338, g=0.777\n",
            ">5, 188/234, d=1.395, g=0.765\n",
            ">5, 189/234, d=1.391, g=0.765\n",
            ">5, 190/234, d=1.365, g=0.768\n",
            ">5, 191/234, d=1.339, g=0.759\n",
            ">5, 192/234, d=1.362, g=0.785\n",
            ">5, 193/234, d=1.354, g=0.776\n",
            ">5, 194/234, d=1.342, g=0.796\n",
            ">5, 195/234, d=1.369, g=0.785\n",
            ">5, 196/234, d=1.385, g=0.803\n",
            ">5, 197/234, d=1.389, g=0.783\n",
            ">5, 198/234, d=1.390, g=0.767\n",
            ">5, 199/234, d=1.378, g=0.774\n",
            ">5, 200/234, d=1.413, g=0.783\n",
            ">5, 201/234, d=1.370, g=0.778\n",
            ">5, 202/234, d=1.422, g=0.774\n",
            ">5, 203/234, d=1.375, g=0.774\n",
            ">5, 204/234, d=1.391, g=0.759\n",
            ">5, 205/234, d=1.367, g=0.775\n",
            ">5, 206/234, d=1.389, g=0.785\n",
            ">5, 207/234, d=1.381, g=0.798\n",
            ">5, 208/234, d=1.379, g=0.782\n",
            ">5, 209/234, d=1.388, g=0.807\n",
            ">5, 210/234, d=1.391, g=0.826\n",
            ">5, 211/234, d=1.377, g=0.824\n",
            ">5, 212/234, d=1.357, g=0.816\n",
            ">5, 213/234, d=1.384, g=0.816\n",
            ">5, 214/234, d=1.365, g=0.814\n",
            ">5, 215/234, d=1.351, g=0.809\n",
            ">5, 216/234, d=1.401, g=0.785\n",
            ">5, 217/234, d=1.368, g=0.785\n",
            ">5, 218/234, d=1.412, g=0.797\n",
            ">5, 219/234, d=1.399, g=0.804\n",
            ">5, 220/234, d=1.386, g=0.802\n",
            ">5, 221/234, d=1.420, g=0.797\n",
            ">5, 222/234, d=1.397, g=0.799\n",
            ">5, 223/234, d=1.376, g=0.781\n",
            ">5, 224/234, d=1.400, g=0.772\n",
            ">5, 225/234, d=1.374, g=0.797\n",
            ">5, 226/234, d=1.369, g=0.784\n",
            ">5, 227/234, d=1.404, g=0.773\n",
            ">5, 228/234, d=1.380, g=0.777\n",
            ">5, 229/234, d=1.393, g=0.776\n",
            ">5, 230/234, d=1.376, g=0.807\n",
            ">5, 231/234, d=1.344, g=0.832\n",
            ">5, 232/234, d=1.433, g=0.821\n",
            ">5, 233/234, d=1.347, g=0.789\n",
            ">5, 234/234, d=1.398, g=0.779\n",
            ">6, 1/234, d=1.418, g=0.795\n",
            ">6, 2/234, d=1.350, g=0.847\n",
            ">6, 3/234, d=1.309, g=0.878\n",
            ">6, 4/234, d=1.337, g=0.841\n",
            ">6, 5/234, d=1.446, g=0.761\n",
            ">6, 6/234, d=1.464, g=0.766\n",
            ">6, 7/234, d=1.439, g=0.799\n",
            ">6, 8/234, d=1.398, g=0.902\n",
            ">6, 9/234, d=1.277, g=1.001\n",
            ">6, 10/234, d=1.291, g=1.024\n",
            ">6, 11/234, d=1.272, g=0.960\n",
            ">6, 12/234, d=1.313, g=0.876\n",
            ">6, 13/234, d=1.437, g=0.773\n",
            ">6, 14/234, d=1.484, g=0.768\n",
            ">6, 15/234, d=1.389, g=0.898\n",
            ">6, 16/234, d=1.230, g=1.053\n",
            ">6, 17/234, d=1.203, g=1.057\n",
            ">6, 18/234, d=1.303, g=0.839\n",
            ">6, 19/234, d=1.449, g=0.733\n",
            ">6, 20/234, d=1.539, g=0.696\n",
            ">6, 21/234, d=1.447, g=0.793\n",
            ">6, 22/234, d=1.393, g=0.914\n",
            ">6, 23/234, d=1.298, g=0.998\n",
            ">6, 24/234, d=1.278, g=1.027\n",
            ">6, 25/234, d=1.254, g=0.978\n",
            ">6, 26/234, d=1.349, g=0.887\n",
            ">6, 27/234, d=1.449, g=0.780\n",
            ">6, 28/234, d=1.434, g=0.768\n",
            ">6, 29/234, d=1.346, g=0.804\n",
            ">6, 30/234, d=1.306, g=0.864\n",
            ">6, 31/234, d=1.305, g=0.888\n",
            ">6, 32/234, d=1.266, g=0.884\n",
            ">6, 33/234, d=1.342, g=0.815\n",
            ">6, 34/234, d=1.386, g=0.773\n",
            ">6, 35/234, d=1.419, g=0.781\n",
            ">6, 36/234, d=1.395, g=0.766\n",
            ">6, 37/234, d=1.379, g=0.801\n",
            ">6, 38/234, d=1.378, g=0.825\n",
            ">6, 39/234, d=1.405, g=0.833\n",
            ">6, 40/234, d=1.379, g=0.833\n",
            ">6, 41/234, d=1.418, g=0.821\n",
            ">6, 42/234, d=1.368, g=0.824\n",
            ">6, 43/234, d=1.316, g=0.829\n",
            ">6, 44/234, d=1.337, g=0.839\n",
            ">6, 45/234, d=1.310, g=0.822\n",
            ">6, 46/234, d=1.310, g=0.836\n",
            ">6, 47/234, d=1.331, g=0.800\n",
            ">6, 48/234, d=1.368, g=0.787\n",
            ">6, 49/234, d=1.333, g=0.773\n",
            ">6, 50/234, d=1.403, g=0.775\n",
            ">6, 51/234, d=1.365, g=0.771\n",
            ">6, 52/234, d=1.386, g=0.738\n",
            ">6, 53/234, d=1.434, g=0.749\n",
            ">6, 54/234, d=1.375, g=0.760\n",
            ">6, 55/234, d=1.380, g=0.769\n",
            ">6, 56/234, d=1.403, g=0.760\n",
            ">6, 57/234, d=1.421, g=0.749\n",
            ">6, 58/234, d=1.422, g=0.754\n",
            ">6, 59/234, d=1.404, g=0.764\n",
            ">6, 60/234, d=1.385, g=0.766\n",
            ">6, 61/234, d=1.391, g=0.766\n",
            ">6, 62/234, d=1.396, g=0.788\n",
            ">6, 63/234, d=1.355, g=0.784\n",
            ">6, 64/234, d=1.343, g=0.787\n",
            ">6, 65/234, d=1.343, g=0.780\n",
            ">6, 66/234, d=1.334, g=0.788\n",
            ">6, 67/234, d=1.324, g=0.809\n",
            ">6, 68/234, d=1.338, g=0.790\n",
            ">6, 69/234, d=1.354, g=0.799\n",
            ">6, 70/234, d=1.363, g=0.777\n",
            ">6, 71/234, d=1.333, g=0.777\n",
            ">6, 72/234, d=1.357, g=0.781\n",
            ">6, 73/234, d=1.356, g=0.777\n",
            ">6, 74/234, d=1.321, g=0.785\n",
            ">6, 75/234, d=1.358, g=0.792\n",
            ">6, 76/234, d=1.372, g=0.776\n",
            ">6, 77/234, d=1.356, g=0.787\n",
            ">6, 78/234, d=1.341, g=0.781\n",
            ">6, 79/234, d=1.338, g=0.785\n",
            ">6, 80/234, d=1.357, g=0.797\n",
            ">6, 81/234, d=1.331, g=0.777\n",
            ">6, 82/234, d=1.357, g=0.782\n",
            ">6, 83/234, d=1.384, g=0.787\n",
            ">6, 84/234, d=1.363, g=0.776\n",
            ">6, 85/234, d=1.369, g=0.779\n",
            ">6, 86/234, d=1.342, g=0.793\n",
            ">6, 87/234, d=1.365, g=0.763\n",
            ">6, 88/234, d=1.358, g=0.765\n",
            ">6, 89/234, d=1.366, g=0.783\n",
            ">6, 90/234, d=1.366, g=0.787\n",
            ">6, 91/234, d=1.380, g=0.796\n",
            ">6, 92/234, d=1.345, g=0.788\n",
            ">6, 93/234, d=1.361, g=0.785\n",
            ">6, 94/234, d=1.369, g=0.784\n",
            ">6, 95/234, d=1.385, g=0.781\n",
            ">6, 96/234, d=1.359, g=0.765\n",
            ">6, 97/234, d=1.377, g=0.772\n",
            ">6, 98/234, d=1.351, g=0.762\n",
            ">6, 99/234, d=1.385, g=0.773\n",
            ">6, 100/234, d=1.397, g=0.764\n",
            ">6, 101/234, d=1.435, g=0.754\n",
            ">6, 102/234, d=1.380, g=0.792\n",
            ">6, 103/234, d=1.391, g=0.803\n",
            ">6, 104/234, d=1.406, g=0.806\n",
            ">6, 105/234, d=1.362, g=0.771\n",
            ">6, 106/234, d=1.389, g=0.761\n",
            ">6, 107/234, d=1.418, g=0.772\n",
            ">6, 108/234, d=1.405, g=0.761\n",
            ">6, 109/234, d=1.430, g=0.743\n",
            ">6, 110/234, d=1.414, g=0.771\n",
            ">6, 111/234, d=1.414, g=0.779\n",
            ">6, 112/234, d=1.357, g=0.784\n",
            ">6, 113/234, d=1.331, g=0.804\n",
            ">6, 114/234, d=1.375, g=0.787\n",
            ">6, 115/234, d=1.401, g=0.764\n",
            ">6, 116/234, d=1.442, g=0.748\n",
            ">6, 117/234, d=1.414, g=0.736\n",
            ">6, 118/234, d=1.448, g=0.729\n",
            ">6, 119/234, d=1.443, g=0.756\n",
            ">6, 120/234, d=1.427, g=0.772\n",
            ">6, 121/234, d=1.373, g=0.793\n",
            ">6, 122/234, d=1.366, g=0.792\n",
            ">6, 123/234, d=1.411, g=0.782\n",
            ">6, 124/234, d=1.380, g=0.771\n",
            ">6, 125/234, d=1.432, g=0.756\n",
            ">6, 126/234, d=1.393, g=0.732\n",
            ">6, 127/234, d=1.482, g=0.740\n",
            ">6, 128/234, d=1.450, g=0.765\n",
            ">6, 129/234, d=1.398, g=0.771\n",
            ">6, 130/234, d=1.409, g=0.785\n",
            ">6, 131/234, d=1.385, g=0.805\n",
            ">6, 132/234, d=1.386, g=0.802\n",
            ">6, 133/234, d=1.380, g=0.786\n",
            ">6, 134/234, d=1.391, g=0.758\n",
            ">6, 135/234, d=1.393, g=0.758\n",
            ">6, 136/234, d=1.380, g=0.759\n",
            ">6, 137/234, d=1.401, g=0.753\n",
            ">6, 138/234, d=1.386, g=0.770\n",
            ">6, 139/234, d=1.367, g=0.760\n",
            ">6, 140/234, d=1.362, g=0.769\n",
            ">6, 141/234, d=1.358, g=0.776\n",
            ">6, 142/234, d=1.414, g=0.763\n",
            ">6, 143/234, d=1.372, g=0.758\n",
            ">6, 144/234, d=1.399, g=0.755\n",
            ">6, 145/234, d=1.387, g=0.758\n",
            ">6, 146/234, d=1.406, g=0.752\n",
            ">6, 147/234, d=1.405, g=0.739\n",
            ">6, 148/234, d=1.398, g=0.748\n",
            ">6, 149/234, d=1.382, g=0.764\n",
            ">6, 150/234, d=1.428, g=0.753\n",
            ">6, 151/234, d=1.420, g=0.742\n",
            ">6, 152/234, d=1.415, g=0.734\n",
            ">6, 153/234, d=1.393, g=0.733\n",
            ">6, 154/234, d=1.439, g=0.732\n",
            ">6, 155/234, d=1.423, g=0.744\n",
            ">6, 156/234, d=1.410, g=0.748\n",
            ">6, 157/234, d=1.415, g=0.726\n",
            ">6, 158/234, d=1.401, g=0.746\n",
            ">6, 159/234, d=1.380, g=0.738\n",
            ">6, 160/234, d=1.394, g=0.745\n",
            ">6, 161/234, d=1.383, g=0.749\n",
            ">6, 162/234, d=1.424, g=0.748\n",
            ">6, 163/234, d=1.400, g=0.749\n",
            ">6, 164/234, d=1.397, g=0.748\n",
            ">6, 165/234, d=1.337, g=0.741\n",
            ">6, 166/234, d=1.386, g=0.754\n",
            ">6, 167/234, d=1.402, g=0.750\n",
            ">6, 168/234, d=1.398, g=0.752\n",
            ">6, 169/234, d=1.400, g=0.751\n",
            ">6, 170/234, d=1.405, g=0.743\n",
            ">6, 171/234, d=1.355, g=0.756\n",
            ">6, 172/234, d=1.395, g=0.747\n",
            ">6, 173/234, d=1.402, g=0.754\n",
            ">6, 174/234, d=1.392, g=0.748\n",
            ">6, 175/234, d=1.374, g=0.753\n",
            ">6, 176/234, d=1.426, g=0.759\n",
            ">6, 177/234, d=1.405, g=0.756\n",
            ">6, 178/234, d=1.386, g=0.763\n",
            ">6, 179/234, d=1.414, g=0.772\n",
            ">6, 180/234, d=1.391, g=0.766\n",
            ">6, 181/234, d=1.397, g=0.772\n",
            ">6, 182/234, d=1.384, g=0.761\n",
            ">6, 183/234, d=1.388, g=0.768\n",
            ">6, 184/234, d=1.380, g=0.771\n",
            ">6, 185/234, d=1.390, g=0.767\n",
            ">6, 186/234, d=1.408, g=0.767\n",
            ">6, 187/234, d=1.373, g=0.779\n",
            ">6, 188/234, d=1.413, g=0.771\n",
            ">6, 189/234, d=1.389, g=0.769\n",
            ">6, 190/234, d=1.364, g=0.777\n",
            ">6, 191/234, d=1.366, g=0.774\n",
            ">6, 192/234, d=1.364, g=0.762\n",
            ">6, 193/234, d=1.364, g=0.764\n",
            ">6, 194/234, d=1.360, g=0.764\n",
            ">6, 195/234, d=1.367, g=0.773\n",
            ">6, 196/234, d=1.404, g=0.768\n",
            ">6, 197/234, d=1.374, g=0.775\n",
            ">6, 198/234, d=1.406, g=0.754\n",
            ">6, 199/234, d=1.368, g=0.754\n",
            ">6, 200/234, d=1.380, g=0.759\n",
            ">6, 201/234, d=1.369, g=0.780\n",
            ">6, 202/234, d=1.363, g=0.780\n",
            ">6, 203/234, d=1.383, g=0.783\n",
            ">6, 204/234, d=1.360, g=0.771\n",
            ">6, 205/234, d=1.401, g=0.772\n",
            ">6, 206/234, d=1.388, g=0.742\n",
            ">6, 207/234, d=1.398, g=0.754\n",
            ">6, 208/234, d=1.395, g=0.745\n",
            ">6, 209/234, d=1.417, g=0.750\n",
            ">6, 210/234, d=1.392, g=0.749\n",
            ">6, 211/234, d=1.357, g=0.751\n",
            ">6, 212/234, d=1.360, g=0.760\n",
            ">6, 213/234, d=1.368, g=0.780\n",
            ">6, 214/234, d=1.371, g=0.769\n",
            ">6, 215/234, d=1.362, g=0.762\n",
            ">6, 216/234, d=1.419, g=0.729\n",
            ">6, 217/234, d=1.405, g=0.747\n",
            ">6, 218/234, d=1.415, g=0.722\n",
            ">6, 219/234, d=1.398, g=0.765\n",
            ">6, 220/234, d=1.372, g=0.781\n",
            ">6, 221/234, d=1.325, g=0.819\n",
            ">6, 222/234, d=1.317, g=0.835\n",
            ">6, 223/234, d=1.340, g=0.839\n",
            ">6, 224/234, d=1.388, g=0.816\n",
            ">6, 225/234, d=1.318, g=0.794\n",
            ">6, 226/234, d=1.342, g=0.761\n",
            ">6, 227/234, d=1.355, g=0.735\n",
            ">6, 228/234, d=1.404, g=0.750\n",
            ">6, 229/234, d=1.416, g=0.749\n",
            ">6, 230/234, d=1.352, g=0.783\n",
            ">6, 231/234, d=1.337, g=0.808\n",
            ">6, 232/234, d=1.313, g=0.832\n",
            ">6, 233/234, d=1.293, g=0.866\n",
            ">6, 234/234, d=1.290, g=0.848\n",
            ">7, 1/234, d=1.322, g=0.826\n",
            ">7, 2/234, d=1.381, g=0.761\n",
            ">7, 3/234, d=1.384, g=0.754\n",
            ">7, 4/234, d=1.398, g=0.751\n",
            ">7, 5/234, d=1.415, g=0.752\n",
            ">7, 6/234, d=1.352, g=0.773\n",
            ">7, 7/234, d=1.345, g=0.830\n",
            ">7, 8/234, d=1.348, g=0.882\n",
            ">7, 9/234, d=1.293, g=0.912\n",
            ">7, 10/234, d=1.314, g=0.922\n",
            ">7, 11/234, d=1.343, g=0.862\n",
            ">7, 12/234, d=1.451, g=0.784\n",
            ">7, 13/234, d=1.386, g=0.757\n",
            ">7, 14/234, d=1.446, g=0.726\n",
            ">7, 15/234, d=1.411, g=0.731\n",
            ">7, 16/234, d=1.383, g=0.755\n",
            ">7, 17/234, d=1.386, g=0.796\n",
            ">7, 18/234, d=1.323, g=0.805\n",
            ">7, 19/234, d=1.322, g=0.796\n",
            ">7, 20/234, d=1.309, g=0.798\n",
            ">7, 21/234, d=1.366, g=0.761\n",
            ">7, 22/234, d=1.379, g=0.738\n",
            ">7, 23/234, d=1.446, g=0.734\n",
            ">7, 24/234, d=1.395, g=0.731\n",
            ">7, 25/234, d=1.401, g=0.744\n",
            ">7, 26/234, d=1.364, g=0.762\n",
            ">7, 27/234, d=1.326, g=0.788\n",
            ">7, 28/234, d=1.325, g=0.824\n",
            ">7, 29/234, d=1.303, g=0.834\n",
            ">7, 30/234, d=1.308, g=0.849\n",
            ">7, 31/234, d=1.320, g=0.844\n",
            ">7, 32/234, d=1.349, g=0.816\n",
            ">7, 33/234, d=1.356, g=0.763\n",
            ">7, 34/234, d=1.391, g=0.736\n",
            ">7, 35/234, d=1.407, g=0.712\n",
            ">7, 36/234, d=1.407, g=0.728\n",
            ">7, 37/234, d=1.386, g=0.763\n",
            ">7, 38/234, d=1.327, g=0.784\n",
            ">7, 39/234, d=1.352, g=0.785\n",
            ">7, 40/234, d=1.288, g=0.812\n",
            ">7, 41/234, d=1.314, g=0.774\n",
            ">7, 42/234, d=1.345, g=0.766\n",
            ">7, 43/234, d=1.357, g=0.733\n",
            ">7, 44/234, d=1.407, g=0.724\n",
            ">7, 45/234, d=1.428, g=0.728\n",
            ">7, 46/234, d=1.391, g=0.731\n",
            ">7, 47/234, d=1.381, g=0.756\n",
            ">7, 48/234, d=1.386, g=0.809\n",
            ">7, 49/234, d=1.314, g=0.827\n",
            ">7, 50/234, d=1.344, g=0.883\n",
            ">7, 51/234, d=1.309, g=0.898\n",
            ">7, 52/234, d=1.339, g=0.879\n",
            ">7, 53/234, d=1.386, g=0.805\n",
            ">7, 54/234, d=1.428, g=0.747\n",
            ">7, 55/234, d=1.450, g=0.736\n",
            ">7, 56/234, d=1.424, g=0.744\n",
            ">7, 57/234, d=1.387, g=0.758\n",
            ">7, 58/234, d=1.310, g=0.802\n",
            ">7, 59/234, d=1.303, g=0.820\n",
            ">7, 60/234, d=1.320, g=0.832\n",
            ">7, 61/234, d=1.299, g=0.805\n",
            ">7, 62/234, d=1.309, g=0.807\n",
            ">7, 63/234, d=1.339, g=0.773\n",
            ">7, 64/234, d=1.392, g=0.743\n",
            ">7, 65/234, d=1.364, g=0.742\n",
            ">7, 66/234, d=1.397, g=0.732\n",
            ">7, 67/234, d=1.410, g=0.760\n",
            ">7, 68/234, d=1.368, g=0.783\n",
            ">7, 69/234, d=1.381, g=0.832\n",
            ">7, 70/234, d=1.343, g=0.871\n",
            ">7, 71/234, d=1.296, g=0.906\n",
            ">7, 72/234, d=1.289, g=0.938\n",
            ">7, 73/234, d=1.300, g=0.923\n",
            ">7, 74/234, d=1.312, g=0.875\n",
            ">7, 75/234, d=1.366, g=0.797\n",
            ">7, 76/234, d=1.421, g=0.739\n",
            ">7, 77/234, d=1.429, g=0.722\n",
            ">7, 78/234, d=1.467, g=0.735\n",
            ">7, 79/234, d=1.396, g=0.745\n",
            ">7, 80/234, d=1.343, g=0.781\n",
            ">7, 81/234, d=1.334, g=0.798\n",
            ">7, 82/234, d=1.326, g=0.827\n",
            ">7, 83/234, d=1.328, g=0.791\n",
            ">7, 84/234, d=1.345, g=0.801\n",
            ">7, 85/234, d=1.329, g=0.778\n",
            ">7, 86/234, d=1.442, g=0.762\n",
            ">7, 87/234, d=1.351, g=0.736\n",
            ">7, 88/234, d=1.395, g=0.740\n",
            ">7, 89/234, d=1.393, g=0.753\n",
            ">7, 90/234, d=1.369, g=0.772\n",
            ">7, 91/234, d=1.374, g=0.782\n",
            ">7, 92/234, d=1.340, g=0.804\n",
            ">7, 93/234, d=1.315, g=0.834\n",
            ">7, 94/234, d=1.319, g=0.848\n",
            ">7, 95/234, d=1.291, g=0.867\n",
            ">7, 96/234, d=1.301, g=0.870\n",
            ">7, 97/234, d=1.312, g=0.848\n",
            ">7, 98/234, d=1.374, g=0.796\n",
            ">7, 99/234, d=1.349, g=0.767\n",
            ">7, 100/234, d=1.408, g=0.737\n",
            ">7, 101/234, d=1.428, g=0.744\n",
            ">7, 102/234, d=1.352, g=0.757\n",
            ">7, 103/234, d=1.343, g=0.782\n",
            ">7, 104/234, d=1.305, g=0.818\n",
            ">7, 105/234, d=1.315, g=0.817\n",
            ">7, 106/234, d=1.331, g=0.829\n",
            ">7, 107/234, d=1.391, g=0.797\n",
            ">7, 108/234, d=1.364, g=0.792\n",
            ">7, 109/234, d=1.380, g=0.771\n",
            ">7, 110/234, d=1.377, g=0.749\n",
            ">7, 111/234, d=1.391, g=0.746\n",
            ">7, 112/234, d=1.376, g=0.737\n",
            ">7, 113/234, d=1.371, g=0.772\n",
            ">7, 114/234, d=1.356, g=0.776\n",
            ">7, 115/234, d=1.329, g=0.785\n",
            ">7, 116/234, d=1.373, g=0.812\n",
            ">7, 117/234, d=1.339, g=0.799\n",
            ">7, 118/234, d=1.346, g=0.788\n",
            ">7, 119/234, d=1.367, g=0.806\n",
            ">7, 120/234, d=1.369, g=0.789\n",
            ">7, 121/234, d=1.402, g=0.760\n",
            ">7, 122/234, d=1.375, g=0.742\n",
            ">7, 123/234, d=1.392, g=0.734\n",
            ">7, 124/234, d=1.387, g=0.722\n",
            ">7, 125/234, d=1.384, g=0.732\n",
            ">7, 126/234, d=1.350, g=0.755\n",
            ">7, 127/234, d=1.375, g=0.772\n",
            ">7, 128/234, d=1.312, g=0.783\n",
            ">7, 129/234, d=1.288, g=0.785\n",
            ">7, 130/234, d=1.315, g=0.786\n",
            ">7, 131/234, d=1.335, g=0.792\n",
            ">7, 132/234, d=1.345, g=0.774\n",
            ">7, 133/234, d=1.334, g=0.780\n",
            ">7, 134/234, d=1.349, g=0.740\n",
            ">7, 135/234, d=1.407, g=0.732\n",
            ">7, 136/234, d=1.395, g=0.735\n",
            ">7, 137/234, d=1.349, g=0.770\n",
            ">7, 138/234, d=1.368, g=0.778\n",
            ">7, 139/234, d=1.383, g=0.813\n",
            ">7, 140/234, d=1.311, g=0.856\n",
            ">7, 141/234, d=1.307, g=0.865\n",
            ">7, 142/234, d=1.313, g=0.858\n",
            ">7, 143/234, d=1.378, g=0.833\n",
            ">7, 144/234, d=1.385, g=0.802\n",
            ">7, 145/234, d=1.394, g=0.783\n",
            ">7, 146/234, d=1.391, g=0.758\n",
            ">7, 147/234, d=1.399, g=0.752\n",
            ">7, 148/234, d=1.369, g=0.769\n",
            ">7, 149/234, d=1.397, g=0.761\n",
            ">7, 150/234, d=1.352, g=0.761\n",
            ">7, 151/234, d=1.369, g=0.783\n",
            ">7, 152/234, d=1.342, g=0.798\n",
            ">7, 153/234, d=1.336, g=0.789\n",
            ">7, 154/234, d=1.355, g=0.787\n",
            ">7, 155/234, d=1.337, g=0.764\n",
            ">7, 156/234, d=1.390, g=0.746\n",
            ">7, 157/234, d=1.397, g=0.735\n",
            ">7, 158/234, d=1.401, g=0.715\n",
            ">7, 159/234, d=1.409, g=0.732\n",
            ">7, 160/234, d=1.345, g=0.756\n",
            ">7, 161/234, d=1.369, g=0.781\n",
            ">7, 162/234, d=1.376, g=0.818\n",
            ">7, 163/234, d=1.352, g=0.836\n",
            ">7, 164/234, d=1.299, g=0.828\n",
            ">7, 165/234, d=1.305, g=0.834\n",
            ">7, 166/234, d=1.329, g=0.806\n",
            ">7, 167/234, d=1.384, g=0.776\n",
            ">7, 168/234, d=1.411, g=0.743\n",
            ">7, 169/234, d=1.365, g=0.733\n",
            ">7, 170/234, d=1.389, g=0.740\n",
            ">7, 171/234, d=1.374, g=0.745\n",
            ">7, 172/234, d=1.344, g=0.772\n",
            ">7, 173/234, d=1.328, g=0.790\n",
            ">7, 174/234, d=1.294, g=0.794\n",
            ">7, 175/234, d=1.319, g=0.820\n",
            ">7, 176/234, d=1.332, g=0.789\n",
            ">7, 177/234, d=1.340, g=0.764\n",
            ">7, 178/234, d=1.319, g=0.747\n",
            ">7, 179/234, d=1.387, g=0.742\n",
            ">7, 180/234, d=1.382, g=0.740\n",
            ">7, 181/234, d=1.348, g=0.743\n",
            ">7, 182/234, d=1.384, g=0.771\n",
            ">7, 183/234, d=1.330, g=0.804\n",
            ">7, 184/234, d=1.316, g=0.817\n",
            ">7, 185/234, d=1.314, g=0.822\n",
            ">7, 186/234, d=1.334, g=0.811\n",
            ">7, 187/234, d=1.337, g=0.815\n",
            ">7, 188/234, d=1.347, g=0.790\n",
            ">7, 189/234, d=1.368, g=0.766\n",
            ">7, 190/234, d=1.376, g=0.758\n",
            ">7, 191/234, d=1.392, g=0.754\n",
            ">7, 192/234, d=1.350, g=0.753\n",
            ">7, 193/234, d=1.380, g=0.765\n",
            ">7, 194/234, d=1.369, g=0.762\n",
            ">7, 195/234, d=1.335, g=0.770\n",
            ">7, 196/234, d=1.346, g=0.776\n",
            ">7, 197/234, d=1.341, g=0.780\n",
            ">7, 198/234, d=1.333, g=0.752\n",
            ">7, 199/234, d=1.381, g=0.759\n",
            ">7, 200/234, d=1.382, g=0.779\n",
            ">7, 201/234, d=1.382, g=0.762\n",
            ">7, 202/234, d=1.374, g=0.776\n",
            ">7, 203/234, d=1.366, g=0.777\n",
            ">7, 204/234, d=1.366, g=0.794\n",
            ">7, 205/234, d=1.359, g=0.785\n",
            ">7, 206/234, d=1.342, g=0.799\n",
            ">7, 207/234, d=1.368, g=0.811\n",
            ">7, 208/234, d=1.339, g=0.789\n",
            ">7, 209/234, d=1.344, g=0.792\n",
            ">7, 210/234, d=1.347, g=0.767\n",
            ">7, 211/234, d=1.337, g=0.757\n",
            ">7, 212/234, d=1.387, g=0.765\n",
            ">7, 213/234, d=1.375, g=0.743\n",
            ">7, 214/234, d=1.350, g=0.746\n",
            ">7, 215/234, d=1.345, g=0.756\n",
            ">7, 216/234, d=1.329, g=0.783\n",
            ">7, 217/234, d=1.367, g=0.779\n",
            ">7, 218/234, d=1.316, g=0.777\n",
            ">7, 219/234, d=1.294, g=0.786\n",
            ">7, 220/234, d=1.350, g=0.757\n",
            ">7, 221/234, d=1.327, g=0.756\n",
            ">7, 222/234, d=1.364, g=0.725\n",
            ">7, 223/234, d=1.382, g=0.737\n",
            ">7, 224/234, d=1.412, g=0.748\n",
            ">7, 225/234, d=1.396, g=0.758\n",
            ">7, 226/234, d=1.376, g=0.782\n",
            ">7, 227/234, d=1.337, g=0.815\n",
            ">7, 228/234, d=1.333, g=0.841\n",
            ">7, 229/234, d=1.323, g=0.851\n",
            ">7, 230/234, d=1.324, g=0.867\n",
            ">7, 231/234, d=1.357, g=0.811\n",
            ">7, 232/234, d=1.403, g=0.782\n",
            ">7, 233/234, d=1.411, g=0.753\n",
            ">7, 234/234, d=1.397, g=0.741\n",
            ">8, 1/234, d=1.370, g=0.750\n",
            ">8, 2/234, d=1.362, g=0.753\n",
            ">8, 3/234, d=1.335, g=0.781\n",
            ">8, 4/234, d=1.317, g=0.817\n",
            ">8, 5/234, d=1.329, g=0.795\n",
            ">8, 6/234, d=1.344, g=0.820\n",
            ">8, 7/234, d=1.322, g=0.791\n",
            ">8, 8/234, d=1.363, g=0.764\n",
            ">8, 9/234, d=1.392, g=0.739\n",
            ">8, 10/234, d=1.408, g=0.729\n",
            ">8, 11/234, d=1.370, g=0.754\n",
            ">8, 12/234, d=1.387, g=0.776\n",
            ">8, 13/234, d=1.349, g=0.813\n",
            ">8, 14/234, d=1.365, g=0.838\n",
            ">8, 15/234, d=1.329, g=0.855\n",
            ">8, 16/234, d=1.324, g=0.851\n",
            ">8, 17/234, d=1.344, g=0.863\n",
            ">8, 18/234, d=1.354, g=0.837\n",
            ">8, 19/234, d=1.309, g=0.803\n",
            ">8, 20/234, d=1.402, g=0.757\n",
            ">8, 21/234, d=1.374, g=0.743\n",
            ">8, 22/234, d=1.417, g=0.732\n",
            ">8, 23/234, d=1.370, g=0.734\n",
            ">8, 24/234, d=1.369, g=0.776\n",
            ">8, 25/234, d=1.336, g=0.790\n",
            ">8, 26/234, d=1.322, g=0.810\n",
            ">8, 27/234, d=1.314, g=0.812\n",
            ">8, 28/234, d=1.333, g=0.787\n",
            ">8, 29/234, d=1.326, g=0.786\n",
            ">8, 30/234, d=1.359, g=0.746\n",
            ">8, 31/234, d=1.366, g=0.712\n",
            ">8, 32/234, d=1.370, g=0.722\n",
            ">8, 33/234, d=1.391, g=0.731\n",
            ">8, 34/234, d=1.391, g=0.755\n",
            ">8, 35/234, d=1.378, g=0.793\n",
            ">8, 36/234, d=1.338, g=0.847\n",
            ">8, 37/234, d=1.324, g=0.886\n",
            ">8, 38/234, d=1.305, g=0.870\n",
            ">8, 39/234, d=1.338, g=0.877\n",
            ">8, 40/234, d=1.345, g=0.809\n",
            ">8, 41/234, d=1.360, g=0.775\n",
            ">8, 42/234, d=1.406, g=0.757\n",
            ">8, 43/234, d=1.393, g=0.717\n",
            ">8, 44/234, d=1.386, g=0.721\n",
            ">8, 45/234, d=1.363, g=0.736\n",
            ">8, 46/234, d=1.357, g=0.749\n",
            ">8, 47/234, d=1.337, g=0.778\n",
            ">8, 48/234, d=1.324, g=0.796\n",
            ">8, 49/234, d=1.311, g=0.801\n",
            ">8, 50/234, d=1.313, g=0.789\n",
            ">8, 51/234, d=1.351, g=0.760\n",
            ">8, 52/234, d=1.319, g=0.726\n",
            ">8, 53/234, d=1.387, g=0.733\n",
            ">8, 54/234, d=1.397, g=0.719\n",
            ">8, 55/234, d=1.423, g=0.735\n",
            ">8, 56/234, d=1.376, g=0.770\n",
            ">8, 57/234, d=1.356, g=0.811\n",
            ">8, 58/234, d=1.330, g=0.876\n",
            ">8, 59/234, d=1.277, g=0.897\n",
            ">8, 60/234, d=1.312, g=0.875\n",
            ">8, 61/234, d=1.333, g=0.832\n",
            ">8, 62/234, d=1.322, g=0.798\n",
            ">8, 63/234, d=1.377, g=0.764\n",
            ">8, 64/234, d=1.423, g=0.721\n",
            ">8, 65/234, d=1.425, g=0.707\n",
            ">8, 66/234, d=1.418, g=0.703\n",
            ">8, 67/234, d=1.347, g=0.745\n",
            ">8, 68/234, d=1.293, g=0.785\n",
            ">8, 69/234, d=1.271, g=0.814\n",
            ">8, 70/234, d=1.319, g=0.797\n",
            ">8, 71/234, d=1.324, g=0.782\n",
            ">8, 72/234, d=1.319, g=0.761\n",
            ">8, 73/234, d=1.338, g=0.723\n",
            ">8, 74/234, d=1.363, g=0.709\n",
            ">8, 75/234, d=1.399, g=0.706\n",
            ">8, 76/234, d=1.392, g=0.714\n",
            ">8, 77/234, d=1.388, g=0.758\n",
            ">8, 78/234, d=1.335, g=0.806\n",
            ">8, 79/234, d=1.293, g=0.825\n",
            ">8, 80/234, d=1.314, g=0.841\n",
            ">8, 81/234, d=1.304, g=0.859\n",
            ">8, 82/234, d=1.337, g=0.846\n",
            ">8, 83/234, d=1.349, g=0.811\n",
            ">8, 84/234, d=1.322, g=0.797\n",
            ">8, 85/234, d=1.353, g=0.754\n",
            ">8, 86/234, d=1.434, g=0.733\n",
            ">8, 87/234, d=1.412, g=0.715\n",
            ">8, 88/234, d=1.360, g=0.744\n",
            ">8, 89/234, d=1.352, g=0.762\n",
            ">8, 90/234, d=1.324, g=0.765\n",
            ">8, 91/234, d=1.305, g=0.776\n",
            ">8, 92/234, d=1.331, g=0.786\n",
            ">8, 93/234, d=1.318, g=0.760\n",
            ">8, 94/234, d=1.335, g=0.755\n",
            ">8, 95/234, d=1.349, g=0.737\n",
            ">8, 96/234, d=1.353, g=0.724\n",
            ">8, 97/234, d=1.402, g=0.728\n",
            ">8, 98/234, d=1.391, g=0.741\n",
            ">8, 99/234, d=1.364, g=0.758\n",
            ">8, 100/234, d=1.371, g=0.780\n",
            ">8, 101/234, d=1.350, g=0.807\n",
            ">8, 102/234, d=1.353, g=0.801\n",
            ">8, 103/234, d=1.329, g=0.820\n",
            ">8, 104/234, d=1.350, g=0.824\n",
            ">8, 105/234, d=1.320, g=0.794\n",
            ">8, 106/234, d=1.356, g=0.785\n",
            ">8, 107/234, d=1.353, g=0.773\n",
            ">8, 108/234, d=1.386, g=0.746\n",
            ">8, 109/234, d=1.387, g=0.726\n",
            ">8, 110/234, d=1.389, g=0.737\n",
            ">8, 111/234, d=1.333, g=0.773\n",
            ">8, 112/234, d=1.314, g=0.781\n",
            ">8, 113/234, d=1.325, g=0.797\n",
            ">8, 114/234, d=1.295, g=0.820\n",
            ">8, 115/234, d=1.290, g=0.771\n",
            ">8, 116/234, d=1.324, g=0.751\n",
            ">8, 117/234, d=1.326, g=0.726\n",
            ">8, 118/234, d=1.368, g=0.711\n",
            ">8, 119/234, d=1.404, g=0.732\n",
            ">8, 120/234, d=1.418, g=0.746\n",
            ">8, 121/234, d=1.376, g=0.759\n",
            ">8, 122/234, d=1.364, g=0.792\n",
            ">8, 123/234, d=1.321, g=0.836\n",
            ">8, 124/234, d=1.296, g=0.857\n",
            ">8, 125/234, d=1.362, g=0.846\n",
            ">8, 126/234, d=1.339, g=0.811\n",
            ">8, 127/234, d=1.361, g=0.771\n",
            ">8, 128/234, d=1.394, g=0.749\n",
            ">8, 129/234, d=1.379, g=0.734\n",
            ">8, 130/234, d=1.379, g=0.749\n",
            ">8, 131/234, d=1.373, g=0.740\n",
            ">8, 132/234, d=1.347, g=0.770\n",
            ">8, 133/234, d=1.296, g=0.774\n",
            ">8, 134/234, d=1.302, g=0.789\n",
            ">8, 135/234, d=1.343, g=0.779\n",
            ">8, 136/234, d=1.337, g=0.749\n",
            ">8, 137/234, d=1.378, g=0.730\n",
            ">8, 138/234, d=1.383, g=0.739\n",
            ">8, 139/234, d=1.361, g=0.736\n",
            ">8, 140/234, d=1.371, g=0.746\n",
            ">8, 141/234, d=1.343, g=0.752\n",
            ">8, 142/234, d=1.332, g=0.782\n",
            ">8, 143/234, d=1.325, g=0.788\n",
            ">8, 144/234, d=1.352, g=0.807\n",
            ">8, 145/234, d=1.336, g=0.809\n",
            ">8, 146/234, d=1.343, g=0.812\n",
            ">8, 147/234, d=1.319, g=0.794\n",
            ">8, 148/234, d=1.359, g=0.757\n",
            ">8, 149/234, d=1.377, g=0.748\n",
            ">8, 150/234, d=1.386, g=0.738\n",
            ">8, 151/234, d=1.383, g=0.743\n",
            ">8, 152/234, d=1.364, g=0.749\n",
            ">8, 153/234, d=1.361, g=0.765\n",
            ">8, 154/234, d=1.340, g=0.756\n",
            ">8, 155/234, d=1.326, g=0.770\n",
            ">8, 156/234, d=1.341, g=0.763\n",
            ">8, 157/234, d=1.331, g=0.748\n",
            ">8, 158/234, d=1.348, g=0.747\n",
            ">8, 159/234, d=1.361, g=0.743\n",
            ">8, 160/234, d=1.366, g=0.748\n",
            ">8, 161/234, d=1.376, g=0.747\n",
            ">8, 162/234, d=1.337, g=0.762\n",
            ">8, 163/234, d=1.347, g=0.791\n",
            ">8, 164/234, d=1.343, g=0.796\n",
            ">8, 165/234, d=1.358, g=0.813\n",
            ">8, 166/234, d=1.361, g=0.808\n",
            ">8, 167/234, d=1.351, g=0.794\n",
            ">8, 168/234, d=1.346, g=0.789\n",
            ">8, 169/234, d=1.342, g=0.775\n",
            ">8, 170/234, d=1.364, g=0.759\n",
            ">8, 171/234, d=1.395, g=0.756\n",
            ">8, 172/234, d=1.390, g=0.776\n",
            ">8, 173/234, d=1.352, g=0.777\n",
            ">8, 174/234, d=1.341, g=0.772\n",
            ">8, 175/234, d=1.331, g=0.767\n",
            ">8, 176/234, d=1.318, g=0.776\n",
            ">8, 177/234, d=1.348, g=0.768\n",
            ">8, 178/234, d=1.313, g=0.751\n",
            ">8, 179/234, d=1.367, g=0.744\n",
            ">8, 180/234, d=1.408, g=0.740\n",
            ">8, 181/234, d=1.381, g=0.746\n",
            ">8, 182/234, d=1.353, g=0.776\n",
            ">8, 183/234, d=1.342, g=0.794\n",
            ">8, 184/234, d=1.347, g=0.813\n",
            ">8, 185/234, d=1.344, g=0.811\n",
            ">8, 186/234, d=1.306, g=0.802\n",
            ">8, 187/234, d=1.324, g=0.815\n",
            ">8, 188/234, d=1.350, g=0.780\n",
            ">8, 189/234, d=1.378, g=0.756\n",
            ">8, 190/234, d=1.379, g=0.741\n",
            ">8, 191/234, d=1.368, g=0.749\n",
            ">8, 192/234, d=1.370, g=0.750\n",
            ">8, 193/234, d=1.353, g=0.772\n",
            ">8, 194/234, d=1.330, g=0.794\n",
            ">8, 195/234, d=1.336, g=0.765\n",
            ">8, 196/234, d=1.327, g=0.763\n",
            ">8, 197/234, d=1.360, g=0.753\n",
            ">8, 198/234, d=1.376, g=0.735\n",
            ">8, 199/234, d=1.362, g=0.748\n",
            ">8, 200/234, d=1.381, g=0.751\n",
            ">8, 201/234, d=1.356, g=0.780\n",
            ">8, 202/234, d=1.341, g=0.795\n",
            ">8, 203/234, d=1.327, g=0.826\n",
            ">8, 204/234, d=1.310, g=0.814\n",
            ">8, 205/234, d=1.353, g=0.818\n",
            ">8, 206/234, d=1.325, g=0.803\n",
            ">8, 207/234, d=1.321, g=0.756\n",
            ">8, 208/234, d=1.388, g=0.751\n",
            ">8, 209/234, d=1.369, g=0.721\n",
            ">8, 210/234, d=1.369, g=0.732\n",
            ">8, 211/234, d=1.374, g=0.734\n",
            ">8, 212/234, d=1.331, g=0.762\n",
            ">8, 213/234, d=1.325, g=0.773\n",
            ">8, 214/234, d=1.311, g=0.777\n",
            ">8, 215/234, d=1.349, g=0.790\n",
            ">8, 216/234, d=1.323, g=0.756\n",
            ">8, 217/234, d=1.364, g=0.742\n",
            ">8, 218/234, d=1.416, g=0.737\n",
            ">8, 219/234, d=1.360, g=0.766\n",
            ">8, 220/234, d=1.386, g=0.773\n",
            ">8, 221/234, d=1.333, g=0.805\n",
            ">8, 222/234, d=1.369, g=0.809\n",
            ">8, 223/234, d=1.332, g=0.824\n",
            ">8, 224/234, d=1.351, g=0.820\n",
            ">8, 225/234, d=1.330, g=0.820\n",
            ">8, 226/234, d=1.389, g=0.782\n",
            ">8, 227/234, d=1.385, g=0.768\n",
            ">8, 228/234, d=1.357, g=0.756\n",
            ">8, 229/234, d=1.363, g=0.747\n",
            ">8, 230/234, d=1.373, g=0.761\n",
            ">8, 231/234, d=1.360, g=0.775\n",
            ">8, 232/234, d=1.338, g=0.765\n",
            ">8, 233/234, d=1.330, g=0.770\n",
            ">8, 234/234, d=1.349, g=0.769\n",
            ">9, 1/234, d=1.356, g=0.757\n",
            ">9, 2/234, d=1.362, g=0.746\n",
            ">9, 3/234, d=1.363, g=0.760\n",
            ">9, 4/234, d=1.367, g=0.752\n",
            ">9, 5/234, d=1.359, g=0.768\n",
            ">9, 6/234, d=1.336, g=0.773\n",
            ">9, 7/234, d=1.325, g=0.771\n",
            ">9, 8/234, d=1.349, g=0.814\n",
            ">9, 9/234, d=1.343, g=0.806\n",
            ">9, 10/234, d=1.351, g=0.794\n",
            ">9, 11/234, d=1.367, g=0.779\n",
            ">9, 12/234, d=1.361, g=0.775\n",
            ">9, 13/234, d=1.395, g=0.739\n",
            ">9, 14/234, d=1.368, g=0.735\n",
            ">9, 15/234, d=1.369, g=0.756\n",
            ">9, 16/234, d=1.361, g=0.767\n",
            ">9, 17/234, d=1.345, g=0.769\n",
            ">9, 18/234, d=1.362, g=0.758\n",
            ">9, 19/234, d=1.374, g=0.755\n",
            ">9, 20/234, d=1.373, g=0.746\n",
            ">9, 21/234, d=1.356, g=0.741\n",
            ">9, 22/234, d=1.344, g=0.751\n",
            ">9, 23/234, d=1.367, g=0.753\n",
            ">9, 24/234, d=1.324, g=0.765\n",
            ">9, 25/234, d=1.351, g=0.765\n",
            ">9, 26/234, d=1.348, g=0.776\n",
            ">9, 27/234, d=1.360, g=0.769\n",
            ">9, 28/234, d=1.367, g=0.780\n",
            ">9, 29/234, d=1.332, g=0.776\n",
            ">9, 30/234, d=1.362, g=0.751\n",
            ">9, 31/234, d=1.366, g=0.750\n",
            ">9, 32/234, d=1.365, g=0.747\n",
            ">9, 33/234, d=1.349, g=0.755\n",
            ">9, 34/234, d=1.360, g=0.748\n",
            ">9, 35/234, d=1.361, g=0.761\n",
            ">9, 36/234, d=1.359, g=0.757\n",
            ">9, 37/234, d=1.339, g=0.741\n",
            ">9, 38/234, d=1.331, g=0.744\n",
            ">9, 39/234, d=1.355, g=0.744\n",
            ">9, 40/234, d=1.413, g=0.732\n",
            ">9, 41/234, d=1.345, g=0.752\n",
            ">9, 42/234, d=1.341, g=0.765\n",
            ">9, 43/234, d=1.383, g=0.796\n",
            ">9, 44/234, d=1.333, g=0.808\n",
            ">9, 45/234, d=1.351, g=0.809\n",
            ">9, 46/234, d=1.364, g=0.790\n",
            ">9, 47/234, d=1.372, g=0.778\n",
            ">9, 48/234, d=1.368, g=0.758\n",
            ">9, 49/234, d=1.369, g=0.754\n",
            ">9, 50/234, d=1.356, g=0.748\n",
            ">9, 51/234, d=1.364, g=0.772\n",
            ">9, 52/234, d=1.333, g=0.779\n",
            ">9, 53/234, d=1.329, g=0.786\n",
            ">9, 54/234, d=1.335, g=0.757\n",
            ">9, 55/234, d=1.354, g=0.743\n",
            ">9, 56/234, d=1.357, g=0.739\n",
            ">9, 57/234, d=1.399, g=0.744\n",
            ">9, 58/234, d=1.346, g=0.746\n",
            ">9, 59/234, d=1.352, g=0.795\n",
            ">9, 60/234, d=1.329, g=0.802\n",
            ">9, 61/234, d=1.363, g=0.817\n",
            ">9, 62/234, d=1.338, g=0.812\n",
            ">9, 63/234, d=1.353, g=0.788\n",
            ">9, 64/234, d=1.393, g=0.792\n",
            ">9, 65/234, d=1.391, g=0.765\n",
            ">9, 66/234, d=1.391, g=0.746\n",
            ">9, 67/234, d=1.396, g=0.770\n",
            ">9, 68/234, d=1.358, g=0.764\n",
            ">9, 69/234, d=1.359, g=0.776\n",
            ">9, 70/234, d=1.359, g=0.776\n",
            ">9, 71/234, d=1.338, g=0.780\n",
            ">9, 72/234, d=1.350, g=0.750\n",
            ">9, 73/234, d=1.365, g=0.756\n",
            ">9, 74/234, d=1.387, g=0.738\n",
            ">9, 75/234, d=1.403, g=0.758\n",
            ">9, 76/234, d=1.370, g=0.797\n",
            ">9, 77/234, d=1.372, g=0.824\n",
            ">9, 78/234, d=1.320, g=0.836\n",
            ">9, 79/234, d=1.327, g=0.841\n",
            ">9, 80/234, d=1.378, g=0.815\n",
            ">9, 81/234, d=1.379, g=0.791\n",
            ">9, 82/234, d=1.378, g=0.771\n",
            ">9, 83/234, d=1.391, g=0.752\n",
            ">9, 84/234, d=1.367, g=0.738\n",
            ">9, 85/234, d=1.362, g=0.788\n",
            ">9, 86/234, d=1.309, g=0.796\n",
            ">9, 87/234, d=1.323, g=0.799\n",
            ">9, 88/234, d=1.329, g=0.766\n",
            ">9, 89/234, d=1.330, g=0.738\n",
            ">9, 90/234, d=1.388, g=0.732\n",
            ">9, 91/234, d=1.391, g=0.734\n",
            ">9, 92/234, d=1.400, g=0.714\n",
            ">9, 93/234, d=1.403, g=0.784\n",
            ">9, 94/234, d=1.333, g=0.819\n",
            ">9, 95/234, d=1.334, g=0.850\n",
            ">9, 96/234, d=1.298, g=0.851\n",
            ">9, 97/234, d=1.326, g=0.845\n",
            ">9, 98/234, d=1.341, g=0.813\n",
            ">9, 99/234, d=1.410, g=0.768\n",
            ">9, 100/234, d=1.426, g=0.742\n",
            ">9, 101/234, d=1.372, g=0.746\n",
            ">9, 102/234, d=1.377, g=0.762\n",
            ">9, 103/234, d=1.346, g=0.805\n",
            ">9, 104/234, d=1.322, g=0.819\n",
            ">9, 105/234, d=1.293, g=0.802\n",
            ">9, 106/234, d=1.352, g=0.775\n",
            ">9, 107/234, d=1.390, g=0.747\n",
            ">9, 108/234, d=1.368, g=0.733\n",
            ">9, 109/234, d=1.375, g=0.740\n",
            ">9, 110/234, d=1.342, g=0.733\n",
            ">9, 111/234, d=1.365, g=0.763\n",
            ">9, 112/234, d=1.356, g=0.814\n",
            ">9, 113/234, d=1.332, g=0.818\n",
            ">9, 114/234, d=1.321, g=0.832\n",
            ">9, 115/234, d=1.322, g=0.824\n",
            ">9, 116/234, d=1.346, g=0.828\n",
            ">9, 117/234, d=1.383, g=0.775\n",
            ">9, 118/234, d=1.363, g=0.754\n",
            ">9, 119/234, d=1.378, g=0.744\n",
            ">9, 120/234, d=1.371, g=0.749\n",
            ">9, 121/234, d=1.365, g=0.761\n",
            ">9, 122/234, d=1.312, g=0.774\n",
            ">9, 123/234, d=1.307, g=0.791\n",
            ">9, 124/234, d=1.330, g=0.769\n",
            ">9, 125/234, d=1.355, g=0.750\n",
            ">9, 126/234, d=1.384, g=0.726\n",
            ">9, 127/234, d=1.375, g=0.733\n",
            ">9, 128/234, d=1.376, g=0.737\n",
            ">9, 129/234, d=1.359, g=0.770\n",
            ">9, 130/234, d=1.352, g=0.777\n",
            ">9, 131/234, d=1.336, g=0.804\n",
            ">9, 132/234, d=1.404, g=0.807\n",
            ">9, 133/234, d=1.328, g=0.809\n",
            ">9, 134/234, d=1.335, g=0.800\n",
            ">9, 135/234, d=1.352, g=0.784\n",
            ">9, 136/234, d=1.364, g=0.770\n",
            ">9, 137/234, d=1.386, g=0.741\n",
            ">9, 138/234, d=1.387, g=0.744\n",
            ">9, 139/234, d=1.366, g=0.774\n",
            ">9, 140/234, d=1.360, g=0.769\n",
            ">9, 141/234, d=1.317, g=0.757\n",
            ">9, 142/234, d=1.344, g=0.766\n",
            ">9, 143/234, d=1.351, g=0.761\n",
            ">9, 144/234, d=1.355, g=0.747\n",
            ">9, 145/234, d=1.335, g=0.748\n",
            ">9, 146/234, d=1.390, g=0.746\n",
            ">9, 147/234, d=1.379, g=0.758\n",
            ">9, 148/234, d=1.359, g=0.772\n",
            ">9, 149/234, d=1.352, g=0.782\n",
            ">9, 150/234, d=1.344, g=0.829\n",
            ">9, 151/234, d=1.349, g=0.837\n",
            ">9, 152/234, d=1.372, g=0.805\n",
            ">9, 153/234, d=1.368, g=0.797\n",
            ">9, 154/234, d=1.364, g=0.751\n",
            ">9, 155/234, d=1.370, g=0.752\n",
            ">9, 156/234, d=1.378, g=0.724\n",
            ">9, 157/234, d=1.349, g=0.747\n",
            ">9, 158/234, d=1.347, g=0.773\n",
            ">9, 159/234, d=1.339, g=0.781\n",
            ">9, 160/234, d=1.357, g=0.779\n",
            ">9, 161/234, d=1.324, g=0.775\n",
            ">9, 162/234, d=1.339, g=0.752\n",
            ">9, 163/234, d=1.373, g=0.731\n",
            ">9, 164/234, d=1.409, g=0.725\n",
            ">9, 165/234, d=1.357, g=0.733\n",
            ">9, 166/234, d=1.362, g=0.758\n",
            ">9, 167/234, d=1.360, g=0.800\n",
            ">9, 168/234, d=1.343, g=0.808\n",
            ">9, 169/234, d=1.317, g=0.833\n",
            ">9, 170/234, d=1.354, g=0.841\n",
            ">9, 171/234, d=1.350, g=0.806\n",
            ">9, 172/234, d=1.366, g=0.780\n",
            ">9, 173/234, d=1.380, g=0.762\n",
            ">9, 174/234, d=1.402, g=0.753\n",
            ">9, 175/234, d=1.347, g=0.767\n",
            ">9, 176/234, d=1.353, g=0.777\n",
            ">9, 177/234, d=1.356, g=0.795\n",
            ">9, 178/234, d=1.350, g=0.775\n",
            ">9, 179/234, d=1.352, g=0.762\n",
            ">9, 180/234, d=1.315, g=0.768\n",
            ">9, 181/234, d=1.365, g=0.748\n",
            ">9, 182/234, d=1.336, g=0.741\n",
            ">9, 183/234, d=1.337, g=0.750\n",
            ">9, 184/234, d=1.358, g=0.769\n",
            ">9, 185/234, d=1.392, g=0.774\n",
            ">9, 186/234, d=1.335, g=0.796\n",
            ">9, 187/234, d=1.332, g=0.828\n",
            ">9, 188/234, d=1.399, g=0.798\n",
            ">9, 189/234, d=1.373, g=0.778\n",
            ">9, 190/234, d=1.382, g=0.760\n",
            ">9, 191/234, d=1.363, g=0.755\n",
            ">9, 192/234, d=1.357, g=0.744\n",
            ">9, 193/234, d=1.332, g=0.764\n",
            ">9, 194/234, d=1.349, g=0.755\n",
            ">9, 195/234, d=1.332, g=0.767\n",
            ">9, 196/234, d=1.340, g=0.761\n",
            ">9, 197/234, d=1.340, g=0.737\n",
            ">9, 198/234, d=1.376, g=0.735\n",
            ">9, 199/234, d=1.355, g=0.738\n",
            ">9, 200/234, d=1.369, g=0.740\n",
            ">9, 201/234, d=1.377, g=0.747\n",
            ">9, 202/234, d=1.357, g=0.772\n",
            ">9, 203/234, d=1.334, g=0.790\n",
            ">9, 204/234, d=1.353, g=0.798\n",
            ">9, 205/234, d=1.338, g=0.785\n",
            ">9, 206/234, d=1.356, g=0.798\n",
            ">9, 207/234, d=1.336, g=0.786\n",
            ">9, 208/234, d=1.374, g=0.760\n",
            ">9, 209/234, d=1.366, g=0.755\n",
            ">9, 210/234, d=1.371, g=0.753\n",
            ">9, 211/234, d=1.366, g=0.745\n",
            ">9, 212/234, d=1.360, g=0.765\n",
            ">9, 213/234, d=1.335, g=0.770\n",
            ">9, 214/234, d=1.337, g=0.773\n",
            ">9, 215/234, d=1.342, g=0.755\n",
            ">9, 216/234, d=1.353, g=0.750\n",
            ">9, 217/234, d=1.376, g=0.742\n",
            ">9, 218/234, d=1.351, g=0.736\n",
            ">9, 219/234, d=1.360, g=0.758\n",
            ">9, 220/234, d=1.335, g=0.763\n",
            ">9, 221/234, d=1.362, g=0.788\n",
            ">9, 222/234, d=1.337, g=0.789\n",
            ">9, 223/234, d=1.322, g=0.815\n",
            ">9, 224/234, d=1.329, g=0.806\n",
            ">9, 225/234, d=1.351, g=0.792\n",
            ">9, 226/234, d=1.359, g=0.765\n",
            ">9, 227/234, d=1.409, g=0.756\n",
            ">9, 228/234, d=1.368, g=0.744\n",
            ">9, 229/234, d=1.382, g=0.757\n",
            ">9, 230/234, d=1.339, g=0.769\n",
            ">9, 231/234, d=1.338, g=0.770\n",
            ">9, 232/234, d=1.328, g=0.779\n",
            ">9, 233/234, d=1.344, g=0.754\n",
            ">9, 234/234, d=1.334, g=0.750\n",
            ">10, 1/234, d=1.377, g=0.735\n",
            ">10, 2/234, d=1.399, g=0.742\n",
            ">10, 3/234, d=1.373, g=0.787\n",
            ">10, 4/234, d=1.341, g=0.809\n",
            ">10, 5/234, d=1.325, g=0.832\n",
            ">10, 6/234, d=1.334, g=0.822\n",
            ">10, 7/234, d=1.327, g=0.817\n",
            ">10, 8/234, d=1.345, g=0.810\n",
            ">10, 9/234, d=1.368, g=0.787\n",
            ">10, 10/234, d=1.368, g=0.753\n",
            ">10, 11/234, d=1.386, g=0.735\n",
            ">10, 12/234, d=1.351, g=0.753\n",
            ">10, 13/234, d=1.353, g=0.778\n",
            ">10, 14/234, d=1.303, g=0.805\n",
            ">10, 15/234, d=1.326, g=0.806\n",
            ">10, 16/234, d=1.377, g=0.753\n",
            ">10, 17/234, d=1.338, g=0.745\n",
            ">10, 18/234, d=1.375, g=0.721\n",
            ">10, 19/234, d=1.402, g=0.728\n",
            ">10, 20/234, d=1.385, g=0.763\n",
            ">10, 21/234, d=1.348, g=0.783\n",
            ">10, 22/234, d=1.351, g=0.800\n",
            ">10, 23/234, d=1.341, g=0.830\n",
            ">10, 24/234, d=1.333, g=0.830\n",
            ">10, 25/234, d=1.334, g=0.824\n",
            ">10, 26/234, d=1.357, g=0.784\n",
            ">10, 27/234, d=1.384, g=0.768\n",
            ">10, 28/234, d=1.389, g=0.758\n",
            ">10, 29/234, d=1.365, g=0.748\n",
            ">10, 30/234, d=1.352, g=0.759\n",
            ">10, 31/234, d=1.328, g=0.779\n",
            ">10, 32/234, d=1.342, g=0.785\n",
            ">10, 33/234, d=1.378, g=0.741\n",
            ">10, 34/234, d=1.389, g=0.734\n",
            ">10, 35/234, d=1.362, g=0.736\n",
            ">10, 36/234, d=1.369, g=0.757\n",
            ">10, 37/234, d=1.330, g=0.768\n",
            ">10, 38/234, d=1.341, g=0.795\n",
            ">10, 39/234, d=1.335, g=0.812\n",
            ">10, 40/234, d=1.317, g=0.829\n",
            ">10, 41/234, d=1.327, g=0.848\n",
            ">10, 42/234, d=1.370, g=0.812\n",
            ">10, 43/234, d=1.354, g=0.777\n",
            ">10, 44/234, d=1.380, g=0.753\n",
            ">10, 45/234, d=1.378, g=0.753\n",
            ">10, 46/234, d=1.393, g=0.767\n",
            ">10, 47/234, d=1.357, g=0.782\n",
            ">10, 48/234, d=1.330, g=0.771\n",
            ">10, 49/234, d=1.308, g=0.793\n",
            ">10, 50/234, d=1.331, g=0.747\n",
            ">10, 51/234, d=1.355, g=0.754\n",
            ">10, 52/234, d=1.396, g=0.715\n",
            ">10, 53/234, d=1.416, g=0.736\n",
            ">10, 54/234, d=1.371, g=0.742\n",
            ">10, 55/234, d=1.361, g=0.767\n",
            ">10, 56/234, d=1.339, g=0.787\n",
            ">10, 57/234, d=1.317, g=0.797\n",
            ">10, 58/234, d=1.331, g=0.825\n",
            ">10, 59/234, d=1.320, g=0.791\n",
            ">10, 60/234, d=1.341, g=0.794\n",
            ">10, 61/234, d=1.367, g=0.764\n",
            ">10, 62/234, d=1.358, g=0.749\n",
            ">10, 63/234, d=1.352, g=0.749\n",
            ">10, 64/234, d=1.374, g=0.750\n",
            ">10, 65/234, d=1.353, g=0.766\n",
            ">10, 66/234, d=1.346, g=0.772\n",
            ">10, 67/234, d=1.319, g=0.791\n",
            ">10, 68/234, d=1.289, g=0.782\n",
            ">10, 69/234, d=1.374, g=0.753\n",
            ">10, 70/234, d=1.377, g=0.728\n",
            ">10, 71/234, d=1.370, g=0.726\n",
            ">10, 72/234, d=1.403, g=0.753\n",
            ">10, 73/234, d=1.367, g=0.785\n",
            ">10, 74/234, d=1.335, g=0.830\n",
            ">10, 75/234, d=1.303, g=0.839\n",
            ">10, 76/234, d=1.352, g=0.837\n",
            ">10, 77/234, d=1.368, g=0.791\n",
            ">10, 78/234, d=1.354, g=0.775\n",
            ">10, 79/234, d=1.360, g=0.744\n",
            ">10, 80/234, d=1.375, g=0.741\n",
            ">10, 81/234, d=1.365, g=0.759\n",
            ">10, 82/234, d=1.361, g=0.771\n",
            ">10, 83/234, d=1.310, g=0.784\n",
            ">10, 84/234, d=1.327, g=0.786\n",
            ">10, 85/234, d=1.344, g=0.769\n",
            ">10, 86/234, d=1.371, g=0.766\n",
            ">10, 87/234, d=1.331, g=0.750\n",
            ">10, 88/234, d=1.365, g=0.742\n",
            ">10, 89/234, d=1.433, g=0.740\n",
            ">10, 90/234, d=1.364, g=0.768\n",
            ">10, 91/234, d=1.370, g=0.792\n",
            ">10, 92/234, d=1.318, g=0.818\n",
            ">10, 93/234, d=1.319, g=0.845\n",
            ">10, 94/234, d=1.337, g=0.815\n",
            ">10, 95/234, d=1.320, g=0.787\n",
            ">10, 96/234, d=1.384, g=0.768\n",
            ">10, 97/234, d=1.389, g=0.738\n",
            ">10, 98/234, d=1.391, g=0.750\n",
            ">10, 99/234, d=1.357, g=0.752\n",
            ">10, 100/234, d=1.356, g=0.785\n",
            ">10, 101/234, d=1.313, g=0.776\n",
            ">10, 102/234, d=1.307, g=0.790\n",
            ">10, 103/234, d=1.340, g=0.779\n",
            ">10, 104/234, d=1.351, g=0.743\n",
            ">10, 105/234, d=1.362, g=0.729\n",
            ">10, 106/234, d=1.422, g=0.745\n",
            ">10, 107/234, d=1.353, g=0.750\n",
            ">10, 108/234, d=1.389, g=0.782\n",
            ">10, 109/234, d=1.344, g=0.819\n",
            ">10, 110/234, d=1.353, g=0.823\n",
            ">10, 111/234, d=1.293, g=0.840\n",
            ">10, 112/234, d=1.318, g=0.818\n",
            ">10, 113/234, d=1.334, g=0.792\n",
            ">10, 114/234, d=1.350, g=0.757\n",
            ">10, 115/234, d=1.387, g=0.751\n",
            ">10, 116/234, d=1.356, g=0.739\n",
            ">10, 117/234, d=1.382, g=0.770\n",
            ">10, 118/234, d=1.301, g=0.781\n",
            ">10, 119/234, d=1.342, g=0.791\n",
            ">10, 120/234, d=1.322, g=0.808\n",
            ">10, 121/234, d=1.307, g=0.755\n",
            ">10, 122/234, d=1.391, g=0.749\n",
            ">10, 123/234, d=1.357, g=0.719\n",
            ">10, 124/234, d=1.395, g=0.733\n",
            ">10, 125/234, d=1.419, g=0.746\n",
            ">10, 126/234, d=1.383, g=0.801\n",
            ">10, 127/234, d=1.325, g=0.822\n",
            ">10, 128/234, d=1.333, g=0.858\n",
            ">10, 129/234, d=1.298, g=0.841\n",
            ">10, 130/234, d=1.322, g=0.841\n",
            ">10, 131/234, d=1.371, g=0.809\n",
            ">10, 132/234, d=1.409, g=0.742\n",
            ">10, 133/234, d=1.400, g=0.730\n",
            ">10, 134/234, d=1.369, g=0.756\n",
            ">10, 135/234, d=1.342, g=0.788\n",
            ">10, 136/234, d=1.345, g=0.806\n",
            ">10, 137/234, d=1.293, g=0.803\n",
            ">10, 138/234, d=1.262, g=0.798\n",
            ">10, 139/234, d=1.385, g=0.753\n",
            ">10, 140/234, d=1.385, g=0.732\n",
            ">10, 141/234, d=1.404, g=0.723\n",
            ">10, 142/234, d=1.370, g=0.731\n",
            ">10, 143/234, d=1.392, g=0.754\n",
            ">10, 144/234, d=1.325, g=0.782\n",
            ">10, 145/234, d=1.326, g=0.822\n",
            ">10, 146/234, d=1.334, g=0.831\n",
            ">10, 147/234, d=1.318, g=0.820\n",
            ">10, 148/234, d=1.337, g=0.798\n",
            ">10, 149/234, d=1.356, g=0.795\n",
            ">10, 150/234, d=1.377, g=0.765\n",
            ">10, 151/234, d=1.372, g=0.745\n",
            ">10, 152/234, d=1.387, g=0.747\n",
            ">10, 153/234, d=1.374, g=0.760\n",
            ">10, 154/234, d=1.353, g=0.766\n",
            ">10, 155/234, d=1.344, g=0.779\n",
            ">10, 156/234, d=1.317, g=0.794\n",
            ">10, 157/234, d=1.349, g=0.780\n",
            ">10, 158/234, d=1.370, g=0.773\n",
            ">10, 159/234, d=1.344, g=0.756\n",
            ">10, 160/234, d=1.354, g=0.737\n",
            ">10, 161/234, d=1.338, g=0.754\n",
            ">10, 162/234, d=1.387, g=0.761\n",
            ">10, 163/234, d=1.353, g=0.774\n",
            ">10, 164/234, d=1.358, g=0.805\n",
            ">10, 165/234, d=1.331, g=0.797\n",
            ">10, 166/234, d=1.346, g=0.785\n",
            ">10, 167/234, d=1.351, g=0.783\n",
            ">10, 168/234, d=1.342, g=0.785\n",
            ">10, 169/234, d=1.362, g=0.753\n",
            ">10, 170/234, d=1.351, g=0.761\n",
            ">10, 171/234, d=1.338, g=0.747\n",
            ">10, 172/234, d=1.358, g=0.770\n",
            ">10, 173/234, d=1.348, g=0.764\n",
            ">10, 174/234, d=1.369, g=0.762\n",
            ">10, 175/234, d=1.363, g=0.770\n",
            ">10, 176/234, d=1.335, g=0.762\n",
            ">10, 177/234, d=1.372, g=0.748\n",
            ">10, 178/234, d=1.404, g=0.746\n",
            ">10, 179/234, d=1.410, g=0.742\n",
            ">10, 180/234, d=1.374, g=0.743\n",
            ">10, 181/234, d=1.336, g=0.761\n",
            ">10, 182/234, d=1.357, g=0.766\n",
            ">10, 183/234, d=1.363, g=0.781\n",
            ">10, 184/234, d=1.343, g=0.785\n",
            ">10, 185/234, d=1.340, g=0.770\n",
            ">10, 186/234, d=1.340, g=0.770\n",
            ">10, 187/234, d=1.351, g=0.758\n",
            ">10, 188/234, d=1.318, g=0.748\n",
            ">10, 189/234, d=1.374, g=0.741\n",
            ">10, 190/234, d=1.384, g=0.776\n",
            ">10, 191/234, d=1.347, g=0.776\n",
            ">10, 192/234, d=1.366, g=0.769\n",
            ">10, 193/234, d=1.323, g=0.766\n",
            ">10, 194/234, d=1.332, g=0.767\n",
            ">10, 195/234, d=1.323, g=0.763\n",
            ">10, 196/234, d=1.349, g=0.744\n",
            ">10, 197/234, d=1.350, g=0.749\n",
            ">10, 198/234, d=1.361, g=0.757\n",
            ">10, 199/234, d=1.359, g=0.781\n",
            ">10, 200/234, d=1.349, g=0.788\n",
            ">10, 201/234, d=1.319, g=0.828\n",
            ">10, 202/234, d=1.339, g=0.812\n",
            ">10, 203/234, d=1.372, g=0.793\n",
            ">10, 204/234, d=1.338, g=0.776\n",
            ">10, 205/234, d=1.390, g=0.775\n",
            ">10, 206/234, d=1.326, g=0.751\n",
            ">10, 207/234, d=1.375, g=0.744\n",
            ">10, 208/234, d=1.381, g=0.756\n",
            ">10, 209/234, d=1.353, g=0.773\n",
            ">10, 210/234, d=1.345, g=0.772\n",
            ">10, 211/234, d=1.333, g=0.790\n",
            ">10, 212/234, d=1.341, g=0.755\n",
            ">10, 213/234, d=1.359, g=0.759\n",
            ">10, 214/234, d=1.390, g=0.735\n",
            ">10, 215/234, d=1.310, g=0.765\n",
            ">10, 216/234, d=1.366, g=0.783\n",
            ">10, 217/234, d=1.373, g=0.789\n",
            ">10, 218/234, d=1.339, g=0.802\n",
            ">10, 219/234, d=1.359, g=0.792\n",
            ">10, 220/234, d=1.368, g=0.791\n",
            ">10, 221/234, d=1.321, g=0.772\n",
            ">10, 222/234, d=1.340, g=0.778\n",
            ">10, 223/234, d=1.359, g=0.773\n",
            ">10, 224/234, d=1.374, g=0.749\n",
            ">10, 225/234, d=1.366, g=0.768\n",
            ">10, 226/234, d=1.352, g=0.773\n",
            ">10, 227/234, d=1.339, g=0.770\n",
            ">10, 228/234, d=1.330, g=0.771\n",
            ">10, 229/234, d=1.309, g=0.773\n",
            ">10, 230/234, d=1.323, g=0.767\n",
            ">10, 231/234, d=1.391, g=0.754\n",
            ">10, 232/234, d=1.388, g=0.733\n",
            ">10, 233/234, d=1.378, g=0.761\n",
            ">10, 234/234, d=1.371, g=0.789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBFJckoEIiJJTolARCTJ/T9CqVN36eP3GAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMMUXLZTLl_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  model_save_name = 'cgan_generator.h5'\n",
        "  path = F\"/content/gdrive/My Drive/training_checkpoints\"\n",
        "  checkpoint_prefix = os.path.join(path, model_save_name) \n",
        "  generator_cgan.save(checkpoint_prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aILedMiuLpp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_and_save_3images(model, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(1, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxHl3i_3HgX_",
        "colab_type": "text"
      },
      "source": [
        "Show three examples of photos generated of your first class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "32c648fe-c680-423d-b456-814e9c166f2d",
        "id": "a2LVOHLChgPK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "# example of loading the generator model and generating images\n",
        "from numpy import asarray\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.models import load_model\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tz_input = x_input.reshape(n_samples, latent_dim)\n",
        "\t# generate labels\n",
        "\tlabels = randint(0, n_classes, n_samples)\n",
        "\treturn [z_input, labels]\n",
        "\n",
        "# create and save a plot of generated images\n",
        "def save_plot(examples, n):\n",
        "\t# plot images\n",
        "\tfor i in range(X.shape[0]):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(1, 4, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
        "\tpyplot.show()\n",
        "\n",
        "# load model\n",
        "model = load_model(checkpoint_prefix)\n",
        "# generate images\n",
        "latent_points, labels = generate_latent_points(100, 3)\n",
        "#print(latent_points.shape[0])\n",
        "# specify labels 8\n",
        "print(\"Select first label as 8\")\n",
        "labels = np.ones(3)*8\n",
        "#print (labels)\n",
        "# generate images\n",
        "X  = model.predict([latent_points, labels])\n",
        "# scale from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot the result\n",
        "save_plot(X, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Select first label as 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAABXCAYAAAANp1DZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtiF9UMbHpiW",
        "colab_type": "text"
      },
      "source": [
        "Show three examples of photos generated of your second class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BImdFikeHsHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load model if somehow the training is interrupted\n",
        "model = load_model(checkpoint_prefix)\n",
        "# generate images\n",
        "latent_points, labels = generate_latent_points(100, 3)\n",
        "print(latent_points.shape[0])\n",
        "# specify labels 6\n",
        "print(\"Select second label as 6\")\n",
        "labels = np.ones(3)*6\n",
        "#print (labels)\n",
        "# generate images\n",
        "X  = model.predict([latent_points, labels])\n",
        "# scale from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot the result\n",
        "save_plot(X, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5c0d4c11-6302-4ff1-ffbf-95c400d3e41e",
        "id": "KY4q8NMChnPo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "source": [
        "# load model\n",
        "model = load_model(checkpoint_prefix)\n",
        "# generate images\n",
        "latent_points, labels = generate_latent_points(100, 3)\n",
        "print(latent_points.shape[0])\n",
        "# specify labels 6\n",
        "print(\"Select second label as 6\")\n",
        "labels = np.ones(3)*6\n",
        "#print (labels)\n",
        "# generate images\n",
        "X  = model.predict([latent_points, labels])\n",
        "# scale from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot the result\n",
        "save_plot(X, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "Select second label as 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAABXCAYAAAANp1DZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niUSBXBQSiUSB/wBTCXq7XoN46wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}